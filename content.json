{"meta":{"title":"JJW-STORY","subtitle":null,"description":null,"author":"JJW","url":"https://jjw-story.github.io","root":"/"},"pages":[{"title":"","date":"2020-04-26T03:11:43.119Z","updated":"2020-04-26T03:11:43.119Z","comments":true,"path":"about/index.html","permalink":"https://jjw-story.github.io/about/index.html","excerpt":"","text":"关于我理想汽车~开发工程师 关于工作城市：北京 关于学习每天都走在学习的大路上 关于座右铭 The Harder You Work, The Luckier You Will Be. (越努力，越幸运) 关于爱好杂乱无章 联系我 Home: JJW-STORE.GITHUB.IO Blog: JJW-STORE.GITHUB.IO Email: JJWSTORY.CHINA@gmail.com GitHub: JJW-STORE WeiBo: JJWStrive"}],"posts":[{"title":"Redis-实践三","slug":"Redis-实践三","date":"2021-04-02T01:43:11.000Z","updated":"2021-04-08T01:22:01.177Z","comments":true,"path":"2021/04/02/Redis-实践三/","link":"","permalink":"https://jjw-story.github.io/2021/04/02/Redis-实践三/","excerpt":"","text":"Redis 事务机制事务是数据库的一个重要功能。所谓的事务，就是指对数据进行读写的一系列操作。事务在执行时，会提供专门的属性保证，包括原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），也就是 ACID 属性。这些属性既包括了对事务执行结果的要求，也有对数据库在事务执行前后的数据状态变化的要求。 事务 ACID 属性的要求 原子性：原子性的要求很明确，就是一个事务中的多个操作必须都完成，或者都不完成。业务应用使用事务时，原子性也是最被看重的一个属性。 一致性：就是指数据库中的数据在事务执行前后是一致的。 隔离性：它要求数据库在执行一个事务时，其它操作无法存取到正在执行事务访问的数据。 持久性：数据库执行事务后，数据的修改要被持久化保存下来。当数据库重启后，数据的值需要是被修改后的值。 Redis 实现事务的方法事务的执行过程包含三个步骤，Redis 提供了 MULTI、EXEC 两个命令来完成这三个步骤。下面我们来分析下： 第一步，客户端要使用一个命令显式地表示一个事务的开启。在 Redis 中，这个命令就是 MULTI。 第二步，客户端把事务中本身要执行的具体操作（例如增删改数据）发送给服务器端。这些操作就是 Redis 本身提供的数据读写命令，例如 GET、SET 等。不过，这些命令虽然被客户端发送到了服务器端，但 Redis 实例只是把这些命令暂存到一个命令队列中，并不会立即执行。 第三步，客户端向服务器端发送提交事务的命令，让数据库实际执行第二步中发送的具体操作。Redis 提供的 EXEC 命令就是执行事务提交的。当服务器端收到 EXEC 命令后，才会实际执行命令队列中的所有命令。 下面的代码就显示了使用 MULTI 和 EXEC 执行一个事务的过程： 12345678910111213#开启事务127.0.0.1:6379&gt; MULTIOK#将a:stock减1，127.0.0.1:6379&gt; DECR a:stockQUEUED#将b:stock减1127.0.0.1:6379&gt; DECR b:stockQUEUED#实际执行事务127.0.0.1:6379&gt; EXEC1) (integer) 42) (integer) 9 Redis 的事务机制能保证哪些属性？原子性如果事务正常执行，没有发生任何错误，那么，MULTI 和 EXEC 配合使用，就可以保证多个操作都完成。但是，如果事务执行发生错误了，原子性还能保证吗？我们需要分三种情况来看。 第一种情况是，在执行 EXEC 命令前，客户端发送的操作命令本身就有错误（比如语法错误，使用了不存在的命令），在命令入队时就被 Redis 实例判断出来了。对于这种情况，在命令入队时，Redis 就会报错并且记录下这个错误。此时，我们还能继续提交命令操作。等到执行了 EXEC 命令之后，Redis 就会拒绝执行所有提交的命令操作，返回事务失败的结果。这样一来，事务中的所有命令都不会再被执行了，保证了原子性。 第二种情况是，事务操作入队时，命令和操作的数据类型不匹配，但 Redis 实例没有检查出错误。但是，在执行完 EXEC 命令以后，Redis 实际执行这些事务操作时，就会报错。不过，需要注意的是，虽然 Redis 会对错误命令报错，但还是会把正确的命令执行完。在这种情况下，事务的原子性就无法得到保证了。（例如事务中的 LPOP 命令对 String 类型数据进行操作，入队时没有报错，但是，在 EXEC 执行时报错了。） 如同第二种情况所属，Redis不能提供回滚机制吗？ 其实，Redis 中并没有提供回滚机制。虽然 Redis 提供了 DISCARD 命令，但是，这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。DISCARD 命令具体怎么用呢？我们来看下下面的代码。 123456789101112131415#读取a:stock的值4127.0.0.1:6379&gt; GET a:stock&quot;4&quot;#开启事务127.0.0.1:6379&gt; MULTI OK#发送事务的第一个操作，对a:stock减1127.0.0.1:6379&gt; DECR a:stockQUEUED#执行DISCARD命令，主动放弃事务127.0.0.1:6379&gt; DISCARDOK#再次读取a:stock的值，值没有被修改127.0.0.1:6379&gt; GET a:stock&quot;4&quot; 第三种情况是，在执行事务的 EXEC 命令时，Redis 实例发生了故障，导致事务执行失败。在这种情况下，如果 Redis 开启了 AOF 日志，那么，只会有部分的事务操作被记录到 AOF 日志中。我们需要使用 redis-check-aof 工具检查 AOF 日志文件，这个工具可以把未完成的事务操作从 AOF 文件中去除。这样一来，我们使用 AOF 恢复实例后，事务操作不会再被执行，从而保证了原子性。当然，如果 AOF 日志并没有开启，那么实例重启后，数据也都没法恢复了，此时，也就谈不上原子性了。 Redis 对事务原子性属性的保证情况小结如下： 命令入队时就报错，会放弃事务执行，保证原子性； 命令入队时没报错，实际执行时报错，不保证原子性； EXEC 命令执行时实例故障，如果开启了 AOF 日志，可以保证原子性。 一致性事务的一致性保证会受到错误命令、实例故障的影响。所以，我们按照命令出错和实例故障的发生时机，分成三种情况来看。 情况一：命令入队时就报错。在这种情况下，事务本身就会被放弃执行，所以可以保证数据库的一致性。 情况二：命令入队时没报错，实际执行时报错。在这种情况下，有错误的命令不会被执行，正确的命令可以正常执行，也不会改变数据库的一致性。 情况三：EXEC 命令执行时实例发生故障。在这种情况下，实例故障后会进行重启，这就和数据恢复的方式有关了，我们要根据实例是否开启了 RDB 或 AOF 来分情况讨论下。如果我们没有开启 RDB 或 AOF，那么，实例故障重启后，数据都没有了，数据库是一致的。如果我们使用了 RDB 快照，因为 RDB 快照不会在事务执行时执行，所以，事务命令操作的结果不会被保存到 RDB 快照中，使用 RDB 快照进行恢复时，数据库里的数据也是一致的。如果我们使用了 AOF 日志，而事务操作还没有被记录到 AOF 日志时，实例就发生了故障，那么，使用 AOF 日志恢复的数据库数据是一致的。如果只有部分操作被记录到了 AOF 日志，我们可以使用 redis-check-aof 清除事务中已经完成的操作，数据库恢复后也是一致的。 所以，总结来说，在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的。 隔离性事务的隔离性保证，会受到和事务一起执行的并发操作的影响。而事务执行又可以分成命令入队（EXEC 命令执行前）和命令实际执行（EXEC 命令执行后）两个阶段，所以，我们就针对这两个阶段，分成两种情况来分析： 并发操作在 EXEC 命令前执行，此时，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证； 并发操作在 EXEC 命令后执行，此时，隔离性可以保证。 WATCH 机制的作用是，在事务执行前，监控一个或多个键的值变化情况，当事务调用 EXEC 命令执行时，WATCH 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。然后，客户端可以再次执行事务，此时，如果没有并发修改事务数据的操作了，事务就能正常执行，隔离性也得到了保证。 WATCH 机制的具体实现是由 WATCH 命令实现的，我给你举个例子，你可以看下下面的图，进一步理解下 WATCH 命令的使用： 当然，如果没有使用 WATCH 机制，在 EXEC 命令前执行的并发操作是会对数据进行读写的。而且，在执行 EXEC 命令的时候，事务要操作的数据已经改变了，在这种情况下，Redis 并没有做到让事务对其它操作隔离，隔离性也就没有得到保障。 刚刚说的是并发操作在 EXEC 命令前执行的情况，下面我再来说一说第二种情况：并发操作在 EXEC 命令之后被服务器端接收并执行。 因为 Redis 是用单线程执行命令，而且，EXEC 命令执行后，Redis 会保证先把命令队列中的所有命令执行完。所以，在这种情况下，并发操作不会破坏事务的隔离性。 持久性因为 Redis 是内存数据库，所以，数据是否持久化保存完全取决于 Redis 的持久化配置模式。如果 Redis 没有使用 RDB 或 AOF，那么事务的持久化属性肯定得不到保证。如果 Redis 使用了 RDB 模式，那么，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发生了实例宕机，这种情况下，事务修改的数据也是不能保证持久化的。如果 Redis 采用了 AOF 模式，因为 AOF 模式的三种配置选项 no、everysec 和 always 都会存在数据丢失的情况，所以，事务的持久性属性也还是得不到保证。 所以，不管 Redis 采用什么持久化模式，事务的持久性属性是得不到保证的。 小结 事务的 ACID 属性是我们使用事务进行正确操作的基本要求。通过这节课的分析，我们了解到了，Redis 的事务机制可以保证一致性和隔离性，但是无法保证持久性。不过，因为 Redis 本身是内存数据库，持久性并不是一个必须的属性，我们更加关注的还是原子性、一致性和隔离性这三个属性。原子性的情况比较复杂，只有当事务中使用的命令语法有误时，原子性得不到保证，在其它情况下，事务都可以原子性执行。 关于 Pipeline 和 WATCH 命令的使用： 如果不使用 Pipeline，客户端是先发一个 MULTI 命令到服务端，客户端收到 OK，然后客户端再发送一个个操作命令，客户端依次收到 QUEUED，最后客户端发送 EXEC 执行整个事务（文章例子就是这样演示的），这样消息每次都是一来一回，效率比较低，而且在这多次操作之间，别的客户端可能就把原本准备修改的值给修改了，所以无法保证隔离性。 而使用 Pipeline 是一次性把所有命令打包好全部发送到服务端，服务端全部处理完成后返回。这么做好的好处，一是减少了来回网络 IO 次数，提高操作性能。二是一次性发送所有命令到服务端，服务端在处理过程中，是不会被别的请求打断的（Redis单线程特性，此时别的请求进不来），这本身就保证了隔离性。我们平时使用的 Redis SDK 在使用开启事务时，一般都会默认开启 Pipeline 的，可以留意观察一下。 关于 WATCH 命令的使用场景： 在上面 1-a 场景中，也就是使用了事务命令，但没有配合 Pipeline 使用，如果想要保证隔离性，需要使用 WATCH 命令保证，也就是文章中讲 WATCH 的例子。但如果是 1-b 场景，使用了 Pipeline 一次发送所有命令到服务端，那么就不需要使用 WATCH 了，因为服务端本身就保证了隔离性。 如果事务 + Pipeline 就可以保证隔离性，那 WATCH 还有没有使用的必要？答案是有的。对于一个资源操作为读取、修改、写回这种场景，如果需要保证事物的原子性，此时就需要用到 WATCH 了。例如想要修改某个资源，但需要事先读取它的值，再基于这个值进行计算后写回，如果在这期间担心这个资源被其他客户端修改了，那么可以先 WATCH 这个资源，再读取、修改、写回，如果写回成功，说明其他客户端在这期间没有修改这个资源。如果其他客户端修改了这个资源，那么这个事务操作会返回失败，不会执行，从而保证了原子性。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/tags/Redis/"},{"name":"实践三","slug":"实践三","permalink":"https://jjw-story.github.io/tags/实践三/"}],"author":"JJW"},{"title":"Redis-实践二","slug":"Redis-实践二","date":"2021-03-25T05:42:00.000Z","updated":"2021-04-09T06:54:09.379Z","comments":true,"path":"2021/03/25/Redis-实践二/","link":"","permalink":"https://jjw-story.github.io/2021/03/25/Redis-实践二/","excerpt":"","text":"旁路策略Redis 是一个独立的系统软件，和业务应用程序是两个软件，当我们部署了 Redis 实例后，它只会被动地等待客户端发送请求，然后再进行处理。所以，如果应用程序想要使用Redis 缓存，我们就要在程序中增加相应的缓存操作代码。所以，我们也把 Redis 称为旁路缓存，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。 缓存的类型只读缓存当 Redis 用作只读缓存时，应用要读取数据的话，会先调用 Redis GET 接口，查询数据是否存在。而所有的数据写请求，会直接发往后端的数据库，在数据库中增删改。对于删改的数据来说，如果 Redis 已经缓存了相应的数据，应用需要把这些缓存的数据删除，Redis中就没有这些数据了。当应用再次读取这些数据时，会发生缓存缺失，应用会把这些数据从数据库中读出来，并写到缓存中。这样一来，这些数据后续再被读取时，就可以直接从缓存中获取了，能起到加速访问的效果。 只读缓存直接在数据库中更新数据的好处是，所有最新的数据都在数据库中，而数据库是提供数据可靠性保障的，这些数据不会有丢失的风险。当我们需要缓存图片、短视频这些用户只读的数据时，就可以使用只读缓存这个类型了。 读写缓存对于读写缓存来说，除了读请求会发送到缓存进行处理（直接在缓存中查询数据是否存在)，所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。此时，得益于Redis 的高性能访问特性，数据的增删改操作可以在缓存中快速完成，处理结果也会快速返回给业务应用，这就可以提升业务应用的响应速度。但是，和只读缓存不一样的是，在使用读写缓存时，最新的数据是在 Redis 中，而 Redis是内存数据库，一旦出现掉电或宕机，内存中的数据就会丢失。这也就是说，应用的最新数据可能会丢失，给应用业务带来风险。 根据业务应用对数据可靠性和缓存性能的不同要求，我们会有同步直写和异步写回两种策略。其中，同步直写策略优先保证数据可靠性，而异步写回策略优先提供快速响应。学习了解这两种策略，可以帮助我们根据业务需求，做出正确的设计选择。 同步直写策略，写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据，才给客户端返回。这样，即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证。不过，同步直写会降低缓存的访问性能。这是因为缓存中处理写请求的速度是很快的，而数据库处理写请求的速度较慢。即使缓存很快地处理了写请求，也需要等待数据库处理完所有的写请求，才能给应用返回结果，这就增加了缓存的响应延迟。 异步写回策略，则是优先考虑了响应延迟。此时，所有写请求都先在缓存中处理。等到这些增改的数据要被从缓存中淘汰出来时，缓存将它们写回后端数据库。这样一来，处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。 缓存淘汰为了保证较高的性价比，缓存的空间容量必然要小于后端数据库的数据总量。不过，内存大小毕竟有限，随着要缓存的数据量越来越大，有限的缓存空间不可避免地会被写满。此时，该怎么办呢？解决这个问题就涉及到缓存系统的一个重要机制，即缓存数据的淘汰机制。简单来说，数据淘汰机制包括两步：第一，根据一定的策略，筛选出对应用访问来说“不重要”的数据；第二，将这些数据从缓存中删除，为新来的数据腾出空间， Redis 缓存淘汰策略？Redis 4.0 之前一共实现了 6 种内存淘汰策略，在 4.0 之后，又增加了 2 种策略。我们可以按照是否会进行数据淘汰把它们分成两类： 进行数据淘汰的策略，只有 noeviction 这一种。 会进行淘汰的 7 种其他策略。 会进行淘汰的 7 种策略，我们可以再进一步根据淘汰候选数据集的范围把它们分成两类： 在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile\u0002lru、volatile-lfu（Redis 4.0 后新增）四种。 在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis4.0 后新增）三种。 这 8 种策略的分类，画到了一张图里： noeviction 策略：默认情况下，Redis 在使用的内存空间超过 maxmemory 值时，并不会淘汰数据，也就是设定的 noeviction 策略。对应到 Redis 缓存，也就是指，一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误。Redis 用作缓存时，实际的数据集通常都是大于缓存容量的，总会有新的数据要写入缓存，这个策略本身不淘汰数据，也就不会腾出新的缓存空间，我们不把它用在 Redis 缓存中。 volatile-random、volatile-ttl、volatile-lru 和 volatile-lfu 这四种淘汰策略。它们筛选的候选数据范围，被限制在已经设置了过期时间的键值对上。也正因为此，即使缓存没有写满，这些数据如果过期了，也会被删除。我们使用 EXPIRE 命令对一键值对设置了过期时间后，无论是这些键值对的过期时间是快到了，还是 Redis 的内存使用量达到了 maxmemory 阈值，Redis 都会进一步按照 volatile-ttl、volatile-random、volatile-lru、volatile-lfu 这四种策略的具体筛选规则进行淘汰。 volatile-ttl 在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。 volatile-random 就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。 volatile-lru 会使用 LRU 算法筛选设置了过期时间的键值对。 volatile-lfu 会使用 LFU 算法选择设置了过期时间的键值对。 相对于 volatile-ttl、volatile-random、volatile-lru、volatile-lfu 这四种策略淘汰的是设置了过期时间的数据，allkeys-lru、allkeys-random、allkeys-lfu 这三种淘汰策略的备选淘汰数据范围，就扩大到了所有键值对，无论这些键值对是否设置了过期时间。它们筛选数据进行淘汰的规则是： allkeys-random 策略，从所有键值对中随机选择并删除数据； allkeys-lru 策略，使用 LRU 算法在所有数据中进行筛选。 allkeys-lfu 策略，使用 LFU 算法在所有数据中进行筛选。 这也就是说，如果一个键值对被删除策略选中了，即使它的过期时间还没到，也需要被删除。当然，如果它的过期时间到了但未被策略选中，同样也会被删除。 LRU 算法LRU 算法的全称是 Least Recently Used，从名字上就可以看出，这是按照最近最少使用的原则来筛选数据，最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中。LRU 会把所有的数据组织成一个链表，链表的头和尾分别表示 MRU 端和 LRU 端，分别代表最近最常使用的数据和最近最不常用的数据。 LRU 算法背后的想法非常朴素：它认为刚刚被访问的数据，肯定还会被再次访问，所以就把它放在 MRU 端；长久不访问的数据，肯定就不会再被访问了，所以就让它逐渐后移到 LRU 端，在缓存满时，就优先删除它。 不过，LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且，当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。所以，在 Redis 中，LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构RedisObject 中的 lru 字段记录）。然后，Redis 在决定淘汰的数据时，第一次会随机选出N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把lru 字段值最小的数据从缓存中淘汰出去。 Redis 提供了一个配置参数 maxmemory-samples，这个参数就是 Redis 选出的数据个数N。例如，我们执行如下命令，可以让 Redis 选出 100 个数据作为候选数据集： 1CONFIG SET maxmemory-samples 100 当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 maxmemory\u0002samples，Redis 就把候选数据集中 lru 字段值最小的数据淘汰。这样一来，Redis 缓存不用为所有的数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能。 三个使用建议： 优先使用 allkeys-lru 策略。这样，可以充分利用 LRU 这一经典缓存算法的优势，把最近最常访问的数据留在缓存中，提升应用的访问性能。如果你的业务数据中有明显的冷热数据区分，我建议你使用 allkeys-lru 策略。 如果业务应用中的数据访问频率相差不大，没有明显的冷热数据区分，建议使用allkeys-random 策略，随机选择淘汰的数据就行。 如果你的业务中有置顶的需求，比如置顶新闻、置顶视频，那么，可以使用 volatile-lru策略，同时不给这些置顶数据设置过期时间。这样一来，这些需要置顶的数据一直不会被删除，而其他数据会在过期时根据 LRU 规则进行筛选。 总结Redis 4.0 版本以后一共提供了 8 种数据淘汰策略，从淘汰数据的候选集范围来看，我们有两种候选范围：一种是所有数据都是候选集，一种是设置了过期时间的数据是候选集。另外，无论是面向哪种候选数据集进行淘汰数据选择，我们都有三种策略，分别是随机选择，根据 LRU 算法选择，以及根据 LFU 算法选择。当然，当面向设置了过期时间的数据集选择淘汰数据时，我们还可以根据数据离过期时间的远近来决定。 一般来说，缓存系统对于选定的被淘汰数据，会根据其是干净数据还是脏数据，选择直接删除还是写回数据库。但是，在 Redis 中，被淘汰数据无论干净与否都会被删除，所以，这是我们在使用 Redis 缓存时要特别注意的：当数据修改成为脏数据时，需要在数据库中也把数据修改过来。 当然，设置缓存容量的大小也很重要，我的建议是：结合实际应用的数据总量、热数据的体量，以及成本预算，把缓存空间大小设置在总数据量的 15% 到 30% 这个区间就可以。 解决缓存和数据库的数据不一致缓存和数据库的数据不一致是如何发生的？读写缓存对于读写缓存来说，如果要对数据进行增删改，就需要在缓存中进行，同时还要根据采取的写回策略，决定是否同步写回到数据库中。 同步直写策略：写缓存时，也同步写数据库，缓存和数据库中的数据一致； 异步写回策略：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障，那么，此时，数据库就没有最新的数据了。 所以，对于读写缓存来说，要想保证缓存和数据库中的数据一致，就要采用同步直写策略。不过，需要注意的是，如果采用这种策略，就需要同时更新缓存和数据库。所以，我们要在业务应用中使用事务机制，来保证缓存和数据库的更新具有原子性，也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则，我们就无法实现同步直写。 只读缓存对于只读缓存来说，如果有数据新增，会直接写入数据库；而有数据删改时，就需要把只读缓存中的数据标记为无效。这样一来，应用后续再访问这些增删改的数据时，因为缓存中没有相应的数据，就会发生缓存缺失。此时，应用再从数据库中把数据读入缓存，这样后续再访问数据时，就能够直接从缓存中读取了。 那么，这个过程中会不会出现数据不一致的情况呢？考虑到新增数据和删改数据的情况不一样，所以我们分开来看。 新增数据：如果是新增数据，数据会直接写到数据库中，不用对缓存做任何操作，此时，缓存中本身就没有新增数据，而数据库中是最新值，此时，缓存和数据库的数据是一致的。 删改数据：如下图： 如何解决数据不一致问题？重试机制具体来说，可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中（例如使用 Kafka 消息队列）。当应用没有能够成功地删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或更新。如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作，此时，我们也可以保证数据库和缓存的数据一致了。否则的话，我们还需要再次进行重试。如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了。 上述过程是在更新数据库和删除缓存值的过程中，其中一个操作失败的情况，实际上，即使这两个操作第一次执行时都没有失败，当有大量并发请求时，应用还是有可能读到不一致的数据。 情况一：先删除缓存，再更新数据库假设线程 A 删除缓存值后，还没有来得及更新数据库（比如说有网络延迟），线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取。这会带来两个问题：线程 B 读取到了旧值；线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值。 解决方案：在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作。我们也把它叫做“延迟双删”。下面的这段伪代码就是“延迟双删”方案的示例： 1234redis.delKey(X)db.update(X)Thread.sleep(N)redis.delKey(X) 情况二：先更新数据库值，再删除缓存值如果线程 A 删除了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。不过，在这种情况下，如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。而且，线程 A 一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存缺失，进而从数据库中读取最新值。所以，这种情况对业务的影响较小。 好了，到这里，我们了解到了，缓存和数据库的数据不一致一般是由两个原因导致的，我给你提供了相应的解决方案。 删除缓存值或更新数据库失败而导致数据不一致，你可以使用重试机制确保删除或更新操作成功。 在删除缓存值、更新数据库的这两步操作中，有其他线程的并发读操作，导致其他线程读取到旧值，应对方案是延迟双删。 小结 缓存雪崩、击穿、穿透问题缓存雪崩缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。缓存雪崩一般是由两个原因导致的，应对方案也有所不同； 缓存中有大量数据同时过期，导致大量请求无法得到处理具体来说，当数据保存在缓存中，并且设置了过期时间时，如果在某一个时刻，大量数据同时过期，此时，应用再访问这些数据的话，就会发生缓存缺失。紧接着，应用就会把请求发送给数据库，从数据库中读取数据。如果应用的并发请求量很大，那么数据库的压力也就很大，这会进一步影响到数据库的其他正常业务请求处理。 针对大量数据同时失效带来的缓存雪崩问题，我给你提供两种解决方案。 首先，我们可以避免给大量的数据设置相同的过期时间。如果业务层的确要求有些数据同时失效，你可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟），这样一来，不同数据的过期时间有所差别，但差别又不会太大，既避免了大量数据同时过期，同时也保证了这些数据基本在相近的时间失效，仍然能满足业务需求。 除了微调过期时间，我们还可以通过服务降级，来应对缓存雪崩。所谓的服务降级，是指发生缓存雪崩时，针对不同的数据采取不同的处理方式。 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息； 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。 这样一来，只有部分过期数据的请求会发送到数据库，数据库的压力就没有那么大了。 Redis 缓存实例发生故障宕机除了大量数据同时失效会导致缓存雪崩，还有一种情况也会发生缓存雪崩，那就是，Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。因为 Redis 实例发生了宕机，我们需要通过其他方法来应对缓存雪崩了。我给你提供两个建议。 一、在业务系统中实现服务熔断或请求限流机制。所谓的服务熔断，是指在发生缓存雪崩时，为了防止引发连锁的数据库雪崩，甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问。再具体点说，就是业务应用调用缓存接口时，缓存客户端并不把请求发给 Redis 缓存实例，而是直接返回，等到 Redis 缓存实例重新恢复服务后，再允许应用请求发送到缓存系统。服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大。为了尽可能减少这种影响，我们也可以进行请求限流。这里说的请求限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。 二、事前预防。通过主从节点的方式构建 Redis 缓存高可靠集群。如果 Redis 缓存的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。 缓存击穿缓存击穿，是发生在某个热点数据失效的场景下。和缓存雪崩相比，缓存击穿失效的数据数量要小很多，应对方法也不一样。缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时。 为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了。这样一来，对热点数据的访问请求，都可以在缓存中进行处理，而 Redis 数万级别的高吞吐量可以很好地应对大量的并发请求访问。 缓存穿透缓存穿透和雪崩、击穿问题不一样，缓存穿透发生时，数据也不在数据库中，这会同时给缓存和数据库带来访问压力。那么，缓存穿透会发生在什么时候呢？一般来说，有两种情况： 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据； 恶意攻击：专门访问数据库中没有的数据。 为了避免缓存穿透的影响，我来给你提供三种应对方案： 缓存空值或缺省值。（一旦发生缓存穿透，我们就可以针对查询的数据，在 Redis 中缓存一个空值或是和业务层协商确定的缺省值（例如，库存的缺省值可以设为 0）。紧接着，应用发送的后续请求再进行查询时，就可以直接从 Redis 中读取空值或缺省值，返回给业务应用了，避免了把大量请求发送给数据库处理，保持了数据库的正常运行。） 在请求入口的前端进行请求检测。(缓存穿透的一个原因是有大量的恶意请求访问不存在的数据，所以，一个有效的应对方案是在请求入口前端，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库。这样一来，也就不会出现缓存穿透问题了。) 使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。 小结 服务熔断、服务降级、请求限流这些方法都是属于“有损”方案，在保证数据库和整体系统稳定的同时，会对业务应用带来负面影响。例如使用服务降级时，有部分数据的请求就只能得到错误返回信息，无法正常处理。如果使用了服务熔断，那么，整个缓存系统的服务都被暂停了，影响的业务范围更大。而使用了请求限流机制后，整个业务系统的吞吐率会降低，能并发处理的用户请求会减少，会影响到用户体验。 所以，我给你的建议是，尽量使用预防式方案： 针对缓存雪崩，合理地设置数据过期时间，以及搭建高可靠缓存集群； 针对缓存击穿，在缓存访问非常频繁的热点数据时，不要设置过期时间； 针对缓存穿透，提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。 布隆过滤器布隆过滤器由一个初值都为 0 的 bit 数组和 N 个哈希函数组成，可以用来快速判断某个数据是否存在。当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记： 首先，使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值。 然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置。 最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作。 如果数据不存在（例如，数据库里没有写入数据），我们也就没有用布隆过滤器标记过数据，那么，bit 数组对应 bit 位的值仍然为 0。 当需要查询某个数据时，我们就执行刚刚说的计算过程，先得到这个数据在 bit 数组中对应的 N 个位置。紧接着，我们查看 bit 数组中这 N 个位置上的 bit 值。只要这 N 个 bit 值有一个不为 1，这就表明布隆过滤器没有对该数据做过标记，所以，查询的数据一定没有在数据库中保存。如下图： 正是基于布隆过滤器的快速检测特性，我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。这样一来，即使发生缓存穿透了，大量请求只会查询 Redis 和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。布隆过滤器可以使用 Redis 实现，本身就能承担较大的并发访问压力。 注意事项： 布隆过滤器会有误判：由于采用固定bit的数组，使用多个哈希函数映射到多个bit上，有可能会导致两个不同的值都映射到相同的一组bit上。虽然有误判，但对于业务没有影响，无非就是还存在一些穿透而已，但整体上已经过滤了大多数无效穿透请求。 布隆过滤器误判率和空间使用的计算：误判本质是因为哈希冲突，降低误判的方法是增加哈希函数 + 扩大整个bit数组的长度，但增加哈希函数意味着影响性能，扩大数组长度意味着空间占用变大，所以使用布隆过滤器，需要在误判率和性能、空间作一个平衡，具体的误判率是有一个计算公式可以推导出来的（比较复杂）。但我们在使用开源的布隆过滤器时比较简单，通常会提供2个参数：预估存入的数据量大小、要求的误判率，输入这些参数后，布隆过滤器会有自动计算出最佳的哈希函数数量和数组占用的空间大小，直接使用即可。 布隆过滤器可以放在缓存和数据库的最前面：把Redis当作布隆过滤器时（4.0提供了布隆过滤器模块，4.0以下需要引入第三方库），当用户产生业务数据写入缓存和数据库后，同时也写入布隆过滤器，之后当用户访问自己的业务数据时，先检查布隆过滤器，如果过滤器不存在，就不需要查询缓存和数据库了，可以同时降低缓存和数据库的压力。 Redis实现的布隆过滤器bigkey问题：Redis布隆过滤器是使用String类型实现的，存储的方式是一个bigkey，建议使用时单独部署一个实例，专门存放布隆过滤器的数据，不要和业务数据混用，否则在集群环境下，数据迁移时会导致Redis阻塞问题。 极端情况，如果 整个bit数组 都是1 或者大部分都是1的场景，这说明什么？ 说明布隆过期已经基本被填满了，也说明超出了布隆过滤器 一开始预期的大小，没错 布隆过滤器是需要事先预知 总容量大小与误判率预期的，否则就会出现 误判率极高 基本等于 没有作用的情况。 缓存污染在一些场景下，有些数据被访问的次数非常少，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存空间。这种情况，就是缓存污染。当缓存污染不严重时，只有少量数据占据缓存空间，此时，对缓存系统的影响不大。但是，缓存污染一旦变得严重后，就会有大量不再访问的数据滞留在缓存中。如果这时数据占满了缓存空间，我们再往缓存中写入新数据时，就需要先把这些数据逐步淘汰出缓存，这就会引入额外的操作时间开销，进而会影响应用的性能。 如何解决缓存污染问题？基础淘汰策略要解决缓存污染，我们也能很容易想到解决方案，那就是得把不会再被访问的数据筛选出来并淘汰掉。这样就不用等到缓存被写满以后，再逐一淘汰旧数据之后，才能写入新数据了。而哪些数据能留存在缓存中，是由缓存的淘汰策略决定的。 8 种数据淘汰策略吗？它们分别是 noeviction、volatile-random、volatile-ttl、volatile-lru、volatile-lfu、allkeys-lru、allkeys-random 和 allkeys-lfu 策略。 noeviction 策略是不会进行数据淘汰的。所以，它肯定不能用来解决缓存污染问题。 volatile-random 和 allkeys-random 这两种策略。它们都是采用随机挑选数据的方式，来筛选即将被淘汰的数据。既然是随机挑选，那么 Redis 就不会根据数据的访问情况来筛选数据。如果被淘汰的数据又被访问了，就会发生缓存缺失。也就是说，应用需要到后端数据库中访问这些数据，降低了应用的请求响应速度。所以，volatile-random 和 allkeys-random 策略，在避免缓存污染这个问题上的效果非常有限。 volatile-ttl 针对的是设置了过期时间的数据，把这些数据中剩余存活时间最短的筛选出来并淘汰掉。虽然 volatile-ttl 策略不再是随机选择淘汰数据了，但是剩余存活时间并不能直接反映数据再次访问的情况。所以，按照 volatile-ttl 策略淘汰数据，和按随机方式淘汰数据类似，也可能出现数据被淘汰后，被再次访问导致的缓存缺失问题。例外情况：业务应用在给数据设置过期时间的时候，就明确知道数据被再次访问的情况，并根据访问情况设置过期时间。此时，Redis 按照数据的剩余最短存活时间进行筛选，是可以把不会再被访问的数据筛选出来的，进而避免缓存污染。例如，业务部门知道数据被访问的时长就是一个小时，并把数据的过期时间设置为一个小时后。这样一来，被淘汰的数据的确是不会再被访问了。 小结：除了在明确知道数据被再次访问的情况下，volatile-ttl 可以有效避免缓存污染。在其他情况下，volatile-random、allkeys-random、volatile-ttl 这三种策略并不能应对缓存污染问题。 LRU 缓存策略LRU 策略的核心思想：如果一个数据刚刚被访问，那么这个数据肯定是热数据，还会被再次访问。Redis 中的 LRU 策略，会在每个数据对应的 RedisObject 结构体中设置一个 lru 字段，用来记录数据的访问时间戳。在进行数据淘汰时，LRU 策略会在候选数据集中淘汰掉 lru 字段值最小的数据（也就是访问时间最久的数据）。 所以，在数据被频繁访问的业务场景中，LRU 策略的确能有效留存访问时间最近的数据。而且，因为留存的这些数据还会被再次访问，所以又可以提升业务应用的访问速度。 但是，也正是因为只看数据的访问时间，使用 LRU 策略在处理扫描式单次查询操作时，无法解决缓存污染。所谓的扫描式单次查询操作，就是指应用对大量的数据进行一次全体读取，每个数据都会被读取，而且只会被读取一次。此时，因为这些被查询的数据刚刚被访问过，所以 lru 字段值都很大。 对于采用了 LRU 策略的 Redis 缓存来说，扫描式单次查询会造成缓存污染。 LFU 缓存策略LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数。当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。 和那些被频繁访问的数据相比，扫描式单次查询的数据因为不会被再次访问，所以它们的访问次数不会再增加。因此，LFU 策略会优先把这些访问次数低的数据淘汰出缓存。这样一来，LFU 策略就可以避免这些数据对缓存造成污染了。 LFU 策略具体实现方法为了避免操作链表的开销，Redis 在实现 LRU 策略时使用了两个近似方法： Redis 是用 RedisObject 结构来保存数据的，RedisObject 结构中设置了一个 lru 字段，用来记录数据的访问时间戳； Redis 并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选。 Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分。 ldt 值：lru 字段的前 16bit，表示数据的访问时间戳； counter 值：lru 字段的后 8bit，表示数据的访问次数。 总结一下：当 LFU 策略筛选数据时，Redis 会在候选集合中，根据数据 lru 字段的后 8bit 选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据 lru 字段的前 16bit 值大小，选择访问时间最久远的数据进行淘汰。 Redis 只使用了 8bit 记录数据的访问次数，而 8bit 记录的最大值是 255，这样可以吗？在实际应用中，一个数据可能会被访问成千上万次。如果每被访问一次，counter 值就加 1 的话，那么，只要访问次数超过了 255，数据的 counter 值就一样了。在进行数据淘汰时，LFU 策略就无法很好地区分并筛选这些数据，反而还可能会把不怎么访问的数据留存在了缓存中。 在实现 LFU 策略时，Redis 并没有采用数据每被访问一次，就给对应的 counter 值加 1 的计数规则，而是采用了一个更优化的计数规则。 简单来说，LFU 策略实现的计数规则是：每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1。 LFU 策略增加计数器值的计算逻辑源码如下： 1234double r = (double)rand()/RAND_MAX;...double p = 1.0/(baseval*server.lfu_log_factor+1);if (r &lt; p) count er++; 使用了这种计算规则后，我们可以通过设置不同的 lfu_log_factor 配置项，来控制计数器值增加的速度，避免 counter 值很快就到 255 了。我们在应用 LFU 策略时，一般可以将 lfu_log_factor 取值为 10。 Redis 在实现 LFU 策略时，还设计了一个counter 值的衰减机制。简单来说，LFU 策略使用衰减因子配置项 lfu_decay_time 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以 lfu_decay_time 值，所得的结果就是数据 counter 要衰减的值。 简单举个例子，假设 lfu_decay_time 取值为 1，如果数据在 N 分钟内没有被访问，那么它的访问次数就要减 N。如果 lfu_decay_time 取值更大，那么相应的衰减值会变小，衰减效果也会减弱。所以，如果业务应用中有短时高频访问的数据的话，建议把 lfu_decay_time 值设置为 1，这样一来，LFU 策略在它们不再被访问后，会较快地衰减它们的访问次数，尽早把它们从缓存中淘汰出去，避免缓存污染。 总结在实际业务应用中，LRU 和 LFU 两个策略都有应用。LRU 和 LFU 两个策略关注的数据访问特征各有侧重，LRU 策略更加关注数据的时效性，而 LFU 策略更加关注数据的访问频次。通常情况下，实际应用的负载具有较好的时间局部性，所以 LRU 策略的应用会更加广泛。但是，在扫描式查询的应用场景中，LFU 策略就可以很好地应对缓存污染问题了，建议你优先使用。 此外，如果业务应用中有短时高频访问的数据，除了 LFU 策略本身会对数据的访问次数进行自动衰减以外，我再给你个小建议：你可以优先使用 volatile-lfu 策略，并根据这些数据的访问时限设置它们的过期时间，以免它们留存在缓存中造成污染。 Redis 无锁的原子操作原子操作是另一种提供并发访问控制的方法。原子操作是指执行过程保持原子性的操作，而且原子操作执行时并不需要再加锁，实现了无锁操作。这样一来，既能保证并发控制，还能减少对系统并发性能的影响。 并发访问中需要对什么进行控制？我们说的并发访问控制，是指对多个客户端访问操作同一份数据的过程进行控制，以保证任何一个客户端发送的操作在 Redis 实例上执行时具有互斥性。例如，客户端 A 的访问操作在执行时，客户端 B 的操作不能执行，需要等到 A 的操作结束后，才能执行。 并发访问控制对应的操作主要是数据修改操作。当客户端需要修改数据时，基本流程分成两步： 客户端先把数据读取到本地，在本地进行修改； 客户端修改完数据后，再写回 Redis。 我们把这个流程叫做“读取 - 修改 - 写回”操作（Read-Modify-Write，简称为 RMW 操作）。当有多个客户端对同一份数据执行 RMW 操作的话，我们就需要让 RMW 操作涉及的代码以原子性方式执行。访问同一份数据的 RMW 操作代码，就叫做临界区代码。 在并发访问时，并发的 RMW 操作会导致数据错误，所以需要进行并发控制。所谓并发控制，就是要保证临界区代码的互斥执行。(出现这个现象的原因是，临界区代码中的客户端读取数据、更新数据、再写回数据涉及了三个操作，而这三个操作在执行时并不具有互斥性，多个客户端基于相同的初始值进行修改，而不是基于前一个客户端修改后的值再修改。) 为了保证数据并发修改的正确性，我们可以用锁把并行操作变成串行操作，串行操作就具有互斥性。一个客户端持有锁后，其他客户端只能等到锁释放，才能拿锁再进行修改。虽然加锁保证了互斥性，但是加锁也会导致系统并发性能降低。 Redis 的两种原子操作方法为了实现并发控制要求的临界区代码互斥执行，Redis 的原子操作采用了两种方法： 把多个操作在 Redis 中实现成一个操作，也就是单命令操作； 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。 Redis 本身的单命令操作Redis 是使用单线程来串行处理客户端的请求操作命令的，所以，当 Redis 执行某个命令操作时，其他命令是无法执行的，这相当于命令操作是互斥执行的。当然，Redis 的快照生成、AOF 重写这些操作，可以使用后台线程或者是子进程执行，也就是和主线程的操作并行执行。不过，这些操作只是读取数据，不会修改数据，所以，我们并不需要对它们做并发控制。 虽然 Redis 的单个命令操作可以原子性地执行，但是在实际应用中，数据修改时可能包含多个操作，至少包括读数据、数据增减、写回数据三个操作，这显然就不是单个命令操作了，那该怎么办呢？ 别担心，Redis 提供了 INCR/DECR 命令，把这三个操作转变为一个原子操作了。INCR/DECR 命令可以对数据进行增值 / 减值操作，而且它们本身就是单个命令操作，Redis 在执行它们时，本身就具有互斥性。 所以，如果我们执行的 RMW 操作是对数据进行增减值的话，Redis 提供的原子操作 INCR 和 DECR 可以直接帮助我们进行并发控制。但是，如果我们要执行的操作不是简单地增减数据，而是有更加复杂的判断逻辑或者是其他操作，那么，Redis 的单命令操作已经无法保证多个操作的互斥执行了。所以，这个时候，我们需要使用第二个方法，也就是 Lua 脚本。 Lua 脚本Redis 会把整个 Lua 脚本作为一个整体执行，在执行的过程中不会被其他命令打断，从而保证了 Lua 脚本中操作的原子性。如果我们有多个操作要执行，但是又无法用 INCR/DECR 这种命令操作来实现，就可以把这些要执行的操作编写到一个 Lua 脚本中。然后，我们可以使用 Redis 的 EVAL 命令来执行脚本。这样一来，这些操作在执行时就具有了互斥性。 例如我们可以用过如下命令实现一个例子： 当一个业务应用的访问用户增加时，我们有时需要限制某个客户端在一定时间范围内的访问次数，比如爆款商品的购买限流、社交网络中的每分钟点赞次数限制等。那该怎么限制呢？我们可以把客户端 IP 作为 key，把客户端的访问次数作为 value，保存到 Redis 中。客户端每访问一次后，我们就用 INCR 增加访问次数。客户端限流其实同时包含了对访问次数和时间范围的限制，例如每分钟的访问次数不能超过 20。所以，我们可以在客户端第一次访问时，给对应键值对设置过期时间，例如设置为 60s 后过期。同时，在客户端每次访问时，我们读取客户端当前的访问次数，如果次数超过阈值，就报错，限制客户端再次访问。你可以看下下面的这段代码，它实现了对客户端每分钟访问次数不超过 20 次的限制。 12345local currentcurrentcurrentcurrent = redis.call(\"incr\", KEYS[1])if tonumber(current) == 1 then redis.call(\"expire\",KEYS[1],60)end 假设我们编写的脚本名称为 lua.script，我们接着就可以使用 Redis 客户端，带上 eval 选项，来执行该脚本。脚本所需的参数将通过以下命令中的 keys 和 args 进行传递。 1redis-cli --eval lua.script keys , args 这样一来，访问次数加 1、判断访问次数是否为 1，以及设置过期时间这三个操作就可以原子性地执行了。即使客户端有多个线程同时执行这个脚本，Redis 也会依次串行执行脚本代码，避免了并发操作带来的数据错误。 lua脚本使用注意点： 1、lua 脚本尽量只编写通用的逻辑代码，避免直接写死变量。变量通过外部调用方传递进来，这样 lua 脚本的可复用度更高。 2、建议先使用SCRIPT LOAD命令把 lua 脚本加载到 Redis 中，然后得到一个脚本唯一摘要值，再通过EVALSHA命令 + 脚本摘要值来执行脚本，这样可以避免每次发送脚本内容到 Redis，减少网络开销。 总结在并发访问时，并发的 RMW 操作会导致数据错误，所以需要进行并发控制。所谓并发控制，就是要保证临界区代码的互斥执行。 Redis 提供了两种原子操作的方法来实现并发控制，分别是单命令操作和 Lua 脚本。因为原子操作本身不会对太多的资源限制访问，可以维持较高的系统并发性能。 但是，单命令原子操作的适用范围较小，并不是所有的 RMW 操作都能转变成单命令的原子操作（例如 INCR/DECR 命令只能在读取数据后做原子增减），当我们需要对读取的数据做更多判断，或者是我们对数据的修改不是简单的增减时，单命令操作就不适用了。 而 Redis 的 Lua 脚本可以包含多个操作，这些操作都会以原子性的方式执行，绕开了单命令操作的限制。不过，如果把很多操作都放在 Lua 脚本中原子执行，会导致 Redis 执行脚本的时间增加，同样也会降低 Redis 的并发性能。所以，我给你一个小建议：在编写 Lua 脚本时，你要避免把不需要做并发控制的操作写入脚本中。 Redis实现分布式锁现分布式锁的两个要求： 要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的原子性； 要求二：共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。 好了，知道了具体的要求，接下来，我们就来学习下 Redis 是怎么实现分布式锁的。 基于单个 Redis 节点实现分布式锁分布式锁是由共享存储系统维护的变量，多个客户端可以向共享存储系统发送命令进行加锁或释放锁操作。Redis 作为一个共享存储系统，可以用来实现分布式锁。 在基于单个 Redis 实例实现分布式锁时，对于加锁操作，我们需要满足三个条件: 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁； 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间； 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端。 Redis 的 SETNX 命令它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。对于释放锁操作来说，我们可以在执行完业务逻辑后，使用 DEL 命令删除锁变量。 1SETNX key value 但是SETNX命令不能给锁变量设置一个过期时间，如果需要设置则需要单独的命令，这样就没有办法保证操作的原子性了。 Redis 的 SET 命令SETNX 命令对于不存在的键值对，它会先创建再设置值（也就是“不存在即设置”），为了能达到和 SETNX 命令一样的效果，Redis 给 SET 命令提供了类似的选项 NX，用来实现“不存在即设置”。如果使用了 NX 选项，SET 命令只有在键值对不存在时，才会进行设置，否则不做赋值操作。此外，SET 命令在执行时还可以带上 EX 或 PX 选项，用来设置键值对的过期时间。 举个例子，执行下面的命令时，只有 key 不存在时，SET 才会创建 key，并对 key 进行赋值。另外，key 的存活时间由 seconds 或者 milliseconds 选项值来决定。 1SET key value [EX seconds | PX milliseconds] [NX] 有了 SET 命令的 NX 和 EX/PX 选项后，我们就可以用下面的命令来实现加锁操作了。 12// 加锁, unique_value作为客户端唯一性的标识SET lock_key unique_value NX PX 10000 因为在加锁操作中，每个客户端都使用了一个唯一标识，所以在释放锁操作时，我们需要判断锁变量的值，是否等于执行释放锁操作的客户端的唯一标识，如下所示： 123456//释放锁 比较unique_value是否相等，避免误释放if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1])else return 0end 这是使用 Lua 脚本（unlock.script）实现的释放锁操作的伪代码，其中，KEYS[1]表示 lock_key，ARGV[1]是当前客户端的唯一标识，这两个值都是我们在执行 Lua 脚本时作为参数传入的。最后，我们执行下面的命令，就可以完成锁释放操作了: 1redis-cli --eval unlock.script lock_key , unique_value 你可能也注意到了，在释放锁操作中，我们使用了 Lua 脚本，这是因为，释放锁操作的逻辑也包含了读取锁变量、判断值、删除锁变量的多个操作，而 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。 基于多个 Redis 节点实现高可靠的分布式锁到这里，你了解了如何使用 SET 命令和 Lua 脚本在 Redis 单节点上实现分布式锁。但是，我们现在只用了一个 Redis 实例来保存锁变量，如果这个 Redis 实例发生故障宕机了，那么锁变量就没有了。此时，客户端也无法进行锁操作了，这就会影响到业务的正常执行。所以，我们在实现分布式锁时，还需要保证锁的可靠性。那怎么提高呢？这就要提到基于多个 Redis 节点实现分布式锁的方式了。 为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock。 Redlock 算法的基本思路，是让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。 Redlock 算法的执行步骤。Redlock 算法的实现需要有 N 个独立的 Redis 实例。接下来，我们可以分成 3 步来完成加锁操作： 客户端获取当前时间。 客户端按顺序依次向 N 个 Redis 实例执行加锁操作。（这里的加锁操作和在单实例上执行的加锁操作一样，使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。） 一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。 客户端只有在满足下面的这两个条件时，才能认为是加锁成功： 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁；条件二：客户端获取锁的总耗时没有超过锁的有效时间。 条件二：客户端获取锁的总耗时没有超过锁的有效时间。 在满足了这两个条件后，我们需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。 当然，如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有 Redis 节点发起释放锁的操作。在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。 总结 - Redis 分布式锁可靠性的问题 使用单个 Redis 节点（只有一个master）使用分布锁，如果实例宕机，那么无法进行锁操作了。那么采用主从集群模式部署是否可以保证锁的可靠性？ 答案是也很难保证。如果在 master 上加锁成功，此时 master 宕机，由于主从复制是异步的，加锁操作的命令还未同步到 slave，此时主从切换，新 master 节点依旧会丢失该锁，对业务来说相当于锁失效了。所以 Redis 作者才提出基于多个 Redis 节点（master节点）的 Redlock 算法。 Redlock 算法存在的问题： Redlock 这个算法涉及的细节很多，作者在提出这个算法时，业界的分布式系统专家还与 Redis 作者发生过一场争论，来评估这个算法的可靠性，争论的细节都是关于异常情况可能导致 Redlock 失效的场景，例如加锁过程中客户端发生了阻塞、机器时钟发生跳跃等等（http://zhangtielei.com/posts/blog-redlock-reasoning.html）。 又比如：假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。客户端1在获得锁之后发生了很长时间的GC pause，在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。 简单总结，基于 Redis 使用分布锁的注意点： 使用 SET $lock_key $unique_val EX $second NX 命令保证加锁原子性，并为锁设置过期时间。 锁的过期时间要提前评估好，要大于操作共享资源的时间。 每个线程加锁时设置随机值，释放锁时判断是否和加锁设置的值一致，防止自己的锁被别人释放。 释放锁时使用 Lua 脚本，保证操作的原子性。 基于多个节点的 Redlock，加锁时超过半数节点操作成功，并且获取锁的耗时没有超过锁的有效时间才算加锁成功。 Redlock 释放锁时，要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉。 使用 Redlock 时要避免机器时钟发生跳跃，需要运维来保证，对运维有一定要求，否则可能会导致 Redlock 失效。例如共 3 个节点，线程 A 操作 2 个节点加锁成功，但其中 1 个节点机器时钟发生跳跃，锁提前过期，线程 B 正好在另外 2 个节点也加锁成功，此时 Redlock 相当于失效了（Redis 作者和分布式系统专家争论的重要点就在这）。 如果为了效率，使用基于单个 Redis 节点的分布式锁即可，此方案缺点是允许锁偶尔失效，优点是简单效率高。 如果是为了正确性，业务对于结果要求非常严格，建议使用 Redlock，但缺点是使用比较重，部署成本高。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/tags/Redis/"},{"name":"实践二","slug":"实践二","permalink":"https://jjw-story.github.io/tags/实践二/"}],"author":"JJW"},{"title":"Redis-实践一","slug":"Redis-实践一","date":"2021-03-16T01:28:10.000Z","updated":"2021-03-25T07:18:58.052Z","comments":true,"path":"2021/03/16/Redis-实践一/","link":"","permalink":"https://jjw-story.github.io/2021/03/16/Redis-实践一/","excerpt":"","text":"String 数据结构详解String 类型并不是适用于所有场合的，它有一个明显的短板，就是它保存数据时所消耗的内存空间较多。集合类型有非常节省内存空间的底层实现结构，但是，集合类型保存的数据模式，是一个键对应一系列值，并不适合直接保存单值的键值对。 为什么 String 类型内存开销大？SDS 和 INT除了记录实际数据，String 类型还需要额外的内存空间记录数据长度、空间使用等信息，这些信息也叫作元数据。当实际保存的数据较小时，元数据的空间开销就显得比较大了，有点“喧宾夺主”的意思。 当你保存 64 位有符号整数时，String 类型会把它保存为一个 8 字节的 Long 类型整数，这种保存方式通常也叫作 int 编码 方式。但是，当你保存的数据中包含字符时，String 类型就会用简单动态字符串（SimpleDynamic String，SDS）结构体来保存： buf：字节数组，保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加一个“\\0”，这就会额外占用 1 个字节的开销。 len：占 4 个字节，表示 buf 的已用长度。 alloc：也占个 4 字节，表示 buf 的实际分配长度，一般大于 len。 可以看到，在 SDS 中，buf 保存实际数据，而 len 和 alloc 本身其实是 SDS 结构体的额外开销。 RedisObject对于 String 类型来说，除了 SDS 的额外开销，还有一个来自于 RedisObject 结构体的开销。因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针，这个指针再进一步指向具体数据类型的实际数据所在，例如指向 String 类型的 SDS 结构所在的内存地址，具体如下图： 为了节省内存空间，Redis 还对 Long 类型整数和 SDS 的内存布局做了专门的设计: 当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。(也就是没有 SDS 数据结构，直接就是 INT(8B) 类型) 当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被称为 embstr 编码方式。 当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。这种布局方式被称为 raw 编码模式。 全局哈希表(DictEntry)Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个 dictEntry 的结构体，用来指向一个键值对。dictEntry 结构中有三个 8 字节的指针，分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节，如下图所示： 内存分配库 jemalloc jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间；如果你申请 24 字节空间，jemalloc 则会分配 32 字节。所以，在我们刚刚说的场景里，dictEntry 结构就占用了 32 字节。 压缩列表（ziplist）数据结构详解Redis 有一种底层数据结构，叫压缩列表（ziplist），这是一种非常节省内存的结构。压缩列表的构成。表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量，以及列表中的 entry 个数。压缩列表尾还有一个 zlend，表示列表结束。如下图: 压缩列表之所以能节省内存，就在于它是用一系列连续的 entry 保存数据。每个 entry 的元数据包括下面几部分: prev_len，表示前一个 entry 的长度。prev_len 有两种取值情况：1 字节或 5 字节。取值 1 字节时，表示上一个 entry 的长度小于 254 字节。虽然 1 字节的值能表示的数值范围是 0 到 255，但是压缩列表中 zlend 的取值默认是 255，因此，就默认用 255表示整个压缩列表的结束，其他表示长度的地方就不能再用 255 这个值了。所以，当上一个 entry 长度小于 254 字节时，prev_len 取值为 1 字节，否则，就取值为 5 字节。 len：表示自身长度，4 字节； encoding：表示编码方式，1 字节； content：保存实际数据。 这些 entry 会挨个儿放置在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。Redis 基于压缩列表实现了 List、Hash 和 Sorted Set 这样的集合类型，这样做的最大好处就是节省了 dictEntry 的开销。当你用 String 类型时，一个键值对就有一个 dictEntry，要用 32 字节空间。但采用集合类型时，一个 key 就对应一个集合的数据，能保存的数据多了很多，但也只用了一个 dictEntry，这样就节省了内存。 Hash 类型底层结构什么时候使用压缩列表?Redis Hash 类型的两种底层实现结构，分别是压缩列表和哈希表。什么时候使用哈希表呢？其实，Hash类型设置了用压缩列表保存数据时的两个阈值，一旦超过了阈值，Hash 类型就会用哈希表来保存数据了。这两个阈值分别对应以下两个配置项： hash-max-ziplist-entries：表示用压缩列表保存时哈希集合中的最大元素个数。 hash-max-ziplist-value：表示用压缩列表保存时哈希集合中单个元素的最大长度。 如果我们往 Hash 集合中写入的元素个数超过了 hash-max-ziplist-entries，或者写入的单个元素大小超过了 hash-max-ziplist-value，Redis 就会自动把 Hash 类型的实现结构由压缩列表转为哈希表。一旦从压缩列表转为了哈希表，Hash 类型就会一直用哈希表进行保存，而不会再转回压缩列表了。在节省内存空间方面，哈希表就没有压缩列表那么高效了。 注意：不管是使用Hash还是Sorted Set，当采用ziplist方式存储时，虽然可以节省内存空间，但是在查询指定元素时，都要遍历整个ziplist，找到指定的元素。所以使用ziplist方式存储时，虽然可以利用CPU高速缓存，但也不适合存储过多的数据（hash-max-ziplist-entries和zset-max-ziplist-entries不宜设置过大），否则查询性能就会下降比较厉害。整体来说，这样的方案就是时间换空间，我们需要权衡使用。 当使用ziplist存储时，我们尽量存储int数据，ziplist在设计时每个entry都进行了优化，针对要存储的数据，会尽量选择占用内存小的方式存储（整数比字符串在存储时占用内存更小），这也有利于我们节省Redis的内存。还有，因为ziplist是每个元素紧凑排列，而且每个元素存储了上一个元素的长度，所以当修改其中一个元素超过一定大小时，会引发多个元素的级联调整（前面一个元素发生大的变动，后面的元素都要重新排列位置，重新分配内存），这也会引发性能问题，需要注意。 扩展类型(Bitmap, HyperLogLog, GEO)BitmapBitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态。你可以把 Bitmap 看作是一个 bit 数组。 Bitmap 提供了 GETBIT/SETBIT 操作，使用一个偏移值 offset 对 bit 数组的某一个 bit 位进行读和写。不过，需要注意的是，Bitmap 的偏移量是从 0 开始算的，也就是说 offset的最小值是 0。当使用 SETBIT 对一个 bit 位进行写操作时，这个 bit 位会被设置为 1。Bitmap 还提供了 BITCOUNT 操作，用来统计这个 bit 数组中所有“1”的个数。 用位数组来表示各元素是否出现，每个元素对应一位，所需的总内存为n bit。能大大减少内存占用且位操作迅速。 如果要统计1亿个数据的基数值，大约需要内存100000000/8/1024/1024 ≈ 12M，内存减少占用的效果显著。然而统计一个对象的基数值需要12M，如果统计10000个对象，就需要将近120G，同样不能广泛用于大数据场景。 使用示例： 12345# 记录用户ID为3000的用户在202008月份的打卡情况SETBIT uid:sign:3000:202008 2 1SETBIT uid:sign:3000:202008 92 1GETBIT uid:sign:3000:202008 2BITCOUNT uid:sign:3000:202008 基数统计（HyperLogLog）HyperLogLog 是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小。在 Redis 中，每个 HyperLogLog 只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数。你看，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。 HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。这也就意味着，你使用HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。 使用示例： 12PFADD page1:uv user1 user2 user3 user4 user5PFCOUNT page1:uv GEOGEO 类型的底层数据结构就是用 Sorted Set 来实现的。 GeoHash 的编码方法为了能高效地对经纬度进行比较，Redis 采用了业界广泛使用的 GeoHash 编码方法，这个方法的基本原理就是“二分区间，区间编码”。当我们要对一组经纬度进行 GeoHash 编码时，我们要先对经度和纬度分别编码，然后再把经纬度各自的编码组合成一个最终编码。 首先，我们来看下经度和纬度的单独编码过程。 对于一个地理位置信息来说，它的经度范围是[-180,180]。GeoHash 编码会把一个经度值编码成一个 N 位的二进制值，我们来对经度范围[-180,180]做 N 次的二分区操作，其中 N可以自定义。在进行第一次二分区时，经度范围[-180,180]会被分成两个子区间:[-180,0) 和[0,180]（我称之为左、右分区）。此时，我们可以查看一下要编码的经度值落在了左分区还是右分区。如果是落在左分区，我们就用 0 表示；如果落在右分区，就用 1 表示。这样一来，每做完一次二分区，我们就可以得到 1 位编码值。 然后，我们再对经度值所属的分区再做一次二分区，同时再次查看经度值落在了二分区后的左分区还是右分区，按照刚才的规则再做 1 位编码。当做完 N 次的二分区后，经度值就可以用一个 N bit 的数来表示了。 对纬度的编码方式，和对经度的一样，只是纬度的范围是[-90，90]。 当一组经纬度值都编完码后，我们再把它们的各自编码值组合在一起，组合的规则是：最终编码值的偶数位上依次是经度的编码值，奇数位上依次是纬度的编码值，其中，偶数位从 0 开始，奇数位从 1 开始。 到这里，我们就知道了，GEO 类型是把经纬度所在的区间编码作为 Sorted Set 中元素的权重分数，把和经纬度相关的车辆 ID 作为 Sorted Set 中元素本身的值保存下来，这样相邻经纬度的查询就可以通过编码值的大小范围查询来实现了。 使用方法： GEOADD 命令：用于把一组经纬度信息和相对应的一个 ID 记录到 GEO 类型集合中； GEORADIUS 命令：会根据输入的经纬度位置，查找以这个经纬度为中心的一定范围内的其他元素。当然，我们可以自己定义这个范围。 使用示例： 1234# 添加车辆位置信息GEOADD cars:locations 116.034579 39.030452 33# 查询指定地点5KM内的车辆，限制10个GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10 如果需要，我们可以二次开发，开发自定义的数据类型。 Redis异步机制Redis 实例有哪些阻塞点？和 Redis 实例交互的对象，以及交互时会发生的操作大致有如下几种： 客户端：网络 IO，键值对增删改查操作，数据库操作； 磁盘：生成 RDB 快照，记录 AOF 日志，AOF 日志重写； 主从节点：主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB文件； 切片集群实例：向其他实例传输哈希槽信息，数据迁移。 和客户端交互时的阻塞点网络 IO 有时候会比较慢，但是 Redis 使用了 IO 多路复用机制，避免了主线程一直处在等待网络连接或请求到来的状态，所以，网络 IO 不是导致 Redis 阻塞的因素。 键值对的增删改查操作是 Redis 和客户端交互的主要部分，也是 Redis 主线程执行的主要任务。所以，复杂度高的增删改查操作肯定会阻塞 Redis。 Redis 中涉及集合的操作复杂度通常为 O(N)，我们要在使用时重视起来。例如集合元素全量查询操作 HGETALL、SMEMBERS，以及集合的聚合统计操作，例如求交、并和差集。这些操作可以作为 Redis 的第一个阻塞点：集合全量查询和聚合操作。 集合自身的删除操作同样也有潜在的阻塞风险。删除操作的本质是要释放键值对占用的内存空间。释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序，所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞。（什么时候会释放大量内存，其实就是在删除大量键值对数据的时候，最典型的就是删除包含了大量元素的集合，也称为 bigkey 删除。） 在 Redis 的数据库级别操作中，清空数据库（例如 FLUSHDB 和 FLUSHALL 操作）必然也是一个潜在的阻塞风险，因为它涉及到删除和释放所有的键值对。所以，这就是 Redis 的第三个阻塞点：清空数据库。 和磁盘交互时的阻塞点Redis 开发者早已认识到磁盘 IO 会带来阻塞，所以就把 Redis 进一步设计为采用子进程的方式生成 RDB 快照文件，以及执行 AOF 日志重写操作。这样一来，这两个操作由子进程负责执行，慢速的磁盘 IO 就不会阻塞主线程了。但是，Redis 直接记录 AOF 日志时，会根据不同的写回策略对数据做落盘保存。一个同步写磁盘的操作的耗时大约是 1～2ms，如果有大量的写操作需要记录在 AOF 日志中，并同步写回的话，就会阻塞主线程了。这就得到了 Redis 的第四个阻塞点了：AOF 日志同步写。 主从节点交互时的阻塞点在主从集群中，主库需要生成 RDB 文件，并传输给从库。主库在复制的过程中，创建和传输 RDB 文件都是由子进程来完成的，不会阻塞主线程。但是，对于从库来说，它在接收了RDB 文件后，需要使用 FLUSHDB 命令清空当前数据库，这就正好撞上了刚才我们分析的第三个阻塞点。 此外，从库在清空当前数据库后，还需要把 RDB 文件加载到内存，这个过程的快慢和RDB 文件的大小密切相关，RDB 文件越大，加载过程越慢，所以，加载 RDB 文件就成为了 Redis 的第五个阻塞点。 切片集群实例交互时的阻塞点最后，当我们部署 Redis 切片集群时，每个 Redis 实例上分配的哈希槽信息需要在不同实例间进行传递，同时，当需要进行负载均衡或者有实例增删时，数据会在不同的实例间进行迁移。不过，哈希槽的信息量不大，而数据迁移是渐进式执行的，所以，一般来说，这两类操作对 Redis 主线程的阻塞风险不大。不过，如果你使用了 Redis Cluster 方案，而且同时正好迁移的是 bigkey 的话，就会造成主线程的阻塞，因为 Redis Cluster 使用了同步迁移（后面章节会具体分析）。 综上所述总结就是有五点： 集合全量查询和聚合操作；(关键路径) bigkey 删除；（非关键路径） 清空数据库；（非关键路径） AOF 日志同步写；（非关键路径） 从库加载 RDB 文件。（关键路径） 如果在主线程中执行这些操作，必然会导致主线程长时间无法服务其他请求。为了避免阻塞式操作，Redis 提供了异步线程机制。所谓的异步线程机制，就是指，Redis 会启动一些子线程，然后把一些任务交给这些子线程，让它们在后台完成，而不再由主线程来执行这些任务。使用异步线程机制执行操作，可以避免阻塞主线程。 所谓关键路径就是必须要执行完成才能进行下一步，或者需要执行完成返回给客户端。非关键路径既不影响主流程，可以异步的来执行，在接收到指令后直接给客户端返回成功，然后后台异步执行。 对于 Redis 的五大阻塞点来说，除了集合全量查询和聚合操作”和“从库加载 RDB 文件，其他三个阻塞点涉及的操作都不在关键路径上，所以，我们可以使用 Redis 的异步子线程机制来实现 bigkey 删除，清空数据库，以及 AOF 日志同步写。 异步的子线程机制Redis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行。 主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成。但实际上，这个时候删除还没有执行，等到后台子线程从任务队列中读取任务后，才开始实际删除键值对，并释放相应的内存空间。因此，我们把这种异步删除也称为惰性删除（lazy free）。此时，删除或清空操作不会阻塞主线程，这就避免了对主线程的性能影响。 当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这样主线程就不用一直等待 AOF 日志写完了。 异步的键值对删除和数据库清空操作是 Redis 4.0 后提供的功能，Redis 也提供了新的命令来执行这两个操作： 键值对删除：当你的集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，我建议你使用 UNLINK 命令。 清空数据库：可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让后台子线程异步地清空数据库，如下所示： 12FLUSHDB ASYNCFLUSHALL AYSNC 总结本节我们学习了 Redis 实例运行时的 4 大类交互对象：客户端、磁盘、主从库实例、切片集群实例。基于这 4 大类交互对象，我们梳理了会导致 Redis 性能受损的 5 大阻塞点，包括集合全量查询和聚合操作、bigkey 删除、清空数据库、AOF 日志同步写，以及从库加载 RDB 文件。 在这 5 大阻塞点中，bigkey 删除、清空数据库、AOF 日志同步写不属于关键路径操作，可以使用异步子线程机制来完成。Redis 在运行时会创建三个子线程，主线程会通过一个任务队列和三个子线程进行交互。子线程会根据任务的具体类型，来执行相应的异步操作。 不过，异步删除操作是 Redis 4.0 以后才有的功能，如果你使用的是 4.0 之前的版本，当你遇到 bigkey 删除时，我给你个小建议：先使用集合类型提供的 SCAN 命令读取数据，然后再进行删除。因为用 SCAN 命令可以每次只读取一部分数据并进行删除，这样可以避免一次性删除大量 key 给主线程带来的阻塞。例如，对于 Hash 类型的 bigkey 删除，你可以使用 HSCAN 命令，每次从 Hash 集合中获取一部分键值对（例如 200 个），再使用 HDEL 删除这些键值对，这样就可以把删除压力分摊到多次操作中，那么，每次删除操作的耗时就不会太长，也就不会阻塞主线程了。 集合全量查询和聚合操作、从库加载 RDB 文件是在关键路径上，无法使用异步操作来完成。对于这两个阻塞点，我也给你两个小建议： 集合全量查询和聚合操作：可以使用 SCAN 命令，分批读取数据，再在客户端进行聚合计算； 从库加载 RDB 文件：把主库的数据量大小控制在 2~4GB 左右，以保证 RDB 文件能以较快的速度加载。 CPU结构对Redis性能的影响主流的 CPU 架构一个 CPU 处理器中一般有多个运行核心，我们把一个运行核心称为一个物理核，每个物理核都可以运行应用程序。每个物理核都拥有私有的一级缓存（Level 1 cache，简称 L1cache），包括一级指令缓存和一级数据缓存，以及私有的二级缓存（Level 2 cache，简称 L2 cache）。物理核的私有缓存：它其实是指缓存空间只能被当前的这个物理核使用，其他的物理核无法对这个核的缓存空间进行数据存取。当数据或指令保存在 L1、L2 缓存时，物理核访问它们的延迟不超过 10 纳秒，速度非常快。那么，如果 Redis 把要运行的指令或存取的数据保存在 L1 和 L2 缓存的话，就能高速地访问这些指令和数据。但是，这些 L1 和 L2 缓存的大小受限于处理器的制造技术，一般只有 KB 级别，存不下太多的数据。如果 L1、L2 缓存中没有所需的数据，应用程序就需要访问内存来获取数据。而应用程序的访存延迟一般在百纳秒级别，是访问 L1、L2 缓存的延迟的近 10 倍，不可避免地会对性能造成影响。 不同的物理核还会共享一个共同的三级缓存（Level 3 cache，简称为 L3 cache）。L3 缓存能够使用的存储资源比较多，所以一般比较大，能达到几 MB 到几十 MB，这就能让应用程序缓存更多的数据。当 L1、L2 缓存中没有数据缓存时，可以访问 L3，尽可能避免访问内存。 现在主流的 CPU 处理器中，每个物理核通常都会运行两个超线程，也叫作逻辑核。同一个物理核的逻辑核会共享使用 L1、L2 缓存。具体如下图： 在主流的服务器上，一个 CPU 处理器会有 10 到 20 多个物理核。同时，为了提升服务器的处理能力，服务器上通常还会有多个 CPU 处理器（也称为多 CPU Socket），每个处理器有自己的物理核（包括 L1、L2 缓存），L3 缓存，以及连接的内存，同时，不同处理器间通过总线连接。下图显示的就是多 CPU Socket 的架构： 我们也把这个架构称为非统一内存访问架构（Non-Uniform MemoryAccess，NUMA 架构）。 在多 CPU 架构上，应用程序可以在不同的处理器上运行。如果应用程序先在一个 Socket 上运行，并且把数据保存到了内存，然后被调度到另一个 Socket 上运行，此时，应用程序再进行内存访问时，就需要访问之前 Socket 上连接的内存，这种访问属于远端内存访问。和访问 Socket 直接连接的内存相比，远端内存访问会增加应用程序的延迟。 CPU 架构对应用程序运行的影响 L1、L2 缓存中的指令和数据的访问速度很快，所以，充分利用 L1、L2 缓存，可以有效缩短应用程序的执行时间； 在 NUMA 架构下，如果应用程序从一个 Socket 上调度到另一个 Socket 上，就可能会出现远端内存访问的情况，这会直接增加应用程序的执行时间。 总结在多核 CPU 架构下，Redis 如果在不同的核上运行，就需要频繁地进行上下文切换，这个过程会增加 Redis 的执行时间，客户端也会观察到较高的尾延迟了。所以，建议你在Redis 运行时，把实例和某个核绑定，这样，就能重复利用核上的 L1、L2 缓存，可以降低响应延迟。 为了提升 Redis 的网络性能，我们有时还会把网络中断处理程序和 CPU 核绑定。在这种情况下，如果服务器使用的是 NUMA 架构，Redis 实例一旦被调度到和中断处理程序不在同一个 CPU Socket，就要跨 CPU Socket 访问网络数据，这就会降低 Redis 的性能。所以，我建议你把 Redis 实例和网络中断处理程序绑在同一个 CPU Socket 下的不同核上，这样可以提升 Redis 的运行性能。 虽然绑核可以帮助 Redis 降低请求执行时间，但是，除了主线程，Redis 还有用于 RDB 和AOF 重写的子进程，以及 4.0 版本之后提供的用于惰性删除的后台线程。当 Redis 实例和一个逻辑核绑定后，这些子进程和后台线程会和主线程竞争 CPU 资源，也会对 Redis 性能造成影响。所以，我给了你两个建议： 如果你不想修改 Redis 代码，可以把按一个 Redis 实例一个物理核方式进行绑定，这样，Redis 的主线程、子进程和后台线程可以共享使用一个物理核上的两个逻辑核。 如果你很熟悉 Redis 的源码，就可以在源码中增加绑核操作，把子进程和后台线程绑到不同的核上，这样可以避免对主线程的 CPU 资源竞争。不过，如果你不熟悉 Redis 源码，也不用太担心，Redis 6.0 出来后，可以支持 CPU 核绑定的配置操作了。 响应延迟问题排查在 Redis 的实际部署应用中，有一个非常严重的问题，那就是 Redis 突然变慢了。 查看 Redis 的响应延迟基于当前环境下的 Redis 基线性能：所谓的基线性能呢，也就是一个系统在低压力、无干扰下的基本性能，这个性能只由当前的软硬件配置决定。 从 2.8.7 版本开始，redis-cli 命令提供了 –intrinsic-latency 选项，可以用来监测和统计测试期间内的最大延迟，这个延迟可以作为 Redis 的基线性能。其中，测试时长可以用–intrinsic-latency 选项的参数来指定。需要注意的是，基线性能和当前的操作系统、硬件配置相关。因此，我们可以把它和Redis 运行时的延迟结合起来，再进一步判断 Redis 性能是否变慢了。一般来说，你要把运行时延迟和基线性能进行对比，如果你观察到的 Redis 运行时延迟是其基线性能的 2 倍及以上，就可以认定 Redis 变慢了。 为了避免网络对基线性能的影响，刚刚说的这个命令需要在服务器端直接运行，这也就是说，我们只考虑服务器端软硬件环境的影响。 如何应对 Redis 变慢？Redis 自身操作特性的影响慢查询命令慢查询命令，就是指在 Redis 中执行速度慢的命令，这会导致 Redis 延迟增加。Redis 提供的命令操作很多，并不是所有命令都慢，这和命令操作的复杂度有关。所以，我们必须要知道 Redis 的不同命令的复杂度。 比如说，Value 类型为 String 时，GET/SET 操作主要就是操作 Redis 的哈希表索引。这个操作复杂度基本是固定的，即 O(1)。但是，当 Value 类型为 Set 时，SORT、SUNION/SMEMBERS 操作复杂度分别为 O(N+M*log(M)) 和 O(N)。其中，N 为 Set 中的元素个数，M 为 SORT 操作返回的元素个数。这个复杂度就增加了很多。Redis 官方文档中对每个命令的复杂度都有介绍，当你需要了解某个命令的复杂度时，可以直接查询。 那该怎么应对这个问题呢？当你发现 Redis 性能变慢时，可以通过 Redis 日志，或者是 latency monitor 工具，查询变慢的请求，根据请求对应的具体命令以及官方文档，确认下是否采用了复杂度高的慢查询命令。如果的确有大量的慢查询命令，有两种处理方式： 用其他高效命令代替。比如说，如果你需要返回一个 SET 中的所有成员时，不要使用SMEMBERS 命令，而是要使用 SSCAN 多次迭代返回，避免一次返回大量数据，造成线程阻塞。 当需要执行排序、交集、并集操作时，可以在客户端完成，而不要用 SORT、SUNION、SINTER 这些命令，以免拖慢 Redis 实例。 keys影响（scan讲解）还有一个比较容易忽略的慢查询命令，就是 KEYS。它用于返回和输入模式匹配的所有key，因为 KEYS 命令需要遍历存储的键值对，所以操作延时高。所以，KEYS 命令一般不被建议用于生产环境中。 如果想要获取整个实例的所有key，建议使用SCAN命令代替。客户端通过执行SCAN $cursor COUNT $count可以得到一批key以及下一个游标$cursor，然后把这个$cursor当作SCAN的参数，再次执行，以此往复，直到返回的$cursor为0时，就把整个实例中的所有key遍历出来了。 scan重复关于SCAN讨论最多的问题就是，Redis在做Rehash时，会不会漏key或返回重复的key。 在使用SCAN命令时，不会漏key，但可能会得到重复的key，这主要和Redis的Rehash机制有关。Redis的所有key存在一个全局的哈希表中，如果存入的key慢慢变多，在达到一定阈值后，为了避免哈希冲突导致查询效率降低，这个哈希表会进行扩容。与之对应的，key数量逐渐变少时，这个哈希表会缩容以节省空间。 1、为什么不会漏key？Redis在SCAN遍历全局哈希表时，采用高位进位法的方式遍历哈希桶（可网上查询图例，一看就明白），当哈希表扩容后，通过这种算法遍历，旧哈希表中的数据映射到新哈希表，依旧会保留原来的先后顺序，这样就可以保证遍历时不会遗漏也不会重复。 2、为什么SCAN会得到重复的key？这个情况主要发生在哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个key返回多次。 SCAN是遍历整个实例的所有key，另外Redis针对Hash/Set/Sorted Set也提供了HSCAN/SSCAN/ZSCAN命令，用于遍历一个key中的所有元素，建议在获取一个bigkey的所有数据时使用，避免发生阻塞风险。 但是使用HSCAN/SSCAN/ZSCAN命令，返回的元素数量与执行SCAN逻辑可能不同。执行SCAN $cursor COUNT $count时一次最多返回count个数的key，数量不会超过count。但Hash/Set/Sorted Set元素数量比较少时，底层会采用intset/ziplist方式存储，如果以这种方式存储，在执行HSCAN/SSCAN/ZSCAN命令时，会无视count参数，直接把所有元素一次性返回，也就是说，得到的元素数量是会大于count参数的。当底层转为哈希表或跳表存储时，才会真正使用发count参数，最多返回count个元素。 过期 key 操作我们来看过期 key 的自动删除机制。它是 Redis 用来回收内存空间的常用机制，应用广泛，本身就会引起 Redis 操作阻塞，导致性能变慢，所以，你必须要知道该机制对性能的影响。Redis 键值对的 key 可以设置过期时间。默认情况下，Redis 每 100 毫秒会删除一些过期key，具体的算法如下： 采样 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 个数的 key，并将其中过期的key 全部删除； 如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下。 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 是 Redis 的一个参数，默认是 20，那么，一秒内基本有 200 个过期 key 会被删除。这一策略对清除过期 key、释放内存空间很有帮助。如果每秒钟删除 200 个过期 key，并不会对 Redis 造成太大影响。 但是，如果触发了上面这个算法的第二条，Redis 就会一直删除以释放内存空间。注意，删除操作是阻塞的（Redis 4.0 后可以用异步线程机制来减少阻塞影响）。所以，一旦该条件触发，Redis 的线程就会一直执行删除，这样一来，就没办法正常服务其他的键值操作了，就会进一步引起其他键值操作的延迟增加，Redis 就会变慢。 那么，算法的第二条是怎么被触发的呢？其中一个重要来源，就是频繁使用带有相同时间参数的 EXPIREAT 命令设置过期 key，这就会导致，在同一秒内有大量的 key 同时过期。遇到这种情况时，千万不要嫌麻烦，你首先要根据实际业务的使用需求，决定 EXPIREAT 和 EXPIRE 的过期时间参数。其次，如果一批 key 的确是同时过期，你还可以在EXPIREAT 和 EXPIRE 的过期时间参数上，加上一个一定大小范围内的随机数，这样，既保证了 key 在一个邻近时间范围内被删除，又避免了同时过期造成的压力。 文件系统影响前面说过，为了保证数据可靠性，Redis 会采用 AOF 日志或 RDB 快照。其中，AOF日志提供了三种日志写回策略：no、everysec、always。这三种写回策略依赖文件系统的两个系统调用完成，也就是 write 和 fsync。write 只要把日志记录写到内核缓冲区，就可以返回了，并不需要等待日志实际写回到磁盘；而 fsync 需要把日志记录写回到磁盘后才能返回，时间较长。下面这张表展示了三种写回策略所执行的系统调用。 当写回策略配置为 everysec 和 always 时，Redis 需要调用 fsync 把日志写回磁盘。但是，这两种写回策略的具体执行情况还不太一样： 在使用 everysec 时，Redis 允许丢失一秒的操作记录，所以，Redis 主线程并不需要确保每个操作记录日志都写回磁盘。而且，fsync 的执行时间很长，如果是在 Redis 主线程中执行 fsync，就容易阻塞主线程。所以，当写回策略配置为 everysec 时，Redis 会使用后台的子线程异步完成 fsync 的操作。 而对于 always 策略来说，Redis 需要确保每个操作记录日志都写回磁盘，如果用后台子线程异步完成，主线程就无法及时地知道每个操作是否已经完成了，这就不符合 always 策略的要求了。所以，always 策略并不使用后台子线程来执行。 在使用 AOF 日志时，为了避免日志文件不断增大，Redis 会执行 AOF 重写，生成体量缩小的新的 AOF 日志文件。AOF 重写本身需要的时间很长，也容易阻塞 Redis 主线程，所以，Redis 使用子进程来进行 AOF 重写。AOF 重写会对磁盘进行大量 IO 操作，同时，fsync 又需要等到数据写到磁盘后才能返回，所以，当 AOF 重写的压力比较大时，就会导致 fsync 被阻塞。虽然 fsync 是由后台子线程负责执行的，但是，主线程会监控 fsync 的执行进度。当主线程使用后台子线程执行了一次 fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的 fsync 还没有执行完，那么它就会阻塞。所以，如果后台子线程执行的 fsync 频繁阻塞的话（比如 AOF 重写占用了大量的磁盘 IO 带宽），主线程也会阻塞，导致 Redis 性能变慢。 排查和解决方法你可以检查下 Redis 配置文件中的 appendfsync 配置项，该配置项的取值表明了Redis 实例使用的是哪种 AOF 日志写回策略。如果 AOF 写回策略使用了 everysec 或 always 配置，请先确认下业务方对数据可靠性的要求，明确是否需要每一秒或每一个操作都记日志。如果业务应用对延迟非常敏感，但同时允许一定量的数据丢失，那么，可以把配置项 no\u0002appendfsync-on-rewrite 设置为 yes，如下所示： 1no-appendfsync-on-rewrite yes 这个配置项设置为 yes 时，表示在 AOF 重写时，不进行 fsync 操作。也就是说，Redis 实例把写命令写到内存后，不调用后台线程进行 fsync 操作，就可以直接返回了。当然，如果此时实例发生宕机，就会导致数据丢失。反之，如果这个配置项设置为 no（也是默认配置），在 AOF 重写时，Redis 实例仍然会调用后台线程进行 fsync 操作，这就会给实例带来阻塞。 如果的确需要高性能，同时也需要高可靠数据保证，我建议你考虑采用高速的固态硬盘作为 AOF 日志的写入设备。 操作系统影响swap影响内存 swap 是操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，涉及到磁盘的读写，所以，一旦触发 swap，无论是被换入数据的进程，还是被换出数据的进程，其性能都会受到慢速磁盘读写的影响。 Redis 是内存数据库，内存使用量大，如果没有控制好内存的使用量，或者和其他内存需求大的应用一起运行了，就可能受到 swap 的影响，而导致性能变慢。通常，触发 swap 的原因主要是物理机器内存不足，对于 Redis 而言，有两种常见的情况： Redis 实例自身使用了大量的内存，导致物理机器的可用内存不足； 和 Redis 实例在同一台机器上运行的其他进程，在进行大量的文件读写操作。文件读写本身会占用系统内存，这会导致分配给 Redis 实例的内存量变少，进而触发 Redis 发生swap。 内存大页影响Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。 很多人都觉得：“Redis 是内存数据库，内存大页不正好可以满足 Redis 的需求吗？而且在分配相同的内存量时，内存大页还能减少分配次数，不也是对 Redis 友好吗?” 虽然内存大页可以给 Redis 带来内存分配方面的收益，但是，不要忘了，Redis 为了提供数据可靠性保证，需要将数据做持久化保存。这个写入过程由额外的线程执行，所以，此时，Redis 主线程仍然可以接收客户端写请求。客户端的写请求可能会修改正在进行持久化的数据。在这一过程中，Redis 就会采用写时复制机制，也就是说，一旦有数据要被修改，Redis 并不会直接修改内存中的数据，而是将这些数据拷贝一份，然后再进行修改。 如果采用了内存大页，那么，即使客户端请求只修改 100B 的数据，Redis 也需要拷贝2MB 的大页。相反，如果是常规内存页机制，只用拷贝 4KB。两者相比，你可以看到，当客户端请求修改或新写入数据较多时，内存大页机制将导致大量的拷贝，这就会影响Redis 正常的访存操作，最终导致性能变慢。 在实际生产环境中部署时，建议不要使用内存大页机制，操作也很简单，只需要执行下面的命令就可以了： 1echo never /sys/kernel/mm/transparent_hugepage/enabled 小结为了方便你应用，我给你梳理了一个包含 9 个检查点的 Checklist，希望你在遇到 Redis性能变慢时，按照这些步骤逐一检查，高效地解决问题。 获取 Redis 实例在当前环境下的基线性能。 是否用了慢查询命令？如果是的话，就使用其他命令替代慢查询命令，或者把聚合计算命令放在客户端做。 是否对过期 key 设置了相同的过期时间？对于批量删除的 key，可以在每个 key 的过期时间上加一个随机数，避免同时删除。 是否存在 bigkey？ 对于 bigkey 的删除操作，如果你的 Redis 是 4.0 及以上的版本，可以直接利用异步线程机制减少主线程阻塞；如果是 Redis 4.0 以前的版本，可以使用SCAN 命令迭代删除；对于 bigkey 的集合查询和聚合操作，可以使用 SCAN 命令在客户端完成。 Redis AOF 配置级别是什么？业务层面是否的确需要这一可靠性级别？如果我们需要高性能，同时也允许数据丢失，可以将配置项 no-appendfsync-on-rewrite 设置为yes，避免 AOF 重写和 fsync 竞争磁盘 IO 资源，导致 Redis 延迟增加。当然， 如果既需要高性能又需要高可靠性，最好使用高速固态盘作为 AOF 日志的写入盘。 Redis 实例的内存使用是否过大？发生 swap 了吗？如果是的话，就增加机器内存，或者是使用 Redis 集群，分摊单机 Redis 的键值对数量和内存压力。同时，要避免出现Redis 和其他内存需求大的应用共享机器的情况。 在 Redis 实例的运行环境中，是否启用了透明大页机制？如果是的话，直接关闭内存大页机制就行了。 是否运行了 Redis 主从集群？如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞。 是否使用了多核 CPU 或 NUMA 架构的机器运行 Redis 实例？使用多核 CPU 时，可以给 Redis 实例绑定物理核；使用 NUMA 架构时，注意把 Redis 实例和网络中断处理程序运行在同一个 CPU Socket 上。 Redis 内存碎片在使用 Redis 时，我们经常会遇到这样一个问题：明明做了数据删除，数据量已经不大了，为什么使用 top 命令查看时，还会发现 Redis 占用了很多内存呢？实际上，这是因为，当数据删除后，Redis 释放的内存空间会由内存分配器管理，并不会立即返回给操作系统。所以，操作系统仍然会记录着给 Redis 分配了大量内存。 但是，这往往会伴随一个潜在的风险点：Redis 释放的内存空间可能并不是连续的，那么，这些不连续的内存空间很有可能处于一种闲置的状态。这就会导致一个问题：虽然有空闲空间，Redis 却无法用来保存数据，不仅会减少 Redis 能够实际保存的数据量，还会降低 Redis 运行机器的成本回报率。这就是内存碎片产生的影响，虽然操作系统的剩余内存空间总量足够，但是，应用申请的是一块连续地址空间的 N 字节，但在剩余的内存空间中，没有大小为 N 字节的连续空间了，那么，这些剩余空间就是内存碎片。 内存碎片形成原因内存碎片的形成有内因和外因两个层面的原因。简单来说，内因是操作系统的内存分配机制，外因是 Redis 的负载特征。 内存分配器的分配策略内存分配器的分配策略就决定了操作系统无法做到“按需分配”。这是因为，内存分配器一般是按固定大小来分配内存，而不是完全按照应用程序申请的内存空间大小给程序分配。Redis 可以使用 libc、jemalloc、tcmalloc 多种内存分配器来分配内存，默认使用jemalloc。jemalloc 的分配策略之一，是按照一系列固定的大小划分内存空间，例如 8 字节、16 字节、32 字节、48 字节，…, 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间。这样的分配方式本身是为了减少分配次数。例如，Redis 申请一个 20 字节的空间保存数据，jemalloc 就会分配 32 字节，此时，如果应用还要写入 10 字节的数据，Redis 就不用再向操作系统申请空间了，因为刚才分配的 32 字节已经够用了，这就避免了一次分配操作。但是，如果 Redis 每次向分配器申请的内存空间大小不一样，这种分配方式就会有形成碎片的风险，而这正好来源于 Redis 的外因了。 键值对大小不一样和删改操作内存分配器只能按固定大小分配内存，所以，分配的内存空间一般都会比申请的空间大一些，不会完全一致，这本身就会造成一定的碎片，降低内存空间存储效率。比如说，应用 A 保存 6 字节数据，jemalloc 按分配策略分配 8 字节。如果应用 A 不再保存新数据，那么，这里多出来的 2 字节空间就是内存碎片了。 第二个外因是，这些键值对会被修改和删除，这会导致空间的扩容和释放。具体来说，一方面，如果修改后的键值对变大或变小了，就需要占用额外的空间或者释放不用的空间。另一方面，删除的键值对就不再需要内存空间了，此时，就会把空间释放出来，形成空闲空间。 大量内存碎片的存在，会造成 Redis 的内存实际利用率变低，接下来，我们就要来解决这个问题了。 如何判断是否有内存碎片？Redis 自身提供了 INFO 命令，可以用来查询内存使用的详细信息，命令如下： 12345678910INFO memory# Memory# 操作系统实际分配给 Redis 的物理内存空间(包含内存碎片)used_memory:1073741736used_memory_human:1024.00M# used_memory 是 Redis 为了保存数据实际申请使用的空间used_memory_rss:1997159792used_memory_rss_human:1.86G# 内存碎片率。used_memory_rss 和 used_memory 相除的结果mem_fragmentation_ratio:1.86 mem_fragmentation_ratio 大于 1 但小于 1.5。这种情况是合理的。这是因为，刚才介绍的那些因素是难以避免的。毕竟，内因的内存分配器是一定要使用的，分配策略都是通用的，不会轻易修改；而外因由 Redis 负载决定，也无法限制。所以，存在内存碎片也是正常的。 mem_fragmentation_ratio 大于 1.5 。这表明内存碎片率已经超过了 50%。一般情况下，这个时候，我们就需要采取一些措施来降低内存碎片率了。 如何清理内存碎片？当 Redis 发生内存碎片后，一个“简单粗暴”的方法就是重启 Redis 实例。当然，这并不是一个“优雅”的方法，毕竟，重启 Redis 会带来两个后果： 如果 Redis 中的数据没有持久化，那么，数据就会丢失； 即使 Redis 数据持久化了，我们还需要通过 AOF 或 RDB 进行恢复，恢复时长取决于AOF 或 RDB 的大小，如果只有一个 Redis 实例，恢复阶段无法提供服务。 从 4.0-RC3 版本以后，Redis 自身提供了一种内存碎片自动清理的方法，我们先来看这个方法的基本机制。 内存碎片清理，简单来说，就是搬家让位，合并空间。当有数据把一块连续的内存空间分割成好几块不连续的空间时，操作系统就会把数据拷贝到别处。此时，数据拷贝需要能把这些数据原来占用的空间都空出来，把原本不连续的内存空间变成连续的空间。否则，如果数据拷贝后，并没有形成连续的内存空间，这就不能算是清理了。（有点像JVM新生代空间清理） 需要注意的是：碎片清理是有代价的，操作系统需要把多份数据拷贝到新位置，把原有空间释放出来，这会带来时间开销。因为 Redis 是单线程，在数据拷贝时，Redis 只能等着，这就导致 Redis 无法及时处理请求，性能就会降低。而且，有的时候，数据拷贝还需要注意顺序，这种对顺序性的要求，会进一步增加 Redis 的等待时间，导致性能降低。 Redis 专门为自动内存碎片清理功机制设置的参数了。我们可以通过设置参数，来控制碎片清理的开始和结束时机，以及占用的 CPU 比例，从而减少碎片清理对 Redis 本身请求处理的性能影响。 首先，Redis 需要启用自动内存碎片清理，可以把 activedefrag 配置项设置为 yes，命令如下： 1config set activedefrag yes 这个命令只是启用了自动清理功能，但是，具体什么时候清理，会受到下面这两个参数的控制。这两个参数分别设置了触发内存清理的一个条件，如果同时满足这两个条件，就开始清理。在清理的过程中，只要有一个条件不满足了，就停止自动清理。 active-defrag-ignore-bytes 100mb：表示内存碎片的字节数达到 100MB 时，开始清理； active-defrag-threshold-lower 10：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。 为了尽可能减少碎片清理对 Redis 正常请求处理的影响，自动内存碎片清理功能在执行时，还会监控清理操作占用的 CPU 时间，而且还设置了两个参数，分别用于控制清理操作占用的 CPU 时间比例的上、下限，既保证清理工作能正常进行，又避免了降低 Redis 性能。这两个参数具体如下： active-defrag-cycle-min 25： 表示自动清理过程所用 CPU 时间的比例不低于25%，保证清理能正常开展； active-defrag-cycle-max 75：表示自动清理过程所用 CPU 时间的比例不高于75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。 本节注意内存碎片自动清理涉及内存拷贝，这对 Redis 而言，是个潜在的风险。如果你在实践过程中遇到 Redis 性能变慢，记得通过日志看下是否正在进行碎片清理。如果 Redis 的确正在清理碎片，那么，我建议你调小 active-defrag-cycle-max 的值，以减轻对正常请求处理的影响。 如果 mem_fragmentation_ratio 小于 1 了，Redis 的内存使用是什么情况呢？会对 Redis 的性能和内存空间利用率造成什么影响？ mem_fragmentation_ratio小于1，说明used_memory_rss小于了used_memory，这意味着操作系统分配给Redis进程的物理内存，要小于Redis实际存储数据的内存，也就是说Redis没有足够的物理内存可以使用了，这会导致Redis一部分内存数据会被换到Swap中，之后当Redis访问Swap中的数据时，延迟会变大，性能下降。 Redis缓冲区缓冲区的功能其实很简单，主要就是用一块内存空间来暂时存放命令数据，以免出现因为数据和命令的处理速度慢于发送速度而导致的数据丢失和性能问题。但因为缓冲区的内存空间有限，如果往里面写入数据的速度持续地大于从里面读取数据的速度，就会导致缓冲区需要越来越多的内存来暂存数据。当缓冲区占用的内存超出了设定的上限阈值时，就会出现缓冲区溢出。 客户端输入和输出缓冲区为了避免客户端和服务器端的请求发送和处理速度不匹配，服务器端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区，我们称之为客户端输入缓冲区和输出缓冲区。如下图： 如何应对输入缓冲区溢出？输入缓冲区就是用来暂存客户端发送的请求命令的，所以可能导致溢出的情况主要是下面两种： 写入了 bigkey，比如一下子写入了多个百万级别的集合类型数据； 服务器端处理请求的速度过慢，例如，Redis 主线程出现了间歇性阻塞，无法及时处理正常发送的请求，导致客户端发送的请求在缓冲区越积越多。 要查看和服务器端相连的每个客户端对输入缓冲区的使用情况，我们可以使用 CLIENT LIST 命令： 12CLIENT LISTid=5 addr=127.0.0.1:50487 fd=9 name= age=4 idle=0 flags=N db=0 sub=0 psub=0 mu... 有了 CLIENT LIST 命令，我们就可以通过输出结果来判断客户端输入缓冲区的内存占用情况了。如果 qbuf 很大，而同时 qbuf-free 很小，就要引起注意了，因为这时候输入缓冲区已经占用了很多内存，而且没有什么空闲空间了。此时，客户端再写入大量命令的话，就会引起客户端输入缓冲区溢出，Redis 的处理办法就是把客户端连接关闭，结果就是业务程序无法进行数据存取了。 通常情况下，Redis 服务器端不止服务一个客户端，当多个客户端连接占用的内存总量，超过了 Redis 的 maxmemory 配置项时（例如 4GB），就会触发 Redis 进行数据淘汰。一旦数据被淘汰出 Redis，再要访问这部分数据，就需要去后端数据库读取，这就降低了业务应用的访问性能。此外，更糟糕的是，如果使用多个客户端，导致 Redis 内存占用过大，也会导致内存溢出（out-of-memory）问题，进而会引起 Redis 崩溃，给业务应用造成严重影响。 所以，我们必须得想办法避免输入缓冲区溢出。我们可以从两个角度去考虑如何避免，一是把缓冲区调大，二是从数据命令的发送和处理速度入手。 缓冲区调大Redis 的客户端输入缓冲区大小的上限阈值，在代码中就设定为了 1GB。也就是说，Redis服务器端允许为每个客户端最多暂存 1GB 的命令和数据。1GB 的大小，对于一般的生产环境已经是比较合适的了。一方面，这个大小对于处理绝大部分客户端的请求已经够用了； 所以：Redis 并没有提供参数让我们调节客户端输入缓冲区的大小。那我们就只能从数据命令的发送和处理速度入手，也就是前面提到的避免客户端写入 bigkey，以及避免 Redis 主线程阻塞。 输出缓冲区溢出Redis 为每个客户端设置的输出缓冲区也包括两部分：一部分，是一个大小为 16KB的固定缓冲空间，用来暂存 OK 响应和出错信息；另一部分，是一个可以动态增加的缓冲空间，用来暂存大小可变的响应结果。那什么情况下会发生输出缓冲区溢出呢？ 总结有三种： 服务器端返回 bigkey 的大量结果； 执行了 MONITOR 命令； 缓冲区大小设置得不合理。 输入缓冲区不同，我们可以通过 client-output-buffer-limit 配置项，来设置缓冲区的大小。具体设置的内容包括两方面： 设置缓冲区大小的上限阈值； 设置输出缓冲区持续写入数据的数量上限阈值，和持续写入数据的时间的上限阈值。 对于和 Redis 实例进行交互的应用程序来说，主要使用两类客户端和 Redis 服务器端交互，分别是常规和 Redis 服务器端进行读写命令交互的普通客户端，以及订阅了 Redis 频道的订阅客户端。当我们给普通客户端设置缓冲区大小时，通常可以在 Redis 配置文件中进行这样的设置： 1client-output-buffer-limit normal 0 0 0 其中，normal 表示当前设置的是普通客户端，第 1 个 0 设置的是缓冲区大小限制，第 2个 0 和第 3 个 0 分别表示缓冲区持续写入量限制和持续写入时间限制。对于普通客户端来说，它每发送完一个请求，会等到请求结果返回后，再发送下一个请求，这种发送方式称为阻塞式发送。在这种情况下，如果不是读取体量特别大的 bigkey，服务器端的输出缓冲区一般不会被阻塞的。 对于订阅客户端来说，一旦订阅的 Redis 频道有消息了，服务器端都会通过输出缓冲区把消息发给客户端。所以，订阅客户端和服务器间的消息发送方式，不属于阻塞式发送。不过，如果频道消息较多的话，也会占用较多的输出缓冲区空间。因此我们会有如下设置： 1client-output-buffer-limit pubsub 8mb 2mb 60 其中，pubsub 参数表示当前是对订阅客户端进行设置；8mb 表示输出缓冲区的大小上限为 8MB，一旦实际占用的缓冲区大小要超过 8MB，服务器端就会直接关闭客户端的连接；2mb 和 60 表示，如果连续 60 秒内对输出缓冲区的写入量超过 2MB 的话，服务器端也会关闭客户端连接。 总结下如何应对输出缓冲区溢出： 避免 bigkey 操作返回大量数据结果； 避免在线上环境中持续使用 MONITOR 命令。 使用 client-output-buffer-limit 设置合理的缓冲区大小上限，或是缓冲区连续写入时间和写入量上限。 主从集群中的缓冲区主从集群间的数据复制包括全量复制和增量复制两种。全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库。无论在哪种形式的复制中，为了保证主从节点的数据一致，都会用到缓冲区。但是，这两种复制场景下的缓冲区，在溢出影响和大小设置方面并不一样。 复制缓冲区的溢出问题在全量复制过程中，主节点在向从节点传输 RDB 文件的同时，会继续接收客户端发送的写命令请求。这些写命令就会先保存在复制缓冲区中，等 RDB 文件传输完成后，再发送给从节点去执行。主节点上会为每个从节点都维护一个复制缓冲区，来保证主从节点间的数据同步。 所以，如果在全量复制时，从节点接收和加载 RDB 较慢，同时主节点接收到了大量的写命令，写命令在复制缓冲区中就会越积越多，最终导致溢出。复制缓冲区一旦发生溢出，主节点也会直接关闭和从节点进行复制操作的连接，导致全量复制失败。那如何避免复制缓冲区发生溢出呢？ 一方面，我们可以控制主节点保存的数据量大小。按通常的使用经验，我们会把主节点的数据量控制在 2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区累积过多命令。 另一方面，我们可以使用 client-output-buffer-limit 配置项，来设置合理的复制缓冲区大小。设置的依据，就是主节点的数据量大小、主节点的写负载压力和主节点本身的内存大小。 关于复制缓冲区，我们还会遇到一个问题。主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和。如果集群中的从节点数非常多的话，主节点的内存开销就会非常大。所以，我们还必须得控制和主节点连接的从节点个数，不要使用大规模的主从集群。 复制积压缓冲区的溢出问题增量复制时使用的缓冲区，这个缓冲区称为复制积压缓冲区。主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步，如下图所示： 复制积压缓冲区就是我们之前学的 repl_backlog_buffer。首先，复制积压缓冲区是一个大小有限的环形缓冲区。当主节点把复制积压缓冲区写满后，会覆盖缓冲区中的旧命令数据。如果从节点还没有同步这些旧命令数据，就会造成主从节点间重新开始执行全量复制。其次，为了应对复制积压缓冲区的溢出问题，我们可以调整复制积压缓冲区的大小，也就是设置 repl_backlog_size 这个参数的值。（具体看主从同步那节） 小结这节课，我们一起学习了 Redis 中使用的缓冲区。使用缓冲区以后，当命令数据的接收方处理速度跟不上发送方的发送速度时，缓冲区可以避免命令数据的丢失。 按照缓冲区的用途，例如是用于客户端通信还是用于主从节点复制，我把缓冲区分成了客户端的输入和输出缓冲区，以及主从集群中主节点上的复制缓冲区和复制积压缓冲区。这样学习的好处是，你可以很清楚 Redis 中到底有哪些地方使用了缓冲区，那么在排查问题的时候，就可以快速找到方向——从客户端和服务器端的通信过程以及主从节点的复制过程中分析原因。 现在，从缓冲区溢出对 Redis 的影响的角度，我再把这四个缓冲区分成两类做个总结。 缓冲区溢出导致网络连接关闭：普通客户端、订阅客户端，以及从节点客户端，它们使用的缓冲区，本质上都是 Redis 客户端和服务器端之间，或是主从节点之间为了传输命令数据而维护的。这些缓冲区一旦发生溢出，处理机制都是直接把客户端和服务器端的连接，或是主从节点间的连接关闭。网络连接关闭造成的直接影响，就是业务程序无法读写 Redis，或者是主从节点全量同步失败，需要重新执行。 缓冲区溢出导致命令数据丢失：主节点上的复制积压缓冲区属于环形缓冲区，一旦发生溢出，新写入的命令数据就会覆盖旧的命令数据，导致旧命令数据的丢失，进而导致主从节点重新进行全量复制。 从本质上看，缓冲区溢出，无非就是三个原因：命令数据发送过快过大；命令数据处理较慢；缓冲区空间过小。明白了这个，我们就可以有针对性地拿出应对策略了。 针对命令数据发送过快过大的问题，对于普通客户端来说可以避免 bigkey，而对于复制缓冲区来说，就是避免过大的 RDB 文件。 针对命令数据处理较慢的问题，解决方案就是减少 Redis 主线程上的阻塞操作，例如使用异步的删除操作。 针对缓冲区空间过小的问题，解决方案就是使用 client-output-buffer-limit 配置项设置合理的输出缓冲区、复制缓冲区和复制积压缓冲区大小。当然，我们不要忘了，输入缓冲区的大小默认是固定的，我们无法通过配置来修改它，除非直接去修改 Redis 源码。 应用程序和Redis实例交互时，应用程序中使用的客户端需要使用缓冲区吗？如果使用的话，对Redis的性能和内存使用有什么影响？ 客户端和服务端交互，一般都会制定一个交互协议，客户端给服务端发数据时，都会按照这个协议把数据拼装好，然后写到客户端buffer中，客户端再一次性把buffer数据写到操作系统的网络缓冲区中，最后由操作系统发送给服务端。这样服务端就能从网络缓冲区中读取到一整块数据，然后按照协议解析数据即可。使用buffer发送数据会比一个个发送数据到服务端效率要高很多。 客户端还可以使用Pipeline批量发送命令到服务端，以提高访问性能。不使用Pipeline时，客户端是发送一个命令、读取一次结果。而使用Pipeline时，客户端先把一批命令暂存到buffer中，然后一次性把buffer中的命令发送到服务端，服务端处理多个命令后批量返回结果，这样做的好处是可以减少来回网络IO的次数，降低延迟，提高访问性能。当然，Redis服务端的buffer内存也会相应增长，可以控制好Pipeline命令的数量防止buffer超限。 在应用 Redis 主从集群时，主从节点进行故障切换是需要一定时间的，此时，主节点无法服务外来请求。如果客户端有缓冲区暂存请求，那么，客户端仍然可以正常接收业务应用的请求，这就可以避免直接给应用返回无法服务的错误。 Redis慢操作Redis慢日志Redis 的慢查询日志记录了执行时间超过一定阈值的命令操作。当我们发现 Redis 响应变慢、请求延迟增加时，就可以在慢查询日志中进行查找，确定究竟是哪些命令执行时间很长。在使用慢查询日志前，我们需要设置两个参数： slowlog-log-slower-than：这个参数表示，慢查询日志对执行时间大于多少微秒的命令进行记录。 slowlog-max-len：这个参数表示，慢查询日志最多能记录多少条命令记录。慢查询日志的底层实现是一个具有预定大小的先进先出队列，一旦记录的命令数量超过了队列长度，最先记录的命令操作就会被删除。这个值默认是 128。但是，如果慢查询命令较多的话，日志里就存不下了；如果这个值太大了，又会占用一定的内存空间。所以，一般建议设置为 1000 左右，这样既可以多记录些慢查询命令，方便排查，也可以避免内存开销。 设置好参数后，慢查询日志就会把执行时间超过 slowlog-log-slower-than 阈值的命令操作记录在日志中。 我们可以使用 SLOWLOG GET 命令，来查看慢查询日志中记录的命令操作，例如，我们执行如下命令，可以查看最近的一条慢查询的日志信息。 12345678SLOWLOG GET 11) 1) (integer) 33 //每条日志的唯一ID编号 2) (integer) 1600990583 //命令执行时的时间戳 3) (integer) 20906 //命令执行的时长，单位是微秒 4) 1) &quot;keys&quot; //具体的执行命令和参数 2) &quot;abc*&quot; 5) &quot;127.0.0.1:54793&quot; //客户端的IP和端口号 6) &quot;&quot; //客户端的名称，此处为空 有了慢查询日志后，我们就可以快速确认，究竟是哪些命令的执行时间比较长，然后可以反馈给业务部门，让业务开发人员避免在应用 Redis 的过程中使用这些命令，或是减少操作的数据量，从而降低命令的执行复杂度。 latency monitor 监控工具Redis 从 2.8.13 版本开始，还提供了 latency monitor 监控工具，这个工具可以用来监控 Redis 运行过程中的峰值延迟情况。和慢查询日志的设置相类似，要使用 latency monitor，首先要设置命令执行时长的阈值。当一个命令的实际执行时长超过该阈值时，就会被 latency monitor 监控到。比如，我们可以把 latency monitor 监控的命令执行时长阈值设为 1000 微秒，如下所示： 1config set latency-monitor-threshold 1000 设置好了 latency monitor 的参数后，我们可以使用 latency latest 命令，查看最新和最大的超过阈值的延迟情况，如下所示： 12345latency latest1) 1) &quot;command&quot; 2) (integer) 1600991500 //命令执行的时间戳 3) (integer) 2500 //最近的超过阈值的延迟 4) (integer) 10100 //最大的超过阈值的延迟 Redis Bigkey 排查在应用 Redis 时，我们要尽量避免 bigkey 的使用，这是因为，Redis 主线程在操作bigkey 时，会被阻塞。那么，一旦业务应用中使用了 bigkey，我们该如何进行排查呢？ Redis 可以在执行 redis-cli 命令时带上–bigkeys 选项，进而对整个数据库中的键值对大小情况进行统计分析，比如说，统计每种数据类型的键值对个数以及平均大小。此外，这个命令执行后，会输出每种数据类型中最大的 bigkey 的信息，对于 String 类型来说，会输出最大 bigkey 的字节长度，对于集合类型来说，会输出最大 bigkey 的元素个数，如下所示： 123456789101112131415161718192021./redis-cli --bigkeys-------- summary -------Sampled 32 keys in the keyspace!Total key length in bytes is 184 (avg len 5.75)//统计每种数据类型中元素个数最多的bigkeyBiggest list found &apos;product1&apos; has 8 itemsBiggest hash found &apos;dtemp&apos; has 5 fieldsBiggest string found &apos;page2&apos; has 28 bytesBiggest stream found &apos;mqstream&apos; has 4 entriesBiggest set found &apos;userid&apos; has 5 membersBiggest zset found &apos;device:temperature&apos; has 6 members//统计每种数据类型的总键值个数，占所有键值个数的比例，以及平均大小4 lists with 15 items (12.50% of keys, avg size 3.75)5 hashs with 14 fields (15.62% of keys, avg size 2.80)10 strings with 68 bytes (31.25% of keys, avg size 6.80)1 streams with 4 entries (03.12% of keys, avg size 4.00)7 sets with 19 members (21.88% of keys, avg size 2.71)5 zsets with 17 members (15.62% of keys, avg size 3.40) 不过，在使用–bigkeys 选项时，有一个地方需要注意一下。这个工具是通过扫描数据库来查找 bigkey 的，所以，在执行的过程中，会对 Redis 实例的性能产生影响。如果你在使用主从集群，我建议你在从节点上执行该命令。因为主节点上执行时，会阻塞主节点。如果没有从节点，那么，我给你两个小建议：第一个建议是，在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；第二个建议是，可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能。例如，我们执行如下命令时，redis-cli会每扫描 100 次暂停 100 毫秒（0.1 秒）。 1/redis-cli --bigkeys -i 0.1 当然，使用 Redis 自带的–bigkeys 选项排查 bigkey，有两个不足的地方： 这个方法只能返回每种类型中最大的那个 bigkey，无法得到大小排在前 N 位的bigkey； 对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。但是，一个集合中的元素个数多，并不一定占用的内存就多。因为，有可能每个元素占用的内存很小，这样的话，即使元素个数有很多，总内存开销也不大。 所以，如果我们想统计每个数据类型中占用内存最多的前 N 个 bigkey，可以自己开发一个程序，来进行统计。 这里提供一个基本的开发思路：使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。接下来，对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。对于集合类型来说，有两种方法可以获得它占用的内存大小，如果你能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。如果你不能提前知道写入集合的元素大小，可以使用 MEMORY USAGE 命令（需要 Redis4.0 及以上版本），查询一个键值对占用的内存空间。例如，执行以下命令，可以获得 key为 user:info 这个集合类型占用的内存空间大小。 12MEMORY USAGE user:info(integer) 315663239 这样一来，你就可以在开发的程序中，把每一种数据类型中的占用内存空间大小排在前 N位的 key 统计出来，这也就是每个数据类型中的前 N 个 bigkey。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/categories/Redis/"}],"tags":[{"name":"实践一","slug":"实践一","permalink":"https://jjw-story.github.io/tags/实践一/"},{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/tags/Redis/"}],"author":"JJW"},{"title":"Redis-基础","slug":"Redis-基础","date":"2021-03-01T08:30:27.000Z","updated":"2021-04-01T06:02:03.887Z","comments":true,"path":"2021/03/01/Redis-基础/","link":"","permalink":"https://jjw-story.github.io/2021/03/01/Redis-基础/","excerpt":"","text":"Redis基础总览 Redis全景图 Redis问题画像图 Redis功能模块图 Redis数据结构Redis在接收到一个键值对操作后，能以微秒级别的速度找到数据，并快速完成操作。主要原因是： 它是内存数据库，所有操作都在内存上完成，内存的访问速度本身就很快。 要归功于它的数据结构。这是因为，键值对是按一定的数据结构来组织的，操作键值对最终就是对数据结构进行增删改查操作，所以高效的数据结构是 Redis 快速处理数据的基础。 我们一般说的Redis数据结构有五种：String（字符串）、List（列表）、Hash（哈希）、Set（集合）和 Sorted Set（有序集合）。这些只是 Redis 键值对中值的数据类型，也就是数据的保存形式。这里，我们说的数据结构，是要去看看它们的底层实现。 底层数据结构简单来说，底层数据结构一共有 6 种，分别是简单动态字符串、双向链表、压缩列表、哈希表、跳表和整数数组。它们和数据类型的对应关系如下图所示： 可以看到，String 类型的底层实现只有一种数据结构，也就是简单动态字符串。而 List、Hash、Set 和 Sorted Set 这四种数据类型，都有两种底层实现结构。通常情况下，我们会把这四种类型称为集合类型，它们的特点是一个键对应了一个集合的数据。 键和值的组织结构为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。这也就是说，不管值是 String，还是集合类型，哈希桶中的元素都是指向它们的指针。 因为这个哈希表保存了所有的键值对，所以，我也把它称为全局哈希表。哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素。 哈希表的冲突问题 和 rehash当你往哈希表中写入更多数据时，哈希冲突是不可避免的问题。这里的哈希冲突，也就是指，两个 key 的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中。Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。 这里依然存在一个问题，哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低。 所以，Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。具体做法就是： 为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步： 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中； 释放哈希表 1 的空间。 这个过程看似简单，但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。 渐进式 rehash简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的entries。 这样就巧妙地把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。 渐进式 rehash 执行期间的哈希表操作因为在进行渐进式 rehash 的过程中， 字典会同时使用 1 和 2 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 1 里面进行查找， 如果没找到的话， 就会继续到 2 里面进行查找， 诸如此类。 另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 2 里面， 而 1 则不再进行任何添加操作： 这一措施保证了 1 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。 渐进式rehash带来的问题渐进式rehash避免了redis阻塞，可以说非常完美，但是由于在rehash时，需要分配一个新的hash表，在rehash期间，同时有两个hash表在使用，会使得redis内存使用量瞬间突增，在Redis 满容状态下由于Rehash会导致大量Key驱逐。 采用渐进式 hash 时，如果实例暂时没有收到新请求，是不是就不做 rehash 了？ 其实不是的。Redis 会执行定时任务，定时任务中就包含了 rehash 操作。所谓的定时任务，就是按照一定频率（例如每 100ms/ 次）执行的任务。在 rehash 被触发后，即使没有收到新请求，Redis 也会定时执行一次 rehash 操作，而且，每次执行时长不会超过 1ms，以免对其他任务造成影响。 Redis 什么时候做 rehash？Redis 会使用装载因子（load factor）来判断是否需要做 rehash。装载因子的计算方式是，哈希表中所有 entry 的个数除以哈希表的哈希桶个数。Redis 会根据装载因子的两种情况，来触发 rehash 操作： 在第一种情况下，如果装载因子等于 1，同时我们假设，所有键值对是平均分布在哈希表的各个桶中的，那么，此时，哈希表可以不用链式哈希，因为一个哈希桶正好保存了一个键值对。 但是，如果此时再有新的数据写入，哈希表就要使用链式哈希了，这会对查询性能产生影响。在进行 RDB 生成和 AOF 重写时，哈希表的 rehash 是被禁止的，这是为了避免对RDB 和 AOF 重写造成影响。如果此时，Redis 没有在生成 RDB 和重写 AOF，那么，就可以进行 rehash。否则的话，再有数据写入时，哈希表就要开始使用查询较慢的链式哈希了。 在第二种情况下，也就是装载因子大于等于 5 时，就表明当前保存的数据量已经远远大于哈希桶的个数，哈希桶里会有大量的链式哈希存在，性能会受到严重影响，此时，就立马开始做 rehash。 刚刚说的是触发 rehash 的情况，如果装载因子小于 1，或者装载因子大于 1 但是小于 5，同时哈希表暂时不被允许进行 rehash（例如，实例正在生成 RDB 或者重写 AOF），此时，哈希表是不会进行 rehash 操作的。 集合数据操作效率到这里，应该就能理解，Redis 的键和值是怎么通过哈希表组织的了。对于String 类型来说，找到哈希桶就能直接增删改查了，所以，哈希表的 O(1) 操作复杂度也就是它的复杂度了。 和 String 类型不同，一个集合类型的值，第一步是通过全局哈希表找到对应的哈希桶位置，第二步是在集合中再增删改查。 集合类型底层数据结构集合类型的底层数据结构主要有 5 种：整数数组、双向链表、哈希表、压缩列表和跳表。 压缩列表：压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个zlend，表示列表结束。 在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。 跳表：有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。查找过程就是在多级索引上跳来跳去，最后定位到元素。这也正好符合“跳”表的叫法。当数据量很大时，跳表的查找复杂度就是 O(logN)。 数据结构的时间复杂度： 小结Redis 之所以能快速操作键值对，一方面是因为 O(1) 复杂度的哈希表被广泛使用，包括String、Hash 和 Set，它们的操作复杂度基本由哈希表决定，另一方面，Sorted Set 也采用了 O(logN) 复杂度的跳表。不过，集合类型的范围操作，因为要遍历底层数据结构，复杂度通常是 O(N)。这里，我的建议是：用其他命令来替代，例如可以用 SCAN 来代替，避免在 Redis 内部产生费时的全集合遍历操作。 当然，我们不能忘了复杂度较高的 List 类型，它的两种底层实现结构：双向链表和压缩列表的操作复杂度都是 O(N)。因此，我的建议是：因地制宜地使用 List 类型。例如，既然它的 POP/PUSH 效率很高（他们都保存了表头三个字段），那么就将它主要用于FIFO 队列场景，而不是作为一个可以随机读写的集合。 高性能IO模型单线程Redis我们通常说，Redis 是单线程，主要是指 Redis 的网络 IO和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。所以，严格来说，Redis 并不是单线程。 Redis 为什么用单线程？要更好地理解 Redis 为什么用单线程，我们就要先了解多线程的开销。，通常情况下，在我们采用多线程后，如果没有良好的系统设计，系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。（比如Redis 有 List 的数据类型，并提供出队（LPOP）和入队（LPUSH）操作，如果多线程同时操作，队列长度的增减就会出现并发问题）。这就是多线程编程模式面临的共享资源的并发访问控制问题。 解决这个问题，如果没有精细的设计，比如说，只是简单地采用一个粗粒度互斥锁，就会出现不理想的结果，即使增加了线程，大部分线程也在等待获取访问共享资源的互斥锁，并行变串行采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。为了避免这些问题，Redis 直接采用了单线程模式。 单线程 Redis 为什么那么快？通常来说，单线程的处理能力要比多线程差很多，但是 Redis 却能使用单线程模型达到每秒数十万级别的处理能力，这是为什么呢？一方面，Redis 的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如哈希表和跳表，这是它实现高性能的一个重要原因。另一方面，就是 Redis 采用了多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。 基本 IO 模型与阻塞点以 Get 请求为例，Redis 为了处理一个 Get 请求，需要监听客户端请求（bind/listen），和客户端建立连接（accept），从 socket 中读取请求（recv），解析客户端发送请求（parse），根据请求类型读取键值数据（get），最后给客户端返回结果，即向 socket 中写回数据（send）。 在这里的网络 IO 操作中，有潜在的阻塞点，分别是 accept() 和 recv()。当 Redis监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这里，导致其他客户端无法和 Redis 建立连接。类似的，当 Redis 通过 recv() 从一个客户端读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()。这就导致 Redis 整个线程阻塞，无法处理其他客户端请求，效率很低。不过，幸运的是，socket 网络模型本身支持非阻塞模式。 基于多路复用的高性能 I/O 模型Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个IO 流的效果。（内核监听）模型图如下： 为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升Redis 的响应性能。 小结Redis 单线程是指它对网络 IO 和数据读写的操作采用了一个线程，而采用单线程的一个核心原因是避免多线程开发的并发控制问题。单线程的 Redis 也能获得高性能，跟多路复用的 IO 模型密切相关，因为这避免了 accept() 和 send()/recv() 潜在的网络 IO 操作阻塞点。 即便如此，单线程处理还是有瓶颈，我们思考Redis单线程处理IO请求性能瓶颈主要包括2个方面： 在客户端的并发量很大的情况下，单线程始终有它的瓶颈在。 redis 线程是循环处理每个事件的。如果其中一个事件比较耗时，会影响后面事件的及时处理。 经典问题Redis 在接收多个网络客户端发送的请求操作时，如果有一个客户端和 Redis 的网络连接断开了，Redis 会一直等待该客户端恢复连接吗？为什么？ Redis 不会等待客户端恢复连接。原因是，Redis 的网络连接是由操作系统进行处理的，操作系统内核负责监听网络连接套接字上的连接请求或数据请求，而 Redis 采用了 IO 多路复用机制 epoll，不会阻塞在某一个特定的套接字上。epoll 机制监测到套接字上有请求到达时，就会触发相应的事件，并把事件放到一个队列中，Redis 就会对这个事件队列中的事件进行处理。这样一来，Redis 只用查看和处理事件队列，就可以了。当客户端网络连接断开或恢复时，操作系统会进行处理，并且在客户端能再次发送请求时，把接收到的请求以事件形式通知 Redis。 AOF日志Redis存在的一个问题，一旦服务器宕机，内存中的数据将全部丢失。 AOF 日志原理AOF 日志与数据库的写前日志（Write Ahead Log, WAL）正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存。 传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。 除此之外，AOF 还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。 不过，AOF 也有两个潜在的风险： 如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。 AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。 这两个风险都是和 AOF 写回磁盘的时机相关的。这也就意味着，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除了。 三种写回策略其实，对于这个问题，AOF 机制给我们提供了三个选择，也就是 AOF 配置项 appendfsync 的三个可选值。 Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；（“同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能；） No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。（虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了；） Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；（每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。） 到这里，我们就可以根据系统对高性能和高可靠性的要求，来选择使用哪种写回策略了。 AOF 重写机制AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。这也就意味着，我们一定要小心 AOF 文件过大带来的性能问题。这里的“性能问题”，主要在于以下三个方面： 文件系统本身对文件大小有限制，无法保存过大的文件； 如果文件太大，之后再往里面追加命令记录的话，效率也会变低； 如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。 AOF 重写机制就是来解决上述问题。简单来说，AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。比如说，当读取了键值对“testkey”: “testvalue”之后，重写机制会记录 set testkey testvalue 这条命令。这样，当需要恢复时，可以重新执行该命令，实现“testkey”: “testvalue”的写入。 实际上，重写机制具有“多变一”功能。所谓的“多变一”，也就是说，旧日志文件中的多条命令，在重写后的新日志中变成了一条命令。（AOF 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键值对的写入了。） AOF 重写会阻塞吗?和 AOF 日志由主线程写回不同，重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。我把重写的过程总结为一个拷贝，两处日志。 一个拷贝就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝（这里实际拷贝内存页表（虚拟内存和物理内存的映射索引表）既可以理解为指针，否则会引起内存突然翻倍而剧增）一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。 两处日志因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。 总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。 AOF重写触发时机有两个配置项在控制AOF重写的触发时机： auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认为64MB auto-aof-rewrite-percentage: 这个值的计算方法是：当前AOF文件大小和上一次重写后AOF文件大小的差值，再除以上一次重写后AOF文件大小。也就是当前AOF文件比上一次重写后AOF文件的增量大小，和上一次重写后AOF文件大小的比值。 总结AOF工作原理： 1、Redis 执行 fork() ，现在同时拥有父进程和子进程。2、子进程开始将新 AOF 文件的内容写入到临时文件。3、对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾,这样样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。4、当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。5、搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 AOF 重写过程中有没有其他潜在的阻塞风险？ Redis 主线程 fork 创建 bgrewriteaof 子进程时，内核需要创建用于管理子进程的相关数据结构，这些数据结构在操作系统中通常叫作进程控制块（Process ControlBlock，简称为 PCB）。内核要把主线程的 PCB 内容拷贝给子进程。这个创建和拷贝过程由内核执行，是会阻塞主线程的。而且，在拷贝过程中，子进程要拷贝父进程的页表，这个过程的耗时和 Redis 实例的内存大小有关。如果 Redis 实例内存大，页表就会大，fork执行时间就会长，这就会给主线程带来阻塞风险。 bgrewriteaof 子进程会和主线程共享内存。当主线程收到新写或修改的操作时，主线程会申请新的内存空间，用来保存新写或修改的数据，如果操作的是 bigkey，也就是数据量大的集合类型数据，那么，主线程会因为申请大空间而面临阻塞风险。因为操作系统在分配内存空间时，有查找和锁的开销，这就会导致阻塞。 内存快照内存快照介绍用 AOF 方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。解决方案就是使用内存快照。 内存快照：就是指内存中的数据在某一个时刻的状态记录。对 Redis 来说，它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。这个快照文件就称为 RDB 文件，其中，RDB 就是 Redis DataBase 的缩写。 和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。 Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中，给内存的全量数据做快照，把它们全部写入磁盘也会花费很多时间。而且，全量数据越多，RDB 文件就越大，往磁盘上写数据的时间开销就越大。 对于 Redis 而言，它的单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作，所以，针对任何操作，我们都会提一个灵魂之问：“它会阻塞主线程吗?”RDB 文件的生成是否会阻塞主线程，这就关系到是否会降低 Redis 的性能。 Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave： save：在主线程中执行，会导致阻塞。 bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是Redis RDB 文件生成的默认配置。 这个时候，我们就可以通过 bgsave 命令来执行全量快照，这既提供了数据的可靠性保证，也避免了对 Redis 的性能影响。 快照时数据变更机制我们在做快照时不希望数据“动”，也就是不能被修改。我们说可以使用bgsave异步保存快照，但是主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。 简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。 此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。 这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。 写时复制原理： 快照频率如果间隔时间越短，那么我们在某一时刻如果发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。但是，如果频繁地执行全量快照，也会带来两方面的开销。 频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 那么，有什么其他好方法吗？ 此时，我们可以做增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。（这么做的前提是，我们需要记住哪些数据被修改了。你可不要小瞧这个“记住”功能，它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。）所以，增量快照不是最优解。 Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。 总结这节，我们学习了 Redis 用于避免数据丢失的内存快照方法。这个方法的优势在于，可以快速恢复数据库，也就是只需要把 RDB 文件直接读入内存，这就避免了 AOF 需要顺序、逐一重新执行操作命令带来的低效性能问题。 不过，内存快照也有它的局限性。它拍的是一张内存的“大合影”，不可避免地会耗时耗力。虽然，Redis 设计了 bgsave 和写时复制方式，尽可能减少了内存快照对正常读写的影响，但是，频繁快照仍然是不太能接受的。而混合使用 RDB 和 AOF，正好可以取两者之长，避两者之短，以较小的性能开销保证数据可靠性和性能。 最后，关于 AOF 和 RDB 的选择问题，我想再给你提三点建议： 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择。 如果允许分钟级别的数据丢失，可以只使用 RDB。 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。 如果有 100 个请求，80 个请求执行的是修改操作。在这个场景下，用RDB 做持久化有什么风险吗？ 内存不足的风险：Redis fork 一个 bgsave 子进程进行 RDB 写入，如果主线程再接收到写操作，就会采用写时复制。写时复制需要给写操作的数据分配新的内存空间。本问题中写的比例为 80%，那么，在持久化过程中，为了保存 80% 写操作涉及的数据，写时复制机制会在实例内存中，为这些数据再分配新内存空间，分配的内存量相当于整个实例数据量的 80%。 主线程和子进程竞争使用 CPU 的风险：生成 RDB 的子进程需要 CPU 核运行，主线程本身也需要 CPU 核运行，而且，如果 Redis 还启用了后台线程，此时，主线程、子进程和后台线程都会竞争 CPU 资源。由于云主机只有 2 核 CPU，这就会影响到主线程处理请求的速度。 主从数据同步主动模式概念Redis 具有高可靠性的含义：一是数据尽量少丢失，二是服务尽量少中断。AOF 和 RDB 保证了前者，而对于后者，Redis 的做法就是增加副本冗余量，将一份数据同时保存在多个实例上。Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。 读操作：主库、从库都可以接收； 写操作：首先到主库执行，然后，主库将写操作同步给从库。 使用读写分离的好处是，不管是主库还是从库，都能接收客户端的写操作，那么，一个直接的问题就是：如果客户端对同一个数据（例如 k1）前后修改了三次，每一次的修改请求都发送到不同的实例上，在不同的实例上执行，那么，这个数据在这三个实例上的副本就不一致了（分别是 v1、v2 和 v3）。在读取这个数据的时候，就可能读取到旧的值。如果我们非要保持这个数据在三个实例上一致，就要涉及到加锁、实例间协商是否完成修改等一系列操作，但这会带来巨额的开销，当然是不太能接受的。 而主从库模式一旦采用了读写分离，所有数据的修改只会在主库上进行，不用协调三个实例。主库有了最新的数据后，会同步给从库，这样，主从库的数据就是一致的。 主从库第一次同步流程 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。（从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数。runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。offset，此时设为 -1，表示第一次复制。主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。主库会把当前所有的数据都复制给从库。） 第二阶段，主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。（主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录RDB 文件生成后收到的所有写操作。） 第三阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。 主从级联模式一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成 RDB 文件和传输 RDB 文件。如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。 我们可以通过主 - 从 - 从模式。我们可以通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。我们可以通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。 主从库间网络断开问题到这里，我们了解了主从库间通过全量复制实现数据同步的过程，以及通过“主 -从 - 从”模式分担主库压力的方式。那么，一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为基于长连接的命令传播，可以避免频繁建立连接的开销。 如果网络连接断开怎么办？在 Redis 2.8 之前，如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大。从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。听名字大概就可以猜到它和全量复制的不同：全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库。 当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。 刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。主库接收的新写操作越多，这个值就会越大。同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。 主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset之间的差距。在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset之间的命令操作同步给从库就行。 因为 repl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。（我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。） replication buffer 和 repl_backlog_buffer 的区别replication buffer 是主从库在进行全量复制时，主库上用于和从库连接的客户端的 buffer，而 repl_backlog_buffer 是为了支持从库增量复制，主库上用于持续保存写操作的一块专用 buffer。 Redis 主从库在进行复制时，当主库要把全量复制期间的写操作命令发给从库时，主库会先创建一个客户端，用来连接从库，然后通过这个客户端，把写操作命令发给从库。在内存中，主库上的客户端就会对应一个 buffer，这个 buffer 就被称为 replication buffer。Redis 通过 client_buffer 配置项来控制这个 buffer 的大小。主库会给每个从库建立一个客户端，所以 replication buffer 不是共享的，而是每个从库都有一个对应的客户端。 repl_backlog_buffer 是一块专用 buffer，在 Redis 服务器启动后，开始一直接收写操作命令，这是所有从库共享的。主库和从库会各自记录自己的复制进度，所以，不同的从库\\在进行恢复时，会把自己的复制进度（slave_repl_offset）发给主库，主库就可以和它独立同步。 小结Redis 的主从库同步的基本原理，总结来说，有三种模式：全量复制、基于长连接的命令传播，以及增量复制。全量复制虽然耗时，但是对于从库来说，如果是第一次同步，全量复制是无法避免的，所以，我给你一个小建议：一个 Redis 实例的数据库不要太大，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力。 长连接复制是主从库正常运行后的常规同步阶段。在这个阶段中，主从库之间通过命令传播实现同步。不过，这期间如果遇到了网络断连，增量复制就派上用场了。我特别建议你留意一下 repl_backlog_size 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。所以，通过调大这个参数，可以减少从库在网络断连时全量复制的风险。 主从库间的数据复制同步使用的是RDB 文件，前面我们学习过，AOF 记录的操作命令更全，相比于 RDB 丢失的数据更少。那么，为什么主从库间的复制不使用 AOF 呢？ 相同数据下，AOF 文件比 RDB 更大，因此需要的网络带宽更多； 在恢复数据时，使用RDB更快。 如果使用AOF文件来同步相对来说丢的数据更少，但是不表示不丢数据。即也需要第三个阶段来保证数据的一致性。因此相对来说使用RDB开销更小些。 在主从切换过程中，客户端能否正常地进行请求操作呢？ 从集群一般是采用读写分离模式，当主库故障后，客户端仍然可以把读请求发送给从库，让从库服务。但是，对于写请求操作，客户端就无法执行了。 如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？ 一方面，客户端需要能缓存应用发送的写请求。只要不是同步写操作 Redis 应用场景一般也没有同步写(由此引发的数据库缓存一致性的解决方案，是否那么绝对)，写请求通常不会在应用程序的关键路径上，所以，客户端缓存写请求后，给应用程序返回一个确认就行。 另一方面，主从切换完成后，客户端要能和新主库重新建立连接，哨兵需要提供订阅频道，让客户端能够订阅到新主库的信息。同时，客户端也需要能主动和哨兵通信，询问新主库的信息。 哨兵机制主从库集群模式下，如果从库发生故障了，客户端可以继续向主库或其他从库发送请求，进行相关的操作，但是如果主库发生故障了，那就直接会影响到从库的同步，因为从库没有相应的主库可以进行数据复制操作了。主库挂了就牵扯三个问题： 主库真的挂了吗？ 该选择哪个从库作为主库？ 怎么把新主库的相关信息通知给从库和客户端呢？ 哨兵机制的基本流程哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。 监控监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。 哨兵对主库的下线判断有“主观下线”和“客观下线”两种。那么，为什么会存在两种判断呢？它们的区别和联系是什么呢？ 哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”。这里面会存在误判的情况，误判，就是主库实际并没有下线，但是哨兵误以为它下线了。误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下。一旦哨兵判断主库下线了，就会开始选择新主库，并让从库和新主库进行数据同步，这个过程本身就会有开销（选新主库和主从同步），所以我们需要判断是否有判，以及减少误判。 哨兵机制通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。所以我们通过多个哨兵一起判断，来描述主库下线是客观下线，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。（如果想降低误判率，可以增加哨兵的个数） 选主主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。一般来说，我把哨兵选择新主库的过程称为“筛选 + 打分”。简单来说，我们在多个从库中，先按照一定的筛选条件，把不符合条件的从库去掉。然后，我们再按照一定的规则，给剩下的从库逐个打分，将得分最高的从库选为新主库，如下图： 筛选 要检查从库的当前在线状态，还要判断它之前的网络连接状态。 打分 优先级最高的从库得分高。（用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。比如给内存大的实例设置一个高优先级） 和旧主库同步程度最接近的从库得分高。（主从库同步时有个命令传播的过程。在这个过程中，主库会用master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。从库的 slave_repl_offset 值最大的说明最接近原来的主库同步位置，则得分最高） ID 号小的从库得分高。（每个实例都会有一个 ID，这个 ID 就类似于这里的从库的编号。目前，Redis 在选主库时，有一个默认的规定：在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。） 通知哨兵会执行最后一个任务：通知。在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。 哨兵集群多个实例组成的哨兵集群，即使有哨兵实例出现故障挂掉了，其他哨兵还能继续协作完成主从库切换的工作，包括判定主库是不是处于下线状态，选择新主库，以及通知从库和客户端。在配置哨兵的信息时，我们只需要设置主库的 IP 和端口，并没有配置其他哨兵的连接信息，就可以组成集群，这是什么原理？ 基于 pub/sub 机制的哨兵集群组成哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制。哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口。 哨兵与从库建立连接这是由哨兵向主库发送 INFO 命令来完成的。就像下图所示，哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接。 通过 pub/sub 机制，哨兵之间可以组成集群，同时，哨兵又通过 INFO 命令，获得了从库连接信息，也能和从库建立连接，并进行监控了。 基于 pub/sub 机制的客户端事件通知哨兵不能只和主、从库连接。因为，主从库切换后，客户端也需要知道新主库的连接信息，才能向新主库发送请求操作。所以，哨兵还需要完成把新主库的信息告诉客户端这个任务。 从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。（包括：主库下线各种事件、从库重新配置等事件、新主库切换事件） 具体的操作步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，我们可以在客户端执行订阅命令，来获取不同的事件消息。 由哪个哨兵执行主从切换？确定由哪个哨兵执行主从切换的过程，和主库“客观下线”的判断过程类似，也是一个“投票仲裁”的过程。任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-downby-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。 一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵，quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：第一，拿到半数以上的赞成票；第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。 小结通常，我们在解决一个系统问题的时候，会引入一个新机制，或者设计一层新功能，就像我们在这两节课学习的内容：为了实现主从切换，我们引入了哨兵；为了避免单个哨兵故障后无法进行主从切换，以及为了减少误判率，又引入了哨兵集群；哨兵集群又需要有一些机制来支撑它的正常运行。 哨兵集群的这些关键机制，包括： 基于 pub/sub 机制的哨兵集群组成过程； 基于 INFO 命令的从库列表，这可以帮助哨兵和从库建立连接； 基于哨兵自身的 pub/sub 功能，这实现了客户端和哨兵之间的事件通知。 对于主从切换，当然不是哪个哨兵想执行就可以执行的，否则就乱套了。所以，这就需要哨兵集群在判断了主库“客观下线”后，经过投票仲裁，选举一个 Leader 出来，由它负责实际的主从切换，即由它来完成新主库的选择以及通知从库与客户端。 最后，我想再给你分享一个经验：要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds。我们曾经就踩过一个“坑”。当时，在我们的项目中，因为这个值在不同的哨兵实例上配置不一致，导致哨兵集群一直没有对有故障的主库形成共识，也就没有及时切换主库，最终的结果就是集群服务不稳定。所以，你一定不要忽略这条看似简单的经验。 哨兵实例是不是越多越好呢？如果同时调大 down-after-milliseconds 值，对减少误判是不是也有好处？ 哨兵实例越多，误判率会越低，但是在判定主库下线和选举 Leader 时，实例需要拿到的赞成票数也越多，等待所有哨兵投完票的时间可能也会相应增加，主从库切换的时间也会变长，客户端容易堆积较多的请求操作，可能会导致客户端请求溢出，从而造成请求丢失。如果业务层对 Redis 的操作有响应时间要求，就可能会因为新主库一直没有选定，新操作无法执行而发生超时报警。 调大 down-after-milliseconds 后，可能会导致这样的情况：主库实际已经发生故障了，但是哨兵过了很长时间才判断出来，这就会影响到 Redis 对业务的可用性。 切片集群在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致Redis 响应变慢了。（所以，当我们的数据量非常大时，一味的扩展机器配置，不是好的选择） 切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。回到我们刚刚的场景中，如果把 25GB 的数据平均分成 5 份（当然，也可以不做均分），使用 5 个实例来保存，每个实例只需要保存 5GB 数据。 如何保存更多数据？纵向扩展（scale up）和横向扩展（scale out）： 纵向扩展：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。例如原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。（） 横向扩展：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。 纵向扩展的好处是，实施起来简单、直接。不过，这个方案也面临两个潜在的问题： 当使用 RDB 对数据进行持久化时，如果数据量增加，需要的内存也会增加，主线程 fork 子进程时就可能会阻塞（比如刚刚的例子中的情况）。不过，如果你不要求持久化保存 Redis 数据，那么，纵向扩展会是一个不错的选择。 纵向扩展会受到硬件和成本的限制。这很容易理解，毕竟，把内存从 32GB 扩展到 64GB 还算容易，但是，要想扩充到 1TB，就会面临硬件容量和成本上的限制了。 横向扩展：一个扩展性更好的方案。这是因为，要想保存更多的数据，采用这种方案的话，只用增加 Redis 的实例个数就行了，不用担心单个实例的硬件和成本限制。在面向百万、千万级别的用户规模时，横向扩展的 Redis 切片集群会是一个非常好的选择。 数据切片和实例的对应分布关系使用数据切片我们需要解决两个问题： 数据切片后，在多个实例之间如何分布？ 客户端怎么确定想要访问的数据在哪个实例上？ Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。具体的映射过程分为两大步：首先根据键值对的 key，按照CRC16 算法计算一个 16 bit的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 我们在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。当然， 我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用cluster addslots 命令，指定每个实例上的哈希槽个数。（例如我们给内存大的实例分配更多的槽，内存小的分配相对少的槽）在手动分配哈希槽时，需要把 16384 个槽都分配完，否则Redis 集群无法正常工作。 客户端如何定位数据？在定位键值对数据时，它所处的哈希槽是可以通过计算得到的，这个计算可以在客户端发送请求时来执行。但是，要进一步定位到实例，还需要知道哈希槽分布在哪个实例上。Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。 在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个： 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽； 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。 此时，实例之间还可以通过相互传递消息，获得最新的哈希槽分配信息，但是，客户端是无法主动感知这些变化的。这就会导致，它缓存的分配信息和最新的分配信息就不一致了，那该怎么办呢？ Redis Cluster 方案提供了一种重定向机制，所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。 当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。如下： 12GET hello:key(error) MOVED 13320 172.16.19.5:6379 如果两个实例的数据还正在迁徙中，还有部分数据没有迁徙，在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息，如下所示： 12GET hello:key(error) ASK 13320 172.16.19.5:6379 ASK 命令表示两层含义：第一，表明 Slot 数据还在迁移中；第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端。和 MOVED 命令不同，ASK 命令并不会更新客户端缓存的哈希槽分配信息。所以，在上图中，如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。这也就是说，ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 总结在应对数据量扩容时，虽然增加内存这种纵向扩展的方法简单直接，但是会造成数据库的内存过大，导致性能变慢。Redis 切片集群提供了横向扩展的模式，也就是使用多个实例，并给每个实例配置一定数量的哈希槽，数据可以通过键的哈希值映射到哈希槽，再通过哈希槽分散保存到不同的实例上。这样做的好处是扩展性好，不管有多少数据，切片集群都能应对。 另外，集群的实例增减，或者是为了实现负载均衡而进行的数据重新分布，会导致哈希槽和实例的映射关系发生变化，客户端发送请求时，会收到命令执行报错信息。了解了MOVED 和 ASK 命令，你就不会为这类报错而头疼了。 在 Redis 3.0 之前，Redis 官方并没有提供切片集群方案，但是，其实当时业界已经有了一些切片集群的方案，例如基于客户端分区的 ShardedJedis，基于代理的Codis、Twemproxy 等。这些方案的应用早于 Redis Cluster 方案，在支撑的集群实例规模、集群稳定性、客户端友好性方面也都有着各自的优势。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://jjw-story.github.io/tags/Redis/"},{"name":"基础","slug":"基础","permalink":"https://jjw-story.github.io/tags/基础/"}],"author":"JJW"},{"title":"设计模式","slug":"Design-行为型","date":"2021-02-09T08:12:44.327Z","updated":"2021-02-25T01:43:05.213Z","comments":true,"path":"2021/02/09/Design-行为型/","link":"","permalink":"https://jjw-story.github.io/2021/02/09/Design-行为型/","excerpt":"","text":"行为型设计模式行为型设计模式主要解决的是“类或对象之间的交互”问题。 行为型设计模式比较多，有 11 个，几乎占了 23 种经典设计模式的一半。它们分别是：观察者模式、模板模式、策略模式、职责链模式、状态模式、迭代器模式、访问者模式、备忘录模式、命令模式、解释器模式、中介模式。 观察者模式原理及应用场景观察者模式（Observer Design Pattern）也被称为发布订阅模式（Publish-Subscribe Design Pattern）。定义是：在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知。 设计模式要干的事情就是解耦，创建型模式是将创建和使用代码解耦，结构型模式是将不同功能代码解耦，行为型模式是将不同的行为代码解耦，具体到观察者模式，它将观察者和被观察者代码解耦。借助设计模式，我们利用更好的代码结构，将一大坨代码拆分成职责更单一的小类，让其满足开闭原则、高内聚低耦合等特性，以此来控制和应对代码的复杂性，提高代码的可扩展性。 观察者模式的应用场景非常广泛，小到代码层面的解耦，大到架构层面的系统解耦，再或者一些产品的设计思路，都有这种模式的影子，比如，邮件订阅、RSS Feeds，本质上都是观察者模式。不同的应用场景和需求下，这个模式也有截然不同的实现方式，有同步阻塞的实现方式，也有异步非阻塞的实现方式；有进程内的实现方式，也有跨进程的实现方式。 “生产者 - 消费者”模型和观察者模式的区别发布-订阅模型，是一对多的关系，可以以同步的方式实现，也可以以异步的方式实现。生产-消费模型，是多对多的关系，一般以异步的方式实现。两者都可以达到解耦的作用。 观察者模式异步实现示例： 123456789101112131415161718192021222324252627282930public class UserController &#123; private UserService userService; // 依赖注入 private List&lt;RegObserver&gt; regObservers = new ArrayList&lt;&gt;(); private Executor executor; public UserController(Executor executor) &#123; this.executor = executor; &#125; public void setRegObservers(List&lt;RegObserver&gt; observers) &#123; regObservers.addAll(observers); &#125; public Long register(String telephone, String password) &#123; //省略输入参数的校验代码 //省略userService.register()异常的try-catch代码 long userId = userService.register(telephone, password); for (RegObserver observer : regObservers) &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; observer.handleRegSuccess(userId); &#125; &#125;); &#125; return userId; &#125;&#125; 模板模式模板模式的原理与实现模板模式，全称是模板方法设计模式，英文是 Template Method Design Pattern。 定义：模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤。 这里的“算法”，我们可以理解为广义上的“业务逻辑”，并不特指数据结构和算法中的“算法”。这里的算法骨架就是“模板”，包含算法骨架的方法就是“模板方法”，这也是模板方法模式名字的由来。 代码实现示例： 123456789101112131415161718192021222324252627282930313233343536373839public abstract class AbstractClass &#123; public final void templateMethod() &#123; //... method1(); //... method2(); //... &#125; protected abstract void method1(); protected abstract void method2();&#125;public class ConcreteClass1 extends AbstractClass &#123; @Override protected void method1() &#123; //... &#125; @Override protected void method2() &#123; //... &#125;&#125;public class ConcreteClass2 extends AbstractClass &#123; @Override protected void method1() &#123; //... &#125; @Override protected void method2() &#123; //... &#125;&#125;AbstractClass demo = ConcreteClass1();demo.templateMethod(); 模板模式把一个算法中不变的流程抽象到父类的模板方法 templateMethod() 中，将可变的部分 method1()、method2() 留给子类 ContreteClass1 和 ContreteClass2 来实现。所有的子类都可以复用父类中模板方法定义的流程代码。 在模板模式经典的实现中，模板方法定义为 final，可以避免被子类重写。需要子类重写的方法定义为 abstract，可以强迫子类去实现。不过，在实际项目开发中，模板模式的实现比较灵活，以上两点都不是必须的。 模板模式有两大作用：复用和扩展。其中，复用指的是，所有的子类可以复用父类中提供的模板方法的代码。扩展指的是，框架通过模板模式提供功能扩展点，让框架用户可以在不修改框架源码的情况下，基于扩展点定制化框架的功能。 模板模式与Callback回调函数的区别和联系回调原理解析相对于普通的函数调用来说，回调是一种双向调用关系。A 类事先注册某个函数 F 到 B 类，A 类在调用 B 类的 P 函数的时候，B 类反过来调用 A 类注册给它的 F 函数。这里的 F 函数就是“回调函数”。A 调用 B，B 反过来又调用 A，这种调用机制就叫作“回调”。 示例代码如下： 1234567891011121314151617181920212223public interface ICallback &#123; void methodToCallback();&#125;public class BClass &#123; public void process(ICallback callback) &#123; //... callback.methodToCallback(); //... &#125;&#125;public class AClass &#123; public static void main(String[] args) &#123; BClass b = new BClass(); b.process(new ICallback() &#123; //回调对象 @Override public void methodToCallback() &#123; System.out.println(\"Call back me.\"); &#125; &#125;); &#125;&#125; 回调跟模板模式一样，也具有复用和扩展的功能。除了回调函数之外，BClass 类的 process() 函数中的逻辑都可以复用。如果 ICallback、BClass 类是框架代码，AClass 是使用框架的客户端代码，我们可以通过 ICallback 定制 process() 函数，也就是说，框架因此具有了扩展的能力。 回调可以分为同步回调和异步回调（或者延迟回调）。同步回调指在函数返回之前执行回调函数；异步回调指的是在函数返回之后执行回调函数。 模板和回调的区别从应用场景上来看，同步回调跟模板模式几乎一致。它们都是在一个大的算法骨架中，自由替换其中的某个步骤，起到代码复用和扩展的目的。而异步回调跟模板模式有较大差别，更像是观察者模式。 从代码实现上来看，回调和模板模式完全不同。回调基于组合关系来实现，把一个对象传递给另一个对象，是一种对象之间的关系；模板模式基于继承关系来实现，子类重写父类的抽象方法，是一种类之间的关系。 回调相对于模板模式会更加灵活，主要体现在下面几点： 像 Java 这种只支持单继承的语言，基于模板模式编写的子类，已经继承了一个父类，不再具有继承的能力。 回调可以使用匿名类来创建回调对象，可以不用事先定义类；而模板模式针对不同的实现都要定义不同的子类。 如果某个类中定义了多个模板方法，每个方法都有对应的抽象方法，那即便我们只用到其中的一个模板方法，子类也必须实现所有的抽象方法。而回调就更加灵活，我们只需要往用到的模板方法中注入回调对象即可。 策略模式策略模式，英文全称是 Strategy Design Pattern。 定义：定义一族算法类，将每个算法分别封装起来，让它们可以互相替换。策略模式可以使算法的变化独立于使用它们的客户端（这里的客户端代指使用算法的代码）。 策略模式代码示例： 1234567891011121314151617181920212223242526272829303132333435// 策略的定义public interface Strategy &#123; void algorithmInterface();&#125;public class ConcreteStrategyA implements Strategy &#123; @Override public void algorithmInterface() &#123; //具体的算法... &#125;&#125;public class ConcreteStrategyB implements Strategy &#123; @Override public void algorithmInterface() &#123; //具体的算法... &#125;&#125;// 策略的创建public class StrategyFactory &#123; private static final Map&lt;String, Strategy&gt; strategies = new HashMap&lt;&gt;(); static &#123; strategies.put(\"A\", new ConcreteStrategyA()); strategies.put(\"B\", new ConcreteStrategyB()); &#125; public static Strategy getStrategy(String type) &#123; if (type == null || type.isEmpty()) &#123; throw new IllegalArgumentException(\"type should not be empty.\"); &#125; return strategies.get(type); &#125;&#125; 策略模式用来解耦策略的定义、创建、使用。实际上，一个完整的策略模式就是由这三个部分组成的。 策略类的定义比较简单，包含一个策略接口和一组实现这个接口的策略类。 策略的创建由工厂类来完成，封装策略创建的细节。 策略模式包含一组策略可选，客户端代码如何选择使用哪个策略，有两种确定方法：编译时静态确定和运行时动态确定。其中，“运行时动态确定”才是策略模式最典型的应用场景。 一提到策略模式，有人就觉得，它的作用是避免 if-else 分支判断逻辑。实际上，这种认识是很片面的。策略模式主要的作用还是解耦策略的定义、创建和使用，控制代码的复杂度，让每个部分都不至于过于复杂、代码量过多。除此之外，对于复杂代码来说，策略模式还能让其满足开闭原则，添加新策略的时候，最小化、集中化代码改动，减少引入 bug 的风险。 实际上，设计原则和思想比设计模式更加普适和重要。掌握了代码的设计原则和思想，我们能更清楚的了解，为什么要用某种设计模式，就能更恰到好处地应用设计模式。 策略模式和工厂模式区别 工厂模式: 1.目的是创建不同且相关的对象2.侧重于”创建对象”3.实现方式上可以通过父类或者接口4.一般创建对象应该是现实世界中某种事物的映射，有它自己的属性与方法！ 策略模式: 1.目的实现方便地替换不同的算法类2.侧重于算法(行为)实现3.实现主要通过接口4.创建对象对行为的抽象而非对对象的抽象，很可能没有属于自己的属性。 职责链模式职责链模式的英文翻译是 Chain Of Responsibility Design Pattern。 定义：将请求的发送和接收解耦，让多个接收对象都有机会处理这个请求。将这些接收对象串成一条链，并沿着这条链传递这个请求，直到链上的某个接收对象能够处理它为止。 在职责链模式中，多个处理器（也就是刚刚定义中说的“接收对象”）依次处理同一个请求。一个请求先经过 A 处理器处理，然后再把请求传递给 B 处理器，B 处理器处理完后再传递给 C 处理器，以此类推，形成一个链条。链条上的每个处理器各自承担各自的处理职责，所以叫作职责链模式。 职责链模式代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 数组实现public interface IHandler &#123; boolean handle();&#125;public class HandlerA implements IHandler &#123; @Override public boolean handle() &#123; boolean handled = false; //... return handled; &#125;&#125;public class HandlerB implements IHandler &#123; @Override public boolean handle() &#123; boolean handled = false; //... return handled; &#125;&#125;public class HandlerChain &#123; private List&lt;IHandler&gt; handlers = new ArrayList&lt;&gt;(); public void addHandler(IHandler handler) &#123; this.handlers.add(handler); &#125; public void handle() &#123; for (IHandler handler : handlers) &#123; boolean handled = handler.handle(); if (handled) &#123; break; &#125; &#125; &#125;&#125;// 使用举例public class Application &#123; public static void main(String[] args) &#123; HandlerChain chain = new HandlerChain(); chain.addHandler(new HandlerA()); chain.addHandler(new HandlerB()); chain.handle(); &#125;&#125; 职责链模式有两种常用的实现。一种是使用链表来存储处理器，另一种是使用数组来存储处理器，后面一种实现方式更加简单。 状态模式状态模式一般用来实现状态机，而状态机常用在游戏、工作流引擎等系统开发中。不过，状态机的实现方式有多种，除了状态模式，比较常用的还有分支逻辑法和查表法。 状态机有 3 个组成部分：状态（State）、事件（Event）、动作（Action）。其中，事件也称为转移条件（Transition Condition）。事件触发状态的转移及动作的执行。不过，动作不是必须的，也可能只转移状态，不执行任何动作。 分支逻辑法：利用 if-else 或者 switch-case 分支逻辑，参照状态转移图，将每一个状态转移原模原样地直译成代码。对于简单的状态机来说，这种实现方式最简单、最直接，是首选。 查表法：对于状态很多、状态转移比较复杂的状态机来说，查表法比较合适。通过二维数组来表示状态转移图，能极大地提高代码的可读性和可维护性。 状态模式：对于状态并不多、状态转移也比较简单，但事件触发执行的动作包含的业务逻辑可能比较复杂的状态机来说，我们首选这种实现方式。 状态模式代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public interface IMario &#123; //所有状态类的接口 State getName(); //以下是定义的事件 void obtainMushRoom(); void obtainCape(); void obtainFireFlower(); void meetMonster();&#125;public class SmallMario implements IMario &#123; private MarioStateMachine stateMachine; public SmallMario(MarioStateMachine stateMachine) &#123; this.stateMachine = stateMachine; &#125; @Override public State getName() &#123; return State.SMALL; &#125; @Override public void obtainMushRoom() &#123; stateMachine.setCurrentState(new SuperMario(stateMachine)); stateMachine.setScore(stateMachine.getScore() + 100); &#125; @Override public void obtainCape() &#123; stateMachine.setCurrentState(new CapeMario(stateMachine)); stateMachine.setScore(stateMachine.getScore() + 200); &#125; @Override public void obtainFireFlower() &#123; stateMachine.setCurrentState(new FireMario(stateMachine)); stateMachine.setScore(stateMachine.getScore() + 300); &#125; @Override public void meetMonster() &#123; // do nothing... &#125;&#125;// 省略SuperMario、CapeMario、FireMario类...public class MarioStateMachine &#123; private int score; private IMario currentState; // 不再使用枚举来表示状态 public MarioStateMachine() &#123; this.score = 0; this.currentState = new SmallMario(this); &#125; public void obtainMushRoom() &#123; this.currentState.obtainMushRoom(); &#125; public void obtainCape() &#123; this.currentState.obtainCape(); &#125; public void obtainFireFlower() &#123; this.currentState.obtainFireFlower(); &#125; public void meetMonster() &#123; this.currentState.meetMonster(); &#125; public int getScore() &#123; return this.score; &#125; public State getCurrentState() &#123; return this.currentState.getName(); &#125; public void setScore(int score) &#123; this.score = score; &#125; public void setCurrentState(IMario currentState) &#123; this.currentState = currentState; &#125;&#125; 迭代器模式迭代器模式（Iterator Design Pattern），也叫作游标模式（Cursor Design Pattern）。它用来遍历集合对象。这里说的“集合对象”也可以叫“容器”“聚合对象”，实际上就是包含一组对象的对象，比如数组、链表、树、图、跳表。迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一。 一个完整的迭代器模式，一般会涉及容器和容器迭代器两部分内容。为了达到基于接口而非实现编程的目的，容器又包含容器接口、容器实现类，迭代器又包含迭代器接口、迭代器实现类。容器中需要定义 iterator() 方法，用来创建迭代器。迭代器接口中需要定义 hasNext()、currentItem()、next() 三个最基本的方法。容器对象通过依赖注入传递到迭代器类中。 遍历集合一般有三种方式：for 循环、foreach 循环、迭代器遍历。后两种本质上属于一种，都可以看作迭代器遍历。相对于 for 循环遍历，利用迭代器来遍历有下面三个优势： 迭代器模式封装集合内部的复杂数据结构，开发者不需要了解如何遍历，直接使用容器提供的迭代器即可； 迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一； 迭代器模式让添加新的遍历算法更加容易，更符合开闭原则。除此之外，因为迭代器都实现自相同的接口，在开发中，基于接口而非实现编程，替换迭代器也变得更加容易。 在通过迭代器来遍历集合元素的同时，增加或者删除集合中的元素，有可能会导致某个元素被重复遍历或遍历不到。不过，并不是所有情况下都会遍历出错，有的时候也可以正常遍历，所以，这种行为称为结果不可预期行为或者未决行为。实际上，“不可预期”比直接出错更加可怕，有的时候运行正确，有的时候运行错误，一些隐藏很深、很难 debug 的 bug 就是这么产生的。 有两种比较干脆利索的解决方案，来避免出现这种不可预期的运行结果。一种是遍历的时候不允许增删元素，另一种是增删元素之后让遍历报错。第一种解决方案比较难实现，因为很难确定迭代器使用结束的时间点（比如用户遍历中途提前break返回）。第二种解决方案更加合理。Java 语言就是采用的这种解决方案。增删元素之后，我们选择 fail-fast 解决方式，让遍历操作直接抛出运行时异常。具体如下： 我们在 ArrayList 中定义一个成员变量 modCount，记录集合被修改的次数，集合每调用一次增加或删除元素的函数，就会给 modCount 加 1。当通过调用集合上的 iterator() 函数来创建迭代器的时候，我们把 modCount 值传递给迭代器的 expectedModCount 成员变量，之后每次调用迭代器上的 hasNext()、next()、currentItem() 函数，我们都会检查集合上的 modCount 是否等于 expectedModCount，也就是看，在创建完迭代器之后，modCount 是否改变过。 访问者模式访问者者模式的英文翻译是 Visitor Design Pattern。访问者模式允许一个或者多个操作应用到一组对象上，设计意图是解耦操作和对象本身，保持类职责单一、满足开闭原则以及应对代码的复杂性。 代码演进如下： 最初版功能实现：这个版本因为Java的静态绑定，所以编译通过不了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public abstract class ResourceFile &#123; protected String filePath; public ResourceFile(String filePath) &#123; this.filePath = filePath; &#125;&#125;public class PdfFile extends ResourceFile &#123; public PdfFile(String filePath) &#123; super(filePath); &#125; //...&#125;//...PPTFile、WordFile代码省略...public class Extractor &#123; public void extract2txt(PPTFile pptFile) &#123; //... System.out.println(\"Extract PPT.\"); &#125; public void extract2txt(PdfFile pdfFile) &#123; //... System.out.println(\"Extract PDF.\"); &#125; public void extract2txt(WordFile wordFile) &#123; //... System.out.println(\"Extract WORD.\"); &#125;&#125;public class ToolApplication &#123; public static void main(String[] args) &#123; Extractor extractor = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for (ResourceFile resourceFile : resourceFiles) &#123; extractor.extract2txt(resourceFile); &#125; &#125; private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory) &#123; List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;&gt;(); //...根据后缀(pdf/ppt/word)由工厂方法创建不同的类对象(PdfFile/PPTFile/WordFile) resourceFiles.add(new PdfFile(\"a.pdf\")); resourceFiles.add(new WordFile(\"b.word\")); resourceFiles.add(new PPTFile(\"c.ppt\")); return resourceFiles; &#125;&#125; 第一版：解决静态参数绑定的问题 123456789101112131415161718192021222324252627282930313233343536373839404142public abstract class ResourceFile &#123; protected String filePath; public ResourceFile(String filePath) &#123; this.filePath = filePath; &#125; abstract public void accept(Extractor extractor);&#125;public class PdfFile extends ResourceFile &#123; public PdfFile(String filePath) &#123; super(filePath); &#125; @Override public void accept(Extractor extractor) &#123; extractor.extract2txt(this); &#125; //...&#125;//...PPTFile、WordFile跟PdfFile类似，这里就省略了...//...Extractor代码不变...public class ToolApplication &#123; public static void main(String[] args) &#123; Extractor extractor = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for (ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(extractor); &#125; &#125; private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory) &#123; List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;&gt;(); //...根据后缀(pdf/ppt/word)由工厂方法创建不同的类对象(PdfFile/PPTFile/WordFile) resourceFiles.add(new PdfFile(\"a.pdf\")); resourceFiles.add(new WordFile(\"b.word\")); resourceFiles.add(new PPTFile(\"c.ppt\")); return resourceFiles; &#125;&#125; 第二版：继续添加新的功能，根据不同的文件类型，使用不同的压缩算法来压缩资源文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public abstract class ResourceFile &#123; protected String filePath; public ResourceFile(String filePath) &#123; this.filePath = filePath; &#125; abstract public void accept(Extractor extractor); abstract public void accept(Compressor compressor);&#125;public class PdfFile extends ResourceFile &#123; public PdfFile(String filePath) &#123; super(filePath); &#125; @Override public void accept(Extractor extractor) &#123; extractor.extract2txt(this); &#125; @Override public void accept(Compressor compressor) &#123; compressor.compress(this); &#125; //...&#125;&#125;//...PPTFile、WordFile跟PdfFile类似，这里就省略了...//...Extractor代码不变public class ToolApplication &#123; public static void main(String[] args) &#123; Extractor extractor = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for (ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(extractor); &#125; Compressor compressor = new Compressor(); for(ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(compressor); &#125; &#125; private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory) &#123; List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;&gt;(); //...根据后缀(pdf/ppt/word)由工厂方法创建不同的类对象(PdfFile/PPTFile/WordFile) resourceFiles.add(new PdfFile(\"a.pdf\")); resourceFiles.add(new WordFile(\"b.word\")); resourceFiles.add(new PPTFile(\"c.ppt\")); return resourceFiles; &#125;&#125; 第三版：上面代码还存在一些问题，添加一个新的业务，还是需要修改每个资源文件类，违反了开闭原则。针对这个问题，我们抽象出来一个 Visitor 接口，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public abstract class ResourceFile &#123; protected String filePath; public ResourceFile(String filePath) &#123; this.filePath = filePath; &#125; abstract public void accept(Visitor vistor);&#125;public class PdfFile extends ResourceFile &#123; public PdfFile(String filePath) &#123; super(filePath); &#125; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; //...&#125;//...PPTFile、WordFile跟PdfFile类似，这里就省略了...public interface Visitor &#123; void visit(PdfFile pdfFile); void visit(PPTFile pdfFile); void visit(WordFile pdfFile);&#125;public class Extractor implements Visitor &#123; @Override public void visit(PPTFile pptFile) &#123; //... System.out.println(\"Extract PPT.\"); &#125; @Override public void visit(PdfFile pdfFile) &#123; //... System.out.println(\"Extract PDF.\"); &#125; @Override public void visit(WordFile wordFile) &#123; //... System.out.println(\"Extract WORD.\"); &#125;&#125;public class Compressor implements Visitor &#123; @Override public void visit(PPTFile pptFile) &#123; //... System.out.println(\"Compress PPT.\"); &#125; @Override public void visit(PdfFile pdfFile) &#123; //... System.out.println(\"Compress PDF.\"); &#125; @Override public void visit(WordFile wordFile) &#123; //... System.out.println(\"Compress WORD.\"); &#125;&#125;public class ToolApplication &#123; public static void main(String[] args) &#123; Extractor extractor = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for (ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(extractor); &#125; Compressor compressor = new Compressor(); for(ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(compressor); &#125; &#125; private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory) &#123; List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;&gt;(); //...根据后缀(pdf/ppt/word)由工厂方法创建不同的类对象(PdfFile/PPTFile/WordFile) resourceFiles.add(new PdfFile(\"a.pdf\")); resourceFiles.add(new WordFile(\"b.word\")); resourceFiles.add(new PPTFile(\"c.ppt\")); return resourceFiles; &#125;&#125; Double Dispatch。在面向对象编程语言中，方法调用可以理解为一种消息传递（Dispatch）。一个对象调用另一个对象的方法，就相当于给它发送一条消息，这条消息起码要包含对象名、方法名和方法参数。 所谓 Single Dispatch，指的是执行哪个对象的方法，根据对象的运行时类型来决定；执行对象的哪个方法，根据方法参数的编译时类型来决定。所谓 Double Dispatch，指的是执行哪个对象的方法，根据对象的运行时类型来决定；执行对象的哪个方法，根据方法参数的运行时类型来决定。 具体到编程语言的语法机制，Single Dispatch 和 Double Dispatch 跟多态和函数重载直接相关。当前主流的面向对象编程语言（比如，Java、C++、C#）都只支持 Single Dispatch，不支持 Double Dispatch。 访问者模式解决的问题其实就是Java等语言不支持双分派的问题。 备忘录模式备忘录模式也叫快照模式，具体来说，就是在不违背封装原则的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便之后恢复对象为先前的状态。这个模式的定义表达了两部分内容：一部分是，存储副本以便后期恢复；另一部分是，要在不违背封装原则的前提下，进行对象的备份和恢复。 备忘录模式的应用场景也比较明确和有限，主要是用来防丢失、撤销、恢复等。它跟平时我们常说的“备份”很相似。两者的主要区别在于，备忘录模式更侧重于代码的设计和实现，备份更侧重架构设计或产品设计。 对于大对象的备份来说，备份占用的存储空间会比较大，备份和恢复的耗时会比较长。针对这个问题，不同的业务场景有不同的处理方式。比如，只备份必要的恢复信息，结合最新的数据来恢复；再比如，全量备份和增量备份相结合，低频全量备份，高频增量备份，两者结合来做恢复。 代码示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class InputText &#123; private StringBuilder text = new StringBuilder(); public String getText() &#123; return text.toString(); &#125; public void append(String input) &#123; text.append(input); &#125; public Snapshot createSnapshot() &#123; return new Snapshot(text.toString()); &#125; public void restoreSnapshot(Snapshot snapshot) &#123; this.text.replace(0, this.text.length(), snapshot.getText()); &#125;&#125;public class Snapshot &#123; private String text; public Snapshot(String text) &#123; this.text = text; &#125; public String getText() &#123; return this.text; &#125;&#125;public class SnapshotHolder &#123; private Stack&lt;Snapshot&gt; snapshots = new Stack&lt;&gt;(); public Snapshot popSnapshot() &#123; return snapshots.pop(); &#125; public void pushSnapshot(Snapshot snapshot) &#123; snapshots.push(snapshot); &#125;&#125;public class ApplicationMain &#123; public static void main(String[] args) &#123; InputText inputText = new InputText(); SnapshotHolder snapshotsHolder = new SnapshotHolder(); Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) &#123; String input = scanner.next(); if (input.equals(\":list\")) &#123; System.out.println(inputText.toString()); &#125; else if (input.equals(\":undo\")) &#123; Snapshot snapshot = snapshotsHolder.popSnapshot(); inputText.restoreSnapshot(snapshot); &#125; else &#123; snapshotsHolder.pushSnapshot(inputText.createSnapshot()); inputText.append(input); &#125; &#125; &#125;&#125; 命令模式命令模式将请求（命令）封装为一个对象，这样可以使用不同的请求参数化其他对象（将不同请求依赖注入到其他对象），并且能够支持请求（命令）的排队执行、记录日志、撤销等（附加控制）功能。 落实到编码实现，命令模式用到最核心的实现手段，就是将函数封装成对象。我们知道，在大部分编程语言中，函数是没法作为参数传递给其他函数的，也没法赋值给变量。借助命令模式，我们将函数封装成对象，这样就可以实现把函数像对象一样使用。 命令模式的主要作用和应用场景，是用来控制命令的执行，比如，异步、延迟、排队执行命令、撤销重做命令、存储命令、给命令记录日志等等，这才是命令模式能发挥独一无二作用的地方。 代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public interface Command &#123; void execute();&#125;public class GotDiamondCommand implements Command &#123; // 省略成员变量 public GotDiamondCommand(/*数据*/) &#123; //... &#125; @Override public void execute() &#123; // 执行相应的逻辑 &#125;&#125;//GotStartCommand/HitObstacleCommand/ArchiveCommand类省略public class GameApplication &#123; private static final int MAX_HANDLED_REQ_COUNT_PER_LOOP = 100; private Queue&lt;Command&gt; queue = new LinkedList&lt;&gt;(); public void mainloop() &#123; while (true) &#123; List&lt;Request&gt; requests = new ArrayList&lt;&gt;(); //省略从epoll或者select中获取数据，并封装成Request的逻辑， //注意设置超时时间，如果很长时间没有接收到请求，就继续下面的逻辑处理。 for (Request request : requests) &#123; Event event = request.getEvent(); Command command = null; if (event.equals(Event.GOT_DIAMOND)) &#123; command = new GotDiamondCommand(/*数据*/); &#125; else if (event.equals(Event.GOT_STAR)) &#123; command = new GotStartCommand(/*数据*/); &#125; else if (event.equals(Event.HIT_OBSTACLE)) &#123; command = new HitObstacleCommand(/*数据*/); &#125; else if (event.equals(Event.ARCHIVE)) &#123; command = new ArchiveCommand(/*数据*/); &#125; // ...一堆else if... queue.add(command); &#125; int handledCount = 0; while (handledCount &lt; MAX_HANDLED_REQ_COUNT_PER_LOOP) &#123; if (queue.isEmpty()) &#123; break; &#125; Command command = queue.poll(); command.execute(); &#125; &#125; &#125;&#125; 命令模式跟策略模式的区别：在策略模式中，不同的策略具有相同的目的、不同的实现、互相之间可以替换。比如，BubbleSort、SelectionSort 都是为了实现排序的，只不过一个是用冒泡排序算法来实现的，另一个是用选择排序算法来实现的。而在命令模式中，不同的命令具有不同的目的，对应不同的处理逻辑，并且互相之间不可替换。 解释器模式解释器模式为某个语言定义它的语法（或者叫文法）表示，并定义一个解释器用来处理这个语法。实际上，这里的“语言”不仅仅指我们平时说的中、英、日、法等各种语言。从广义上来讲，只要是能承载信息的载体，我们都可以称之为“语言”，比如，古代的结绳记事、盲文、哑语、摩斯密码等。 要想了解“语言”要表达的信息，我们就必须定义相应的语法规则。这样，书写者就可以根据语法规则来书写“句子”（专业点的叫法应该是“表达式”），阅读者根据语法规则来阅读“句子”，这样才能做到信息的正确传递。而我们要讲的解释器模式，其实就是用来实现根据语法规则解读“句子”的解释器。 解释器模式的代码实现比较灵活，没有固定的模板。我们前面说过，应用设计模式主要是应对代码的复杂性，解释器模式也不例外。它的代码实现的核心思想，就是将语法解析的工作拆分到各个小类中，以此来避免大而全的解析类。一般的做法是，将语法规则拆分一些小的独立的单元，然后对每个单元进行解析，最终合并为对整个语法规则的解析。 中介模式中介模式的设计思想跟中间层很像，通过引入中介这个中间层，将一组对象之间的交互关系（或者依赖关系）从多对多（网状关系）转换为一对多（星状关系）。原来一个对象要跟 n 个对象交互，现在只需要跟一个中介对象交互，从而最小化对象之间的交互关系，降低了代码的复杂度，提高了代码的可读性和可维护性。 观察者模式和中介模式都是为了实现参与者之间的解耦，简化交互关系。两者的不同在于应用场景上。在观察者模式的应用场景中，参与者之间的交互比较有条理，一般都是单向的，一个参与者只有一个身份，要么是观察者，要么是被观察者。而在中介模式的应用场景中，参与者之间的交互关系错综复杂，既可以是消息的发送者、也可以同时是消息的接收者 设计模式重点每个设计模式都应该由两部分组成：第一部分是应用场景，即这个模式可以解决哪类问题；第二部分是解决方案，即这个模式的设计思路和具体的代码实现。不过，代码实现并不是模式必须包含的。如果你单纯地只关注解决方案这一部分，甚至只关注代码实现，就会产生大部分模式看起来都很相似的错觉。 实际上，设计模式之间的主要区别还是在于设计意图，也就是应用场景。单纯地看设计思路或者代码实现，有些模式确实很相似，比如策略模式和工厂模式。 之前讲策略模式的时候，我们有讲到，策略模式包含策略的定义、创建和使用三部分，从代码结构上来，它非常像工厂模式。它们的区别在于，策略模式侧重“策略”或“算法”这个特定的应用场景，用来解决根据运行时状态从一组策略中选择不同策略的问题，而工厂模式侧重封装对象的创建过程，这里的对象没有任何业务场景的限定，可以是策略，但也可以是其他东西。从设计意图上来，这两个模式完全是两回事儿。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://jjw-story.github.io/categories/设计模式/"}],"tags":[{"name":"Design","slug":"Design","permalink":"https://jjw-story.github.io/tags/Design/"},{"name":"行为型设计模式","slug":"行为型设计模式","permalink":"https://jjw-story.github.io/tags/行为型设计模式/"}],"author":"JJW"},{"title":"设计模式","slug":"Design-结构型","date":"2021-02-05T09:49:37.822Z","updated":"2021-02-05T10:00:41.658Z","comments":true,"path":"2021/02/05/Design-结构型/","link":"","permalink":"https://jjw-story.github.io/2021/02/05/Design-结构型/","excerpt":"","text":"结构型设计模式结构型模式主要总结了一些类或对象组合在一起的经典结构，这些经典的结构可以解决特定应用场景的问题。结构型模式包括：代理模式、桥接模式、装饰器模式、适配器模式、门面模式、组合模式、享元模式。 代理模式代理模式的原理与实现在不改变原始类（或叫被代理类）的情况下，通过引入代理类来给原始类附加功能。一般情况下，我们让代理类和原始类实现同样的接口。但是，如果原始类并没有定义接口，并且原始类代码并不是我们开发维护的。在这种情况下，我们可以通过让代理类继承原始类的方法来实现代理模式。 动态代理的原理与实现静态代理需要针对每个类都创建一个代理类，并且每个代理类中的代码都有点像模板式的“重复”代码，增加了维护成本和开发成本。对于静态代理存在的问题，我们可以通过动态代理来解决。我们不事先为每个原始类编写代理类，而是在运行的时候动态地创建原始类对应的代理类，然后在系统中用代理类替换掉原始类。 代理模式的应用场景代理模式常用在业务系统中开发一些非功能性需求，比如：监控、统计、鉴权、限流、事务、幂等、日志。我们将这些附加功能与业务功能解耦，放到代理类统一处理，让程序员只需要关注业务方面的开发。除此之外，代理模式还可以用在 RPC、缓存等应用场景中。 动态代理示例12345678910111213141516171819202122232425262728293031323334353637public class MetricsCollectorProxy &#123; private MetricsCollector metricsCollector; public MetricsCollectorProxy() &#123; this.metricsCollector = new MetricsCollector(); &#125; public Object createProxy(Object proxiedObject) &#123; Class&lt;?&gt;[] interfaces = proxiedObject.getClass().getInterfaces(); DynamicProxyHandler handler = new DynamicProxyHandler(proxiedObject); return Proxy.newProxyInstance(proxiedObject.getClass().getClassLoader(), interfaces, handler); &#125; private class DynamicProxyHandler implements InvocationHandler &#123; private Object proxiedObject; public DynamicProxyHandler(Object proxiedObject) &#123; this.proxiedObject = proxiedObject; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; long startTimestamp = System.currentTimeMillis(); Object result = method.invoke(proxiedObject, args); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; String apiName = proxiedObject.getClass().getName() + \":\" + method.getName(); RequestInfo requestInfo = new RequestInfo(apiName, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return result; &#125; &#125;&#125;//MetricsCollectorProxy使用举例MetricsCollectorProxy proxy = new MetricsCollectorProxy();IUserController userController = (IUserController) proxy.createProxy(new UserController()); 桥接模式桥接模式，也叫作桥梁模式，英文是 Bridge Design Pattern。 对于这个模式有两种不同的理解方式。在 GoF 的《设计模式》一书中，桥接模式被定义为：“将抽象和实现解耦，让它们可以独立变化。”在其他资料和书籍中，还有另外一种更加简单的理解方式：“一个类存在两个（或多个）独立变化的维度，我们通过组合的方式，让这两个（或多个）维度可以独立进行扩展。” 个人理解： 举个很简单的例子，现在有两个纬度: Car 车 （奔驰、宝马、奥迪等） Transmission 档位类型 （自动挡、手动挡、手自一体等） 按照继承的设计模式，Car是一个Abstract基类，假设有M个车品牌，N个档位一共要写MN个类去描述所有车和档位的结合。而当我们使用桥接模式的话，我首先new一个具体的Car（如奔驰），再new一个具体的Transmission（比如自动档），然后奔驰.set(手动档)就可以了。那么这种模式只有M+N个类就可以描述所有类型，这就是MN的继承类爆炸简化成了M+N组合。 所以桥接模式解决的应该是继承爆炸问题。可以看作是两个abstract组合在一起，独立去拓展，在运行之前将两个具体实现组合到一起。 装饰器模式装饰器模式主要解决继承关系过于复杂的问题，通过组合来替代继承。它主要的作用是给原始类添加增强功能。这也是判断是否该用装饰器模式的一个重要的依据。除此之外，装饰器模式还有一个特点，那就是可以对原始类嵌套使用多个装饰器。为了满足这个应用场景，在设计的时候，装饰器类需要跟原始类继承相同的抽象类或者接口。 装饰器模式的特点第一个比较特殊的地方是：装饰器类和原始类继承同样的父类，这样我们可以对原始类“嵌套”多个装饰器类。 比如，下面这样一段代码，我们对 FileInputStream 嵌套了两个装饰器类：BufferedInputStream 和 DataInputStream，让它既支持缓存读取，又支持按照基本数据类型来读取数据。 1234InputStream in = new FileInputStream(\"/user/admin/test.txt\");InputStream bin = new BufferedInputStream(in);DataInputStream din = new DataInputStream(bin);int data = din.readInt(); 第二个比较特殊的地方是：装饰器类是对功能的增强，这也是装饰器模式应用场景的一个重要特点。装饰器是对原有功能的扩展，代理是增加并不相关的功能。 实际上，符合“组合关系”这种代码结构的设计模式有很多，比如之前讲过的代理模式、桥接模式，还有现在的装饰器模式。尽管它们的代码结构很相似，但是每种设计模式的意图是不同的。就拿比较相似的代理模式和装饰器模式来说吧，代理模式中，代理类附加的是跟原始类无关的功能，而在装饰器模式中，装饰器类附加的是跟原始类相关的增强功能。 123456789101112131415161718192021222324252627282930313233343536373839// 代理模式的代码结构(下面的接口也可以替换成抽象类)public interface IA &#123; void f();&#125;public class A impelements IA &#123; public void f() &#123; //... &#125;&#125;public class AProxy impements IA &#123; private IA a; public AProxy(IA a) &#123; this.a = a; &#125; public void f() &#123; // 新添加的代理逻辑 a.f(); // 新添加的代理逻辑 &#125;&#125;// 装饰器模式的代码结构(下面的接口也可以替换成抽象类)public interface IA &#123; void f();&#125;public class A impelements IA &#123; public void f() &#123; //... &#125;&#125;public class ADecorator impements IA &#123; private IA a; public ADecorator(IA a) &#123; this.a = a; &#125; public void f() &#123; // 功能增强代码 a.f(); // 功能增强代码 &#125;&#125; 代理模式和装饰器模式对比的最优解释： 你是一个优秀的歌手，只会唱歌这一件事，不擅长找演唱机会，谈价钱，搭台，这些事情你可以找一个经纪人帮你搞定，经纪人帮你做好这些事情你就可以安稳的唱歌了，让经纪人做你不关心的事情这叫代理模式。你老爱记错歌词，歌迷和媒体经常吐槽你没有认真对待演唱会，于是你想了一个办法，买个高端耳机，边唱边提醒你歌词，让你摆脱了忘歌词的诟病，高端耳机让你唱歌能力增强，提高了基础能力这叫装饰者模式。 适配器模式适配器模式的英文翻译是 Adapter Design Pattern。顾名思义，这个模式就是用来做适配的，它将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类可以一起工作。对于这个模式，有一个经常被拿来解释它的例子，就是 USB 转接头充当适配器，把两种不兼容的接口，通过转接变得可以一起工作。 适配器模式有两种实现方式：类适配器和对象适配器。其中，类适配器使用继承关系来实现，对象适配器使用组合关系来实现。具体的代码实现如下所示。其中，ITarget 表示要转化成的接口定义。Adaptee 是一组不兼容 ITarget 接口定义的接口，Adaptor 将 Adaptee 转化成一组符合 ITarget 接口定义的接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 类适配器: 基于继承public interface ITarget &#123; void f1(); void f2(); void fc();&#125;public class Adaptee &#123; public void fa() &#123; //... &#125; public void fb() &#123; //... &#125; public void fc() &#123; //... &#125;&#125;public class Adaptor extends Adaptee implements ITarget &#123; public void f1() &#123; super.fa(); &#125; public void f2() &#123; //...重新实现f2()... &#125; // 这里fc()不需要实现，直接继承自Adaptee，这是跟对象适配器最大的不同点&#125;// 对象适配器：基于组合public interface ITarget &#123; void f1(); void f2(); void fc();&#125;public class Adaptee &#123; public void fa() &#123; //... &#125; public void fb() &#123; //... &#125; public void fc() &#123; //... &#125;&#125;public class Adaptor implements ITarget &#123; private Adaptee adaptee; public Adaptor(Adaptee adaptee) &#123; this.adaptee = adaptee; &#125; public void f1() &#123; adaptee.fa(); //委托给Adaptee &#125; public void f2() &#123; //...重新实现f2()... &#125; public void fc() &#123; adaptee.fc(); &#125;&#125; 针对这两种实现方式，在实际的开发中，到底该如何选择使用哪一种呢？判断的标准主要有两个，一个是 Adaptee 接口的个数，另一个是 Adaptee 和 ITarget 的契合程度。 如果 Adaptee 接口并不多，那两种实现方式都可以。 如果 Adaptee 接口很多，而且 Adaptee 和 ITarget 接口定义大部分都相同，那我们推荐使用类适配器，因为 Adaptor 复用父类 Adaptee 的接口，比起对象适配器的实现方式，Adaptor 的代码量要少一些。 如果 Adaptee 接口很多，而且 Adaptee 和 ITarget 接口定义大部分都不相同，那我们推荐使用对象适配器，因为组合结构相对于继承更加灵活。 一般来说，适配器模式可以看作一种“补偿模式”，用来补救设计上的缺陷。应用这种模式算是“无奈之举”，如果在设计初期，我们就能协调规避接口不兼容的问题，那这种模式就没有应用的机会了。 那在实际的开发中，什么情况下才会出现接口不兼容呢？有下面这样 5 种场景： 封装有缺陷的接口设计 统一多个类的接口设计 替换依赖的外部系统 兼容老版本接口 适配不同格式的数据 代理、桥接、装饰器、适配器 4 种设计模式的区别代理、桥接、装饰器、适配器，这 4 种模式是比较常用的结构型设计模式。它们的代码结构非常相似。笼统来说，它们都可以称为 Wrapper 模式，也就是通过 Wrapper 类二次封装原始类。 尽管代码结构相似，但这 4 种设计模式的用意完全不同，也就是说要解决的问题、应用场景不同，这也是它们的主要区别。这里我就简单说一下它们之间的区别。 代理模式：代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。 桥接模式：桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。 装饰器模式：装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。 适配器模式：适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口。 门面模式（外观模式）门面模式，也叫外观模式，英文全称是 Facade Design Pattern。门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用。 假设有一个系统 A，提供了 a、b、c、d 四个接口。系统 B 完成某个业务功能，需要调用 A 系统的 a、b、d 接口。利用门面模式，我们提供一个包裹 a、b、d 接口调用的门面接口 x，给系统 B 直接使用。 门面模式的应用场景： 解决易用性问题（门面模式可以用来封装系统的底层实现，隐藏系统的复杂性，提供一组更加简单易用、更高层的接口） 解决性能问题（我们通过将多个接口调用替换为一个门面接口调用，减少网络通信成本，提高 App 客户端的响应速度） 解决分布式事务问题 我们知道，类、模块、系统之间的“通信”，一般都是通过接口调用来完成的。接口设计的好坏，直接影响到类、模块、系统是否好用。所以，我们要多花点心思在接口设计上。我经常说，完成接口设计，就相当于完成了一半的开发任务。只要接口设计得好，那代码就差不到哪里去。 接口粒度设计得太大，太小都不好。太大会导致接口不可复用，太小会导致接口不易用。在实际的开发中，接口的可复用性和易用性需要“微妙”的权衡。针对这个问题，我的一个基本的处理原则是，尽量保持接口的可复用性，但针对特殊情况，允许提供冗余的门面接口，来提供更易用的接口。 适配器模式和门面模式对比共同点：将不好用的接口适配成好用的接口。 区别：（a）适配器主要是为了解决接口不兼容的问题，而门面模式主要用于设计接口的易用性问题。（b）适配器在代码结构上主要是继承加组合，门面模式在代码结构上主要是封装。（c）适配器可以看作是事后行为，是一种“补偿模式”，主要是用来完善设计上的不足，而门面模式是在设计接口时就需要考虑的，是一种事前行为。 组合模式组合模式跟面向对象设计中的“组合关系（通过组合来组装两个类）”，完全是两码事。这里讲的“组合模式”，主要是用来处理树形结构数据。这里的“数据”，可以简单理解为一组对象集合，正因为其应用场景的特殊性，数据必须能表示成树形结构，这也导致了这种模式在实际的项目开发中并不那么常用。但是，一旦数据满足树形结构，应用这种模式就能发挥很大的作用，能让代码变得非常简洁。 组合模式：将一组对象组织（Compose）成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端（在很多设计模式书籍中，“客户端”代指代码的使用者。）可以统一单个对象和组合对象的处理逻辑。 组合模式的设计思路，与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。 组合模式，将一组对象组织成树形结构，将单个对象和组合对象都看做树中的节点，以统一处理逻辑，并且它利用树形结构的特点，递归地处理每个子树，依次简化代码实现。使用组合模式的前提在于，你的业务场景必须能够表示成树形结构。所以，组合模式的应用场景也比较局限，它并不是一种很常用的设计模式。 享元模式享元模式的原理所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。 具体来讲，当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，我们就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用。这样可以减少内存中对象的数量，起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段）提取出来，设计成享元，让这些大量相似对象引用这些享元。 定义中的“不可变对象”指的是，一旦通过构造函数初始化完成之后，它的状态（对象的成员变量或者属性）就不会再被修改了。所以，不可变对象不能暴露任何 set() 等修改内部状态的方法。之所以要求享元是不可变对象，那是因为它会被多处代码共享使用，避免一处代码对享元进行了修改，影响到其他使用它的代码。 享元模式的实现享元模式的代码实现非常简单，主要是通过工厂模式，在工厂类中，通过一个 Map 或者 List 来缓存已经创建好的享元对象，以达到复用的目的。 享元模式 vs 单例、缓存、对象池区别两种设计模式，不能光看代码实现，而是要看设计意图，也就是要解决的问题。这里的区别也不例外。我们可以用简单几句话来概括一下它们之间的区别。 应用单例模式是为了保证对象全局唯一。应用享元模式是为了实现对象复用，节省内存。缓存是为了提高访问效率，而非复用。池化技术中的“复用”理解为“重复使用”，主要是为了节省时间。 享元模式在 Java Integer 中的应用如下代码： 123456Integer i1 = 56;Integer i2 = 56;Integer i3 = 129;Integer i4 = 129;System.out.println(i1 == i2); // trueSystem.out.println(i3 == i4); // false 首先要理解自动装箱和自动拆箱的概念： 自动装箱，就是自动将基本数据类型转换为包装器类型。所谓的自动拆箱，也就是自动将包装器类型转化为基本数据类型。具体代码如下： 12Integer i = 56; //自动装箱 - 底层执行了：Integer i = Integer.valueOf(59);int j = i; //自动拆箱 - 底层执行了：int j = i.intValue(); 当我们通过自动装箱，也就是调用 valueOf() 来创建 Integer 对象的时候，如果要创建的 Integer 对象的值在 -128 到 127 之间，会从 IntegerCache 类中直接返回，否则才调用 new 方法创建。这里的 IntegerCache 相当于享元对象的工厂类，只不过名字不叫 xxxFactory 而已。我们来看它的具体代码实现。这个类是 Integer 的内部类，你也可以自行查看 JDK 源码。 在 Java Integer 的实现中，-128 到 127 之间的整型对象会被事先创建好，缓存在 IntegerCache 类中。当我们使用自动装箱或者 valueOf() 来创建这个数值区间的整型对象时，会复用 IntegerCache 类事先创建好的对象。这里的 IntegerCache 类就是享元工厂类，事先创建好的整型对象就是享元对象。 实际上，除了 Integer 类型之外，其他包装器类型，比如 Long、Short、Byte 等，也都利用了享元模式来缓存 -128 到 127 之间的数据。 再看如下代码： 12345String s1 = \"小争哥\";String s2 = \"小争哥\";String s3 = new String(\"小争哥\");System.out.println(s1 == s2); // trueSystem.out.println(s1 == s3); // false 在 Java String 类的实现中，JVM 开辟一块存储区专门存储字符串常量，这块存储区叫作字符串常量池，类似于 Integer 中的 IntegerCache。不过，跟 IntegerCache 不同的是，它并非事先创建好需要共享的对象，而是在程序的运行期间，根据需要来创建和缓存字符串常量。 实际上，享元模式对 JVM 的垃圾回收并不友好。因为享元工厂类一直保存了对享元对象的引用，这就导致享元对象在没有任何代码使用的情况下，也并不会被 JVM 垃圾回收机制自动回收掉。因此，在某些情况下，如果对象的生命周期很短，也不会被密集使用，利用享元模式反倒可能会浪费更多的内存。所以，除非经过线上验证，利用享元模式真的可以大大节省内存，否则，就不要过度使用这个模式，为了一点点内存的节省而引入一个复杂的设计模式，得不偿失。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://jjw-story.github.io/categories/设计模式/"}],"tags":[{"name":"Design","slug":"Design","permalink":"https://jjw-story.github.io/tags/Design/"},{"name":"结构型设计模式","slug":"结构型设计模式","permalink":"https://jjw-story.github.io/tags/结构型设计模式/"}],"author":"JJW"},{"title":"设计模式","slug":"Design-创建型","date":"2021-02-05T09:48:09.151Z","updated":"2021-02-05T09:49:20.504Z","comments":true,"path":"2021/02/05/Design-创建型/","link":"","permalink":"https://jjw-story.github.io/2021/02/05/Design-创建型/","excerpt":"","text":"创建型设计模式创建型模式主要解决对象的创建问题，封装复杂的创建过程，解耦对象的创建代码和使用代码。 单例模式单例的定义单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者叫实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 单例的用处从业务概念上，有些数据在系统中只应该保存一份，就比较适合设计为单例类。比如，系统的配置信息类。除此之外，我们还可以使用单例解决资源访问冲突的问题。 单例的实现饿汉式饿汉式的实现方式，在类加载的期间，就已经将 instance 静态实例初始化好了，所以，instance 实例的创建是线程安全的。不过，这样的实现方式不支持延迟加载实例。 1234567891011public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() &#123;&#125; public static IdGenerator getInstance() &#123; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125; 懒汉式懒汉式相对于饿汉式的优势是支持延迟加载。这种实现方式会导致频繁加锁、释放锁，以及并发度低等问题，频繁的调用会产生性能瓶颈。 1234567891011121314public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() &#123;&#125; public static synchronized IdGenerator getInstance() &#123; if (instance == null) &#123; instance = new IdGenerator(); &#125; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125; 双重检测双重检测实现方式既支持延迟加载、又支持高并发的单例实现方式。只要 instance 被创建之后，再调用 getInstance() 函数都不会进入到加锁逻辑中。所以，这种实现方式解决了懒汉式并发度低的问题。 123456789101112131415161718public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() &#123;&#125; public static IdGenerator getInstance() &#123; if (instance == null) &#123; synchronized(IdGenerator.class) &#123; // 此处为类级别的锁 if (instance == null) &#123; instance = new IdGenerator(); &#125; &#125; &#125; return instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125; 静态内部类利用 Java 的静态内部类来实现单例。这种实现方式，既支持延迟加载，也支持高并发，实现起来也比双重检测简单。 12345678910111213141516public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private IdGenerator() &#123;&#125; private static class SingletonHolder&#123; private static final IdGenerator instance = new IdGenerator(); &#125; public static IdGenerator getInstance() &#123; return SingletonHolder.instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125; SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。 枚举最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。 12345678public enum IdGenerator &#123; INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() &#123; return id.incrementAndGet(); &#125;&#125; 单例存在哪些问题？ 单例对 OOP 特性的支持不友好 单例会隐藏类之间的依赖关系 单例对代码的扩展性不友好 单例对代码的可测试性不友好 单例不支持有参数的构造函数 单例有什么替代解决方案？为了保证全局唯一，除了使用单例，我们还可以用静态方法来实现。不过，静态方法这种实现思路，并不能解决我们之前提到的问题。如果要完全解决这些问题，我们可能要从根上，寻找其他方式来实现全局唯一类了。比如，通过工厂模式、IOC 容器（比如 Spring IOC 容器）来保证，由程序员自己来保证（自己在编写代码的时候自己保证不要创建两个类对象）。 12345678910// 静态方法实现方式public class IdGenerator &#123; private static AtomicLong id = new AtomicLong(0); public static long getId() &#123; return id.incrementAndGet(); &#125;&#125;// 使用举例long id = IdGenerator.getId(); 有人把单例当作反模式，主张杜绝在项目中使用。我个人觉得这有点极端。模式没有对错，关键看你怎么用。如果单例类并没有后续扩展的需求，并且不依赖外部系统，那设计成单例类就没有太大问题。对于一些全局的类，我们在其他地方 new 的话，还要在类之间传来传去，不如直接做成单例类，使用起来简洁方便。 如何理解单例模式的唯一性？单例类中对象的唯一性的作用范围是“进程唯一”的。“进程唯一”指的是进程内唯一，进程间不唯一；“线程唯一”指的是线程内唯一，线程间可以不唯一。实际上，“进程唯一”就意味着线程内、线程间都唯一，这也是“进程唯一”和“线程唯一”的区别之处。“集群唯一”指的是进程内唯一、进程间也唯一。 如何实现线程唯一的单例？我们通过一个 HashMap 来存储对象，其中 key 是线程 ID，value 是对象。这样我们就可以做到，不同的线程对应不同的对象，同一个线程只能对应一个对象。实际上，Java 语言本身提供了 ThreadLocal 并发工具类，可以更加轻松地实现线程唯一单例。 如何实现集群环境下的单例？我们需要把这个单例对象序列化并存储到外部共享存储区（比如文件）。进程在使用这个单例对象的时候，需要先从外部共享存储区中将它读取到内存，并反序列化成对象，然后再使用，使用完成之后还需要再存储回外部共享存储区。为了保证任何时刻在进程间都只有一份对象存在，一个进程在获取到对象之后，需要对对象加锁，避免其他进程再将其获取。在进程使用完这个对象之后，需要显式地将对象从内存中删除，并且释放对对象的加锁。 如何实现一个多例模式？“单例”指的是一个类只能创建一个对象。对应地，“多例”指的就是一个类可以创建多个对象，但是个数是有限制的，比如只能创建 3 个对象。多例的实现也比较简单，通过一个 Map 来存储对象类型和对象之间的对应关系，来控制对象的个数。 工厂模式一般情况下，工厂模式分为三种更加细分的类型：简单工厂、工厂方法和抽象工厂。三种工厂模式中，简单工厂和工厂方法比较常用，抽象工厂的应用场景比较特殊，所以很少用到。 当创建逻辑比较复杂，是一个“大工程”的时候，我们就考虑使用工厂模式，封装对象的创建过程，将对象的创建和使用相分离。何为创建逻辑比较复杂呢？我总结了下面两种情况。 第一种情况：类似规则配置解析的例子，代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，我们就考虑使用工厂模式，将这一大坨 if-else 创建对象的代码抽离出来，放到工厂类中。 还有一种情况，尽管我们不需要根据不同的类型创建不同的对象，但是，单个对象本身的创建过程比较复杂，比如前面提到的要组合其他类对象，做各种初始化操作。在这种情况下，我们也可以考虑使用工厂模式，将对象的创建过程封装到工厂类中。 对于第一种情况，当每个对象的创建逻辑都比较简单的时候，我推荐使用简单工厂模式，将多个对象的创建逻辑放到一个工厂类中。当每个对象的创建逻辑都比较复杂的时候，为了避免设计一个过于庞大的简单工厂类，我推荐使用工厂方法模式，将创建逻辑拆分得更细，每个对象的创建逻辑独立到各自的工厂类中。同理，对于第二种情况，因为单个对象本身的创建逻辑就比较复杂，所以，我建议使用工厂方法模式。 工厂方法就是将创建对象的工厂也创建成为一个工厂。 抽象工厂就是我们可以让一个工厂负责创建多个不同类型的对象（IRuleConfigParser、ISystemConfigParser 等），而不是只创建一种 parser 对象。这样就可以有效地减少工厂类的个数。 除了刚刚提到的这几种情况之外，如果创建对象的逻辑并不复杂，那我们就直接通过 new 来创建对象就可以了，不需要使用工厂模式。 现在，我们上升一个思维层面来看工厂模式，它的作用无外乎下面这四个。这也是判断要不要使用工厂模式的最本质的参考标准。 封装变化：创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明。 代码复用：创建代码抽离到独立的工厂类之后可以复用。 隔离复杂性：封装复杂的创建逻辑，调用者无需了解如何创建对象。 控制复杂度：将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁。 简单工厂示例123456789101112131415161718192021222324252627282930313233343536public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = RuleConfigParserFactory.createParser(ruleConfigFileExtension); if (parser == null) &#123; throw new InvalidRuleConfigException( \"Rule config file format is not supported: \" + ruleConfigFilePath); &#125; String configText = \"\"; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; &#125; private String getFileExtension(String filePath) &#123; //...解析文件名获取扩展名，比如rule.json，返回json return \"json\"; &#125;&#125;public class RuleConfigParserFactory &#123; public static IRuleConfigParser createParser(String configFormat) &#123; IRuleConfigParser parser = null; if (\"json\".equalsIgnoreCase(configFormat)) &#123; parser = new JsonRuleConfigParser(); &#125; else if (\"xml\".equalsIgnoreCase(configFormat)) &#123; parser = new XmlRuleConfigParser(); &#125; else if (\"yaml\".equalsIgnoreCase(configFormat)) &#123; parser = new YamlRuleConfigParser(); &#125; else if (\"properties\".equalsIgnoreCase(configFormat)) &#123; parser = new PropertiesRuleConfigParser(); &#125; return parser; &#125;&#125; 工厂方法示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public interface IRuleConfigParserFactory &#123; IRuleConfigParser createParser();&#125;public class JsonRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new JsonRuleConfigParser(); &#125;&#125;public class XmlRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new XmlRuleConfigParser(); &#125;&#125;public class YamlRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new YamlRuleConfigParser(); &#125;&#125;public class PropertiesRuleConfigParserFactory implements IRuleConfigParserFactory &#123; @Override public IRuleConfigParser createParser() &#123; return new PropertiesRuleConfigParser(); &#125;&#125;public class RuleConfigSource &#123; public RuleConfig load(String ruleConfigFilePath) &#123; String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParserFactory parserFactory = null; if (\"json\".equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new JsonRuleConfigParserFactory(); &#125; else if (\"xml\".equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new XmlRuleConfigParserFactory(); &#125; else if (\"yaml\".equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new YamlRuleConfigParserFactory(); &#125; else if (\"properties\".equalsIgnoreCase(ruleConfigFileExtension)) &#123; parserFactory = new PropertiesRuleConfigParserFactory(); &#125; else &#123; throw new InvalidRuleConfigException(\"Rule config file format is not supported: \" + ruleConfigFilePath); &#125; IRuleConfigParser parser = parserFactory.createParser(); String configText = \"\"; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; &#125; private String getFileExtension(String filePath) &#123; //...解析文件名获取扩展名，比如rule.json，返回json return \"json\"; &#125;&#125; 抽象工厂示例12345678910111213141516171819202122232425262728293031public interface IConfigParserFactory &#123; IRuleConfigParser createRuleParser(); ISystemConfigParser createSystemParser(); //此处可以扩展新的parser类型，比如IBizConfigParser&#125;public class JsonConfigParserFactory implements IConfigParserFactory &#123; @Override public IRuleConfigParser createRuleParser() &#123; return new JsonRuleConfigParser(); &#125; @Override public ISystemConfigParser createSystemParser() &#123; return new JsonSystemConfigParser(); &#125;&#125;public class XmlConfigParserFactory implements IConfigParserFactory &#123; @Override public IRuleConfigParser createRuleParser() &#123; return new XmlRuleConfigParser(); &#125; @Override public ISystemConfigParser createSystemParser() &#123; return new XmlSystemConfigParser(); &#125;&#125;// 省略YamlConfigParserFactory和PropertiesConfigParserFactory代码 工厂模式和 DI 容器有何区别？DI 容器底层最基本的设计思路就是基于工厂模式的。DI 容器相当于一个大的工厂类，负责在程序启动的时候，根据配置（要创建哪些类对象，每个类对象的创建需要依赖哪些其他类对象）事先创建好对象。当应用程序需要使用某个类对象的时候，直接从容器中获取即可。正是因为它持有一堆对象，所以这个框架才被称为“容器”。 DI 容器相对于我们工厂模式的例子来说，它处理的是更大的对象创建工程。工厂模式中，一个工厂类只负责某个类对象或者某一组相关类对象（继承自同一抽象类或者接口的子类）的创建，而 DI 容器负责的是整个应用中所有类对象的创建。 除此之外，DI 容器负责的事情要比单纯的工厂模式要多。比如，它还包括配置的解析、对象生命周期的管理。 DI 容器的实现原理，其核心逻辑主要包括：配置文件解析，以及根据配置文件通过“反射”语法来创建对象。其中，创建对象的过程就应用到了我们在学的工厂模式。对象创建、组装、管理完全有 DI 容器来负责，跟具体业务代码解耦，让程序员聚焦在业务代码的开发上。 建造者模式建造者模式的原理和实现比较简单，重点是掌握应用场景，避免过度使用。 如果一个类中有很多属性，为了避免构造函数的参数列表过长，影响代码的可读性和易用性，我们可以通过构造函数配合 set() 方法来解决。但是，如果存在下面情况中的任意一种，我们就要考虑使用建造者模式了。 我们把类的必填属性放到构造函数中，强制创建对象的时候就设置。如果必填的属性有很多，把这些必填属性都放到构造函数中设置，那构造函数就又会出现参数列表很长的问题。如果我们把必填属性通过 set() 方法设置，那校验这些必填属性是否已经填写的逻辑就无处安放了。 如果类的属性之间有一定的依赖关系或者约束条件，我们继续使用构造函数配合 set() 方法的设计思路，那这些依赖关系或约束条件的校验逻辑就无处安放了。 如果我们希望创建不可变对象，也就是说，对象在创建好之后，就不能再修改内部的属性值，要实现这个功能，我们就不能在类中暴露 set() 方法。构造函数配合 set() 方法来设置属性值的方式就不适用了。 工厂模式和建造者模式的区别工厂模式是用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建一种类型的复杂对象，可以通过设置不同的可选参数，“定制化”地创建不同的对象。 建造者模式标准全面的示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class ResourcePoolConfig &#123; private String name; private int maxTotal; private int maxIdle; private int minIdle; private ResourcePoolConfig(Builder builder) &#123; this.name = builder.name; this.maxTotal = builder.maxTotal; this.maxIdle = builder.maxIdle; this.minIdle = builder.minIdle; &#125; //...省略getter方法... //我们将Builder类设计成了ResourcePoolConfig的内部类。 //我们也可以将Builder类设计成独立的非内部类ResourcePoolConfigBuilder。 public static class Builder &#123; private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig build() &#123; // 校验逻辑放到这里来做，包括必填项校验、依赖关系校验、约束条件校验等 if (StringUtils.isBlank(name)) &#123; throw new IllegalArgumentException(\"...\"); &#125; if (maxIdle &gt; maxTotal) &#123; throw new IllegalArgumentException(\"...\"); &#125; if (minIdle &gt; maxTotal || minIdle &gt; maxIdle) &#123; throw new IllegalArgumentException(\"...\"); &#125; return new ResourcePoolConfig(this); &#125; public Builder setName(String name) &#123; if (StringUtils.isBlank(name)) &#123; throw new IllegalArgumentException(\"...\"); &#125; this.name = name; return this; &#125; public Builder setMaxTotal(int maxTotal) &#123; if (maxTotal &lt;= 0) &#123; throw new IllegalArgumentException(\"...\"); &#125; this.maxTotal = maxTotal; return this; &#125; public Builder setMaxIdle(int maxIdle) &#123; if (maxIdle &lt; 0) &#123; throw new IllegalArgumentException(\"...\"); &#125; this.maxIdle = maxIdle; return this; &#125; public Builder setMinIdle(int minIdle) &#123; if (minIdle &lt; 0) &#123; throw new IllegalArgumentException(\"...\"); &#125; this.minIdle = minIdle; return this; &#125; &#125;&#125;// 这段代码会抛出IllegalArgumentException，因为minIdle&gt;maxIdleResourcePoolConfig config = new ResourcePoolConfig.Builder() .setName(\"dbconnectionpool\") .setMaxTotal(16) .setMaxIdle(10) .setMinIdle(12) .build(); 原型模式什么是原型模式？如果对象的创建成本比较大，而同一个类的不同对象之间差别不大（大部分字段都相同），在这种情况下，我们可以利用对已有对象（原型）进行复制（或者叫拷贝）的方式，来创建新对象，以达到节省创建时间的目的。这种基于原型来创建对象的方式就叫作原型设计模式，简称原型模式。 原型模式的两种实现方法原型模式有两种实现方法，深拷贝和浅拷贝。浅拷贝只会复制对象中基本数据类型数据和引用对象的内存地址，不会递归地复制引用对象，以及引用对象的引用对象……而深拷贝得到的是一份完完全全独立的对象。所以，深拷贝比起浅拷贝来说，更加耗时，更加耗内存空间。 如果要拷贝的对象是不可变对象，浅拷贝共享不可变对象是没问题的，但对于可变对象来说，浅拷贝得到的对象和原始对象会共享部分数据，就有可能出现数据被修改的风险，也就变得复杂多了。除非像我们今天实战中举的那个例子，需要从数据库中加载 10 万条数据并构建散列表索引，操作非常耗时，这种情况下比较推荐使用浅拷贝，否则，没有充分的理由，不要为了一点点的性能提升而使用浅拷贝。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://jjw-story.github.io/categories/设计模式/"}],"tags":[{"name":"Design","slug":"Design","permalink":"https://jjw-story.github.io/tags/Design/"},{"name":"创建型设计模式","slug":"创建型设计模式","permalink":"https://jjw-story.github.io/tags/创建型设计模式/"}],"author":"JJW"},{"title":"设计模式","slug":"Desgin-原则-规范-重构","date":"2021-02-05T09:32:46.522Z","updated":"2021-02-05T10:00:43.391Z","comments":true,"path":"2021/02/05/Desgin-原则-规范-重构/","link":"","permalink":"https://jjw-story.github.io/2021/02/05/Desgin-原则-规范-重构/","excerpt":"","text":"面向对象 设计原则单一职责原则（SRP） 如何理解单一职责原则（SRP）？一个类只负责完成一个职责或者功能。不要设计大而全的类，要设计粒度小、功能单一的类。单一职责原则是为了实现代码高内聚、低耦合，提高代码的复用性、可读性、可维护性。 如何判断类的职责是否足够单一？ 不同的应用场景、不同阶段的需求背景、不同的业务层面，对同一个类的职责是否单一，可能会有不同的判定结果。实际上，一些侧面的判断指标更具有指导意义和可执行性，比如，出现下面这些情况就有可能说明这类的设计不满足单一职责原则： 类中的代码行数、函数或者属性过多； 类依赖的其他类过多，或者依赖类的其他类过多； 私有方法过多； 比较难给类起一个合适的名字； 类中大量的方法都是集中操作类中的某几个属性。 类的职责是否设计得越单一越好？ 单一职责原则通过避免设计大而全的类，避免将不相关的功能耦合在一起，来提高类的内聚性。同时，类职责单一，类依赖的和被依赖的其他类也会变少，减少了代码的耦合性，以此来实现代码的高内聚、低耦合。但是，如果拆分得过细，实际上会适得其反，反倒会降低内聚性，也会影响代码的可维护性。 开闭原则（OCP） 如何理解“对扩展开放、对修改关闭”？ 添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。关于定义，我们有两点要注意。第一点是，开闭原则并不是说完全杜绝修改，而是以最小的修改代码的代价来完成新功能的开发。第二点是，同样的代码改动，在粗代码粒度下，可能被认定为“修改”；在细代码粒度下，可能又被认定为“扩展”。 如何做到“对扩展开放、修改关闭”？ 我们要时刻具备扩展意识、抽象意识、封装意识。在写代码的时候，我们要多花点时间思考一下，这段代码未来可能有哪些需求变更，如何设计代码结构，事先留好扩展点，以便在未来需求变更的时候，在不改动代码整体结构、做到最小代码改动的情况下，将新的代码灵活地插入到扩展点上。 很多设计原则、设计思想、设计模式，都是以提高代码的扩展性为最终目的的。特别是 23 种经典设计模式，大部分都是为了解决代码的扩展性问题而总结出来的，都是以开闭原则为指导原则的。最常用来提高代码扩展性的方法有：多态、依赖注入、基于接口而非实现编程，以及大部分的设计模式（比如，装饰、策略、模板、职责链、状态）。 里式替换原则（LSP） 里式替换原则是用来指导，继承关系中子类该如何设计的一个原则。理解里式替换原则，最核心的就是理解“design by contract，按照协议来设计”这几个字。父类定义了函数的“约定”（或者叫协议），那子类可以改变函数的内部实现逻辑，但不能改变函数原有的“约定”。这里的约定包括：函数声明要实现的功能；对输入、输出、异常的约定；甚至包括注释中所罗列的任何特殊说明。 理解这个原则，我们还要弄明白里式替换原则跟多态的区别。虽然从定义描述和代码实现上来看，多态和里式替换有点类似，但它们关注的角度是不一样的。多态是面向对象编程的一大特性，也是面向对象编程语言的一种语法。它是一种代码实现的思路。而里式替换是一种设计原则，用来指导继承关系中子类该如何设计，子类的设计要保证在替换父类的时候，不改变原有程序的逻辑及不破坏原有程序的正确性。 哪些代码明显违背了 LSP？ 子类违背父类声明要实现的功能（父类中提供的 sortOrdersByAmount() 订单排序函数，是按照金额从小到大来给订单排序的，而子类重写这个 sortOrdersByAmount() 订单排序函数之后，是按照创建日期来给订单排序的。那子类的设计就违背里式替换原则。） 子类违背父类对输入、输出、异常的约定 子类违背父类注释中所罗列的任何特殊说明 接口隔离原则（ISP） 如何理解“接口隔离原则”？ 理解“接口隔离原则”的重点是理解其中的“接口”二字。这里有三种不同的理解。 如果把“接口”理解为一组接口集合，可以是某个微服务的接口，也可以是某个类库的接口等。如果部分接口只被部分调用者使用，我们就需要将这部分接口隔离出来，单独给这部分调用者使用，而不强迫其他调用者也依赖这部分不会被用到的接口。 如果把“接口”理解为单个 API 接口或函数，部分调用者只需要函数中的部分功能，那我们就需要把函数拆分成粒度更细的多个函数，让调用者只依赖它需要的那个细粒度函数。 如果把“接口”理解为 OOP 中的接口，也可以理解为面向对象编程语言中的接口语法。那接口的设计要尽量单一，不要让接口的实现类和调用者，依赖不需要的接口函数。 接口隔离原则与单一职责原则的区别 单一职责原则针对的是模块、类、接口的设计。接口隔离原则相对于单一职责原则，一方面更侧重于接口的设计，另一方面它的思考角度也是不同的。接口隔离原则提供了一种判断接口的职责是否单一的标准：通过调用者如何使用接口来间接地判定。如果调用者只使用部分接口或接口的部分功能，那接口的设计就不够职责单一。 依赖反转原则（DIP） 什么是控制反转？ 实际上，控制反转是一个比较笼统的设计思想，并不是一种具体的实现方法，一般用来指导框架层面的设计。这里所说的“控制”指的是对程序执行流程的控制，而“反转”指的是在没有使用框架之前，程序员自己控制整个程序的执行。在使用框架之后，整个程序的执行流程通过框架来控制。流程的控制权从程序员“反转”给了框架。 什么是依赖注入？ 依赖注入和控制反转恰恰相反，它是一种具体的编码技巧。我们不通过 new 的方式在类内部创建依赖类的对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类来使用。 什么是依赖注入框架？ 我们通过依赖注入框架提供的扩展点，简单配置一下所有需要的类及其类与类之间依赖关系，就可以实现由框架来自动创建对象、管理对象的生命周期、依赖注入等原本需要程序员来做的事情。 依赖反转原则 依赖反转原则也叫作依赖倒置原则。这条原则跟控制反转有点类似，主要用来指导框架层面的设计。高层模块不依赖低层模块，它们共同依赖同一个抽象。抽象不要依赖具体实现细节，具体实现细节依赖抽象。 迪米特法则（LOD）（最小知识原则） 如何理解“高内聚、松耦合”？ “高内聚、松耦合”是一个非常重要的设计思想，能够有效提高代码的可读性和可维护性，缩小功能改动导致的代码改动范围。“高内聚”用来指导类本身的设计，“松耦合”用来指导类与类之间依赖关系的设计。 所谓高内聚，就是指相近的功能应该放到同一个类中，不相近的功能不要放到同一类中。相近的功能往往会被同时修改，放到同一个类中，修改会比较集中。所谓松耦合指的是，在代码中，类与类之间的依赖关系简单清晰。即使两个类有依赖关系，一个类的代码改动也不会或者很少导致依赖类的代码改动。 如何理解“迪米特法则”？ 不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口。迪米特法则是希望减少类之间的耦合，让类越独立越好。每个类都应该少了解系统的其他部分。一旦发生变化，需要了解这一变化的类就会比较少。 KISS 原则和 YAGNI 原则 如何理解“KISS 原则”？ 尽量保持简单。KISS 原则是保持代码可读和可维护的重要手段。KISS 原则中的“简单”并不是以代码行数来考量的。代码行数越少并不代表代码越简单，我们还要考虑逻辑复杂度、实现难度、代码的可读性等。而且，本身就复杂的问题，用复杂的方法解决，并不违背 KISS 原则。除此之外，同样的代码，在某个业务场景下满足 KISS 原则，换一个应用场景可能就不满足了 写出满足 KISS 原则的代码？ 不要使用同事可能不懂的技术来实现代码； 不要重复造轮子，要善于使用已经有的工具类库； 不要过度优化。 什么是YAGNI原则？ YAGNI 原则的英文全称是：You Ain’t Gonna Need It。直译就是：你不会需要它。它的意思是：不要去设计当前用不到的功能；不要去编写当前用不到的代码。实际上，这条原则的核心思想就是：不要做过度设计。 编码规范关于命名 命名的关键是能准确达意。对于不同作用域的命名，我们可以适当地选择不同的长度。作用域小的变量（比如临时变量），可以适当地选择短一些的命名方式。除此之外，命名中也可以使用一些耳熟能详的缩写。 我们可以借助类的信息来简化属性、函数的命名，利用函数的信息来简化函数参数的命名。 命名要可读、可搜索。不要使用生僻的、不好读的英文单词来命名。除此之外，命名要符合项目的统一规范，不要用些反直觉的命名。 接口有两种命名方式：一种是在接口中带前缀“I”；另一种是在接口的实现类中带后缀“Impl”。对于抽象类的命名，也有两种方式，一种是带上前缀“Abstract”，一种是不带前缀。这两种命名方式都可以，关键是要在项目中统一。 关于代码风格关于注释 注释的目的就是让代码更容易看懂。只要符合这个要求的内容，你就可以将它写到注释里。总结一下，注释的内容主要包含这样三个方面：做什么、为什么、怎么做。对于一些复杂的类和接口，我们可能还需要写明“如何用”。 注释本身有一定的维护成本，所以并非越多越好。类和函数一定要写注释，而且要写得尽可能全面、详细，而函数内部的注释要相对少一些，一般都是靠好的命名、提炼函数、解释性变量、总结性注释来提高代码可读性。 函数、类多大才合适？ 函数的代码行数不要超过一屏幕的大小，比如 50 行。类的大小限制比较难确定。 一行代码多长最合适？ 最好不要超过 IDE 显示的宽度。当然，限制也不能太小，太小会导致很多稍微长点的语句被折成两行，也会影响到代码的整洁，不利于阅读。 善用空行分割单元块 对于比较长的函数，为了让逻辑更加清晰，可以使用空行来分割各个代码块。在类内部，成员变量与函数之间、静态成员变量与普通成员变量之间、函数之间，甚至成员变量之间，都可以通过添加空行的方式，让不同模块的代码之间的界限更加明确。 类中成员的排列顺序 在 Google Java 编程规范中，依赖类按照字母序从小到大排列。类中先写成员变量后写函数。成员变量之间或函数之间，先写静态成员变量或函数，后写普通变量或函数，并且按照作用域大小依次排列。 关于编码技巧 将复杂的逻辑提炼拆分成函数和类。 通过拆分成多个函数或将参数封装为对象的方式，来处理参数过多的情况。 函数中不要使用参数来做代码执行逻辑的控制。（不要在函数中使用 “布尔类型的标识参数” 或者 “根据参数是否为 null“ 来控制内部逻辑，这明显违背了单一职责原则和接口隔离原则。建议将其拆成两个函数，可读性上也要更好。） 函数设计要职责单一。 移除过深的嵌套层次，方法包括：去掉多余的 if 或 else 语句，使用 continue、break、return 关键字提前退出嵌套，调整执行顺序来减少嵌套，将部分嵌套逻辑抽象成函数。 用字面常量取代魔法数。 用解释性变量来解释复杂表达式，以此提高代码可读性。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://jjw-story.github.io/categories/设计模式/"}],"tags":[{"name":"Design","slug":"Design","permalink":"https://jjw-story.github.io/tags/Design/"},{"name":"原则-规范-重构","slug":"原则-规范-重构","permalink":"https://jjw-story.github.io/tags/原则-规范-重构/"}],"author":"JJW"},{"title":"Flink-运行时架构","slug":"Flink运行时架构","date":"2021-02-05T08:54:54.322Z","updated":"2021-02-05T09:09:19.748Z","comments":true,"path":"2021/02/05/Flink运行时架构/","link":"","permalink":"https://jjw-story.github.io/2021/02/05/Flink运行时架构/","excerpt":"","text":"Flink基本组件栈Flink基本组件栈如下图： Flink的架构体系同样也遵行分层架构设计的理念，基本上分为三层，API&amp;Libraries层、Runtine核心层以及物理部署层。 API&amp;Libraries层：提供了支撑流计算和批计算的接口，同时在此基础之上抽象出不同的应用类型的组件库。 Runtime 核心层：负责对上层不同接口提供基础服务，支持分布式Stream作业的执行、JobGraph到ExecutionGraph 的映射转换、任务调度等，将DataStream和DataSet转成统一的可执行的Task Operator. 物理部署层：Flink 支持多种部署模式，本机，集群（Standalone/YARN）、云（GCE/EC2）、Kubenetes。 Flink运行时组件Flink 运行时架构主要包括四个不同的组件，它们会在运行流处理应用程序时协同工作：作业管理器（JobManager）、资源管理器（ResourceManager）、任务管理器（TaskManager），以及分发器（Dispatcher）。因为 Flink 是用 Java 和 Scala 实现的，所以所有组件都会运行在Java 虚拟机上。具体如如下图： 作业管理器（JobManager）控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序，这个应用程序会包括：作业图（JobGraph）、逻辑数据流图（logical dataflow graph）和打包了所有的类、库和其它资源的JAR包。 JobManager 会把JobGraph转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph），包含了所有可以并发执行的任务。 JobManager 会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中，JobManager会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调。 任务管理器（TaskManager） Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。 启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（tasks）来执行了。 在执行过程中，一个TaskManager可以跟其它运行同一应用程序的TaskManager交换数据。 资源管理器（ResourceManager） 主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger 插槽是Flink中定义的处理资源单元。 Flink为不同的环境和资源管理工具提供了不同资源管理器，比如YARN、Mesos、K8s，以及standalone部署。 当JobManager申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足JobManager的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器。 分发器（Dispatcher） 可以跨作业运行，它为应用提交提供了REST接口。 当一个应用被提交执行时，分发器就会启动并将应用移交给一个JobManager。 Dispatcher也会启动一个Web UI，用来方便地展示和监控作业执行的信息。 Dispatcher在架构中可能并不是必需的，这取决于应用提交运行的方式。 Flink任务提交流程标准任务提交流程标准的任务提交流程如下图所示： 上图是从一个较为高层级的视角，来看应用中各组件的交互协作。如果部署的集群环境不同（例如 YARN，Mesos，Kubernetes，standalone 等），其中一些步骤可以被省略，或是有些组件会运行在同一个 JVM 进程中。 注意：Standalone 模式提交时，4、5、6步骤是在Flink集群启动时就完成好了。其他如yarn模式启动，4、5、6步骤是按上述流程进行的。 YARN模式任务提交流程![avatar]/images/blogs/flink/task-submit-yarn.png) Flink任务提交后，Client向HDFS上传Flink的Jar包和配置， 之后向Yarn ResourceManager提交任务， ResourceManager分配Container资源并通知对应的NodeManager启动ApplicationMaster， ApplicationMaster启动后加载Flink的Jar包和配置构建环境，然后启动JobManager， 之后ApplicationMaster向ResourceManager申请资源启动TaskManager， ResourceManager分配Container资源后，由ApplicationMaster通知资源所在节点的NodeManager启动TaskManager， NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager， TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务。 Flink任务调度原理 客户端不是运行时和程序执行的一部分，但它是任务执行的起点。JobClient负责接受用户的程序代码，用于准备并发送dataflow(JobGraph)给 Master(JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。 当Flink集群启动后，首先会启动一个JobManger和一个或多个的TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Program Code 我们编写的Flink应用程序代码。 Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming 的任务），也可以不结束并等待结果返回。 JobManager 主要负责调度Job并协调Task做checkpoint，职责上很像Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager 从JobManager处接收需要部署的Task。TaskManager是在JVM中一个或多个线程中执行任务的工作节点。任务执行的并行性由每个TaskManager上可用的任务槽决定。每个任务代表分配给任务槽的一组资源。例如：如果TaskManager有四个插槽，那么它将为每个插槽分配25%的内存。可以在任务槽中运行一个或多个线程。同一插槽中的线程共享相同的JVM。同一JVM中的任务共享TCP连接和心跳信息。TaskManager的一个Slot代表一个可用线程，该线程具有固定的内存，注意Slot只对内存隔离，没有对CPU隔离。 默认情况下，Flink允许子任务共享Slot，即使它们是不同的task的subtask，只要他们来自相同的job。这种共享可以有更好的资源利用率。 并行度（Parallelism）Flink 程序的执行具有并行、分布式的特性。在执行过程中，一个流（stream）包含一个或多个分区（stream partition），而每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中彼此互不依赖地执行。 一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。 TaskManger 与 SlotsFlink 中每一个 worker(TaskManager)都是一个 JVM 进程，它可能会在独立的线程上执行一个或多个 subtask。为了控制一个 worker 能接收多少个 task，worker 通过 task slot 来进行控制（一个 worker 至少有一个 task slot）。 每个 task slot 表示 TaskManager 拥有资源的一个固定大小的子集。假如一个TaskManager 有三个 slot，那么它会将其管理的内存分成三份给各个 slot。资源 slot化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到 CPU 的隔离，slot 目前仅仅用来隔离 task 的受管理的内存。 通过调整 task slot 的数量，允许用户定义 subtask 之间如何互相隔离。如果一个TaskManager 一个 slot，那将意味着每个 task group 运行在独立的 JVM 中（该 JVM可能是通过一个特定的容器启动的），而一个 TaskManager 多个 slot 意味着更多的subtask 可以共享同一个 JVM。而在同一个 JVM 进程中的 task 将共享 TCP 连接（基于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个task 的负载。 默认情况下，Flink 允许子任务共享 slot，即使它们是不同任务的子任务（前提是它们来自同一个 job）。 这样的结果是，一个 slot 可以保存作业的整个管道。如下图所示： Task Slot 是静态的概念，是指 TaskManager 具有的并发执行能力，可以通过参数 taskmanager.numberOfTaskSlots 进行配置；而并行度 parallelism 是动态概念，即 TaskManager 运行程序时实际使用的并发能力，可以通过参数 parallelism.default进行配置。 也就是说，假设一共有 3 个 TaskManager，每一个 TaskManager 中的分配 3 个TaskSlot，也就是每个 TaskManager 可以接收 3 个 task，一共 9 个 TaskSlot，如果我们设置 parallelism.default=1，即运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1个，有 8 个空闲，因此，设置合适的并行度才能提高效率。 并行子任务分配利用如下图: 子任务分配及配置级别优先度示例： 程序与数据流（DataFlow） 所有的 Flink 程序都是由三部分组成的： Source 、Transformation 和 Sink。Source 负责读取数据源，Transformation 利用各种算子进行处理加工，Sink 负责输出。 在运行时，Flink 上运行的程序会被映射成“逻辑数据流”（dataflows），它包含了这三部分。每一个 dataflow 以一个或多个 sources 开始以一个或多个 sinks 结束。dataflow 类似于任意的有向无环图（DAG）。在大部分情况下，程序中的转换运算（transformations）跟 dataflow 中的算子（operator）是一一对应的关系，但有时候，一个 transformation 可能对应多个 operator。 执行图（ExecutionGraph）由 Flink 程序直接映射成的数据流图是 StreamGraph，也被称为逻辑流图，因为它们表示的是计算逻辑的高级视图。为了执行一个流处理程序，Flink 需要将逻辑流图转换为物理数据流图（也叫执行图），详细说明程序的执行方式。 Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt;物理执行图。 StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。 JobGraph：StreamGraph 经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 ExecutionGraph ： JobManager 根据 JobGraph 生成 ExecutionGraph。ExecutionGraph 是 JobGraph 的并行化版本，是调度层最核心的数据结构。 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 数据传输形式一个程序中，不同的算子可能具有不同的并行度。 算子之间传输数据的形式可以是 one-to-one (forwarding) 的模式也可以是redistributing 的模式，具体是哪一种形式，取决于算子的种类。 One-to-one：stream维护着分区以及元素的顺序（比如source和map之间）。这意味着map 算子的子任务看到的元素的个数以及顺序跟 source 算子的子任务生产的元素的个数、顺序相同。map、fliter、flatMap等算子都是one-to-one的对应关系。类似于 spark 中的窄依赖 Redistributing：stream的分区会发生改变。每一个算子的子任务依据所选择的transformation发送数据到不同的目标任务。例如，keyBy 基于 hashCode 重分区、而 broadcast 和 rebalance 会随机重新分区，这些算子都会引起redistribute过程，而 redistribute 过程就类似于 Spark 中的 shuffle 过程。类似于 spark 中的宽依赖 任务链（Operator Chains）相同并行度的 one to one 操作，Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的一部分。将算子链接成 task 是非常有效的优化：它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。链接的行为可以在编程 API 中进行指定。","categories":[{"name":"Flink","slug":"Flink","permalink":"https://jjw-story.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://jjw-story.github.io/tags/Flink/"},{"name":"运行时架构","slug":"运行时架构","permalink":"https://jjw-story.github.io/tags/运行时架构/"}],"author":"JJW"},{"title":"Flink-Flink Storm SparkStream对比","slug":"Flink Storm SparkStream对比","date":"2021-02-05T08:54:54.286Z","updated":"2021-02-05T09:25:43.580Z","comments":true,"path":"2021/02/05/Flink Storm SparkStream对比/","link":"","permalink":"https://jjw-story.github.io/2021/02/05/Flink Storm SparkStream对比/","excerpt":"","text":"Flink、SparkStream和Storm对比概述Flink、Spark Streaming、Storm都可以进行实时计算，但各有特点。 在大数据处理领域，批处理任务和流处理任务一般被认为是两种不同的任务，一个大数据框架一般会被设计为只能处理其中一种任务。例如Storm只支持流处理任务，而MapReduce、Spark只支持批处理任务。Spark Streaming是采用了一种micro-batch的架构，即把输入的数据流且分为细粒度的batch，并为每一个batch数据提交一个批处理的Spark任务，所以Spark Streaming本质上还是基于Spark批处理系统对流式数据进行处理，和Storm等完全流式的数据处理方式完全不同。 Flink通过灵活的执行引擎，能够同时支持批处理任务和流处理任务。在执行引擎这一层，流处理系统与批处理系统最大的不同在于节点间的数据传输方式。对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理。而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点。这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求。 Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型。 Flink以固定的缓存块为单位进行网络数据传输，用户可以通过设置缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟。如果缓存块的超时值为无限大，则Flink的数据传输方式类似上文所提到批处理系统的标准模型，此时系统可以获得最高的吞吐量。同时缓存块的超时值也可以设置为0到无限大之间的任意值。缓存块的超时阈值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，反之亦然。通过调整缓存块的超时阈值，用户可根据需求灵活的权衡系统延迟和吞吐量。 工作原理介绍StormStorm是一个免费并开源的分布式实时计算系统。利用Storm可以很容易做到可靠地处理无限的数据流，像Hadoop批量处理大数据一样，Storm可以实时处理数据。 Storm也是主从架构，主节点Nimbus负责资源分配和任务调度，工作节点Supervisor负责真正执行任务。一个Supervisor可以有多个工作进程Worker，每个工作进程中可以运行多个Task，每个Task都是一个线程，执行实际的数据处理。Task中运行的是Spout或者Bolt，前者表示的是数据源，后者表示的是消息处理单元，运行用户的程序逻辑。还有一个概念叫Topology，表示应用程序计算逻辑的拓扑结构。 Storm中每条消息的数据结构称为Tuple，每一个Tuple都是一个值的集合，值是以name, value的形式存在tuple中。 Apache Storm的主要亮点是，它是一个容错，快速，没有“单点故障”（SPOF）分布式应用程序，但是Storm不支持有状态计算。 SparkStreamingSparkStreaming是对于Spark核心API的拓展，从而支持对于实时数据流的可拓展，高吞吐量和容错性流处理。 Spark Streaming内部的基本工作原理如下：接收实时输入数据流，然后将数据拆分成多个batch，比如每收集1秒的数据封装为一个batch，然后将每个batch交给Spark的计算引擎进行处理，最后会生产出一个结果数据流，其中的数据，也是由一个一个的batch所组成的。 Spark Streaming提供了一种高级的抽象，叫做DStream，英文全称为Discretized Stream，中文翻译为“离散流”，它代表了一个持续不断的数据流。DStream的内部，其实一系列持续不断产生的RDD。RDD是Spark Core的核心抽象，即，不可变的，分布式的数据集。DStream中的每个RDD都包含了一个时间段内的数据，即DStream就是多个RDD组成的一个序列，所以SparkStreaming是以微批来模拟流，并不是一个纯实时的场景。 SparkStreaming支持有状态计算。Spark Streaming 通过 updateStateByKey 操作保存任意的状态。为使用这个功能，需要做下面两步： 定义状态，状态可以是一个任意的数据类型。 定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。 updateStateByKey 操作要求必须开启Checkpoint机制。且不能是本地文件系统，必须是HDFS。 SparkStreaming的反压：SparkStreaming 从v1.5开始引入反压机制（back-pressure）,由于它是微批处理，所以通过估计当前系统处理数据的速率，动态调节数据接收速率来适配集群数据处理能力(和Flink不同，Flink不需要反压)。 FlinkFlink 被设计成能用上千个点在大规模集群上运行。除了支持独立集群部署外，Flink 还支持 YARN 和Mesos 方式部署。当Flink系统启动时，首先启动JobManager和一至多个TaskManager。JobManager负责协调Flink系统，TaskManager则是执行并行程序的worker。当系统以本地形式启动时，一个JobManager和一个TaskManager会启动在同一个JVM中。 当一个程序被提交后，系统会创建一个Client来进行预处理，将程序转变成一个并行数据流的（parallel data flow）形式，交给JobManager和TaskManager执行。 Flink 保存点提供了一个状态化的版本机制，使得能以无丢失状态和最短停机时间的方式更新应用或者回退历史数据。 容错性对比消息传输正确性保证语义At Most Once：在框架中每条消息传输零次或者一次，也就是说，消息可能会丢失。 At Least Once：在框架中每条消息会进行多次传输尝试，至少需要有一次成功。也就是说，消息不会丢失，但可能会重复。 Exactly Once：在框架中每条消息有且只有一次，也就是说，消息既不会丢失也不会重复。这种消息传递类型是目前各大流式框架需要提供的功能。 容错性对比Storm的容错通过ack机制实现，每个bolt或spout处理完成一条data后会发送一条ack消息给acker bolt。当该条data被所有节点都处理过后，它会收到来自所有节点ack， 这样一条data处理就是成功的。storm可以保证数据不丢失，但是只能达到at least once语义。此外，因为需要每条data都做ack，所以容错的开销很大。 Spark依赖checkpoint机制来进行容错，只要batch执行到doCheckpoint操作前挂了，那么该batch就会被完整的重新计算。spark可以保证计算过程的exactly once。 Flink使用Chandy-Chandy-Lamport Algorithm 来做Asynchronous Distributed Snapshots（异步分布式快照），其本质也是checkpoint。如下图，flink定时往流里插入一个barrier（隔栏），这些barriers把数据分割成若干个小的部分，当barrier流到某个operator时，operator立即会对barrier对应的一小部分数据做checkpoint并且把barrier传给下游（checkpoint操作是异步的，并不会打断数据的处理），直到所有的sink operator做完自己checkpoint后，一个完整的checkpoint才算完成。当出现failure时，flink会从最新完整的checkpoint点开始恢复。 flink的checkpoint机制非常轻量，barrier不会打断streaming的流动，而且做checkpoint操作也是异步的。其次，相比storm需要ack每条data，flink做的是small batch的checkpoint，容错的代价相对要低很多。最重要的是flink的checkpoint机制能保证exactly once。 吞吐量和延迟吞吐量Storm的容错机制需要对每条data进行ack，因此容错开销对吞吐量影响巨大，吞吐量下降甚至可以达到70%。所以storm在打开ack容错机制后，吞吐量下降非常明显。 Spark是mirco-batch级别的计算，各种优化做的也很好，它的吞吐量是最大的。但是需要提一下，有状态计算（如updateStateByKey算子）需要通过额外的rdd来维护状态，导致开销较大，对吞吐量影响也较大。 Flink的容错机制较为轻量，对吞吐量影响较小，而且拥有图和调度上的一些优化机制，使得flink可以达到很高的吞吐量（Flink在开启checkpoint和关闭的情况下吞吐量变化不大，说明flink的容错机制确实代价不高）。 延迟Storm是native streaming实现，可以轻松的达到几十毫秒级别的延迟，在几款框架中它的延迟是最低的。 Spark基于micro-batch实现，提高了吞吐量，但是付出了延迟的代价。一般spark的延迟是秒级别的。 Flink也是native streaming实现，也可以达到百毫秒级别的延迟。 总结如何选择实时框架 需要关注流数据是否需要进行状态管理 At-least-once或者Exectly-once消息投递模式是否有特殊要求 对于小型独立的项目，并且需要低延迟的场景，建议使用storm 如果你的项目已经使用了spark，并且秒级别的实时处理可以满足需求的话，建议使用spark streaming 要求消息投递语义为Exactly Once的场景；数据量较大，要求高吞吐低延迟的场景；需要进行状态管理或窗口统计的场景，建议使用Flink","categories":[{"name":"Flink","slug":"Flink","permalink":"https://jjw-story.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://jjw-story.github.io/tags/Flink/"},{"name":"Flink Storm SparkStream对比","slug":"Flink-Storm-SparkStream对比","permalink":"https://jjw-story.github.io/tags/Flink-Storm-SparkStream对比/"}],"author":"JJW"},{"title":"Flink-入门","slug":"Flink入门","date":"2021-02-05T07:46:21.505Z","updated":"2021-02-05T08:54:02.962Z","comments":true,"path":"2021/02/05/Flink入门/","link":"","permalink":"https://jjw-story.github.io/2021/02/05/Flink入门/","excerpt":"","text":"Flink入门Flink简介Apache Flink是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution model），能够支持流处理和批处理两种应用类型。 由于流处理和批处理所提供的SLA(服务等级协议)是完全不相同， 流处理一般需要支持低延迟、Exactly-once保证，而批处理需要支持高吞吐、高效处理，所以在实现的时候通常是分别给出两套实现方法，或者通过一个独立的开源框架来实现其中每一种处理方案。比较典型的有：实现批处理的开源方案有MapReduce、Spark；实现流处理的开源方案有Storm；Spark的Streaming 其实本质上也是微批处理。 Flink在实现流处理和批处理时，与传统的一些方案完全不同，它从另一个视角看待流处理和批处理，将二者统一起来：Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。 Flink能在所有常见集群环境中运行，并能运行任意规模应用。Flink 旨在任意规模上运行有状态流式应用。因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO。而且 Flink 很容易维护非常大的应用程序状态。其异步和增量的检查点算法对处理延迟产生最小的影响，同时保证精确一次状态的一致性。 Flink很好的利用内存性能，有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。 处理无界和有界数据在Flink的世界观中，一切都是由流组成的，离线数据是有界的流；实时数据是一个没有界限的流。 无界流：有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流：有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 Flink的应用场景 实时数仓。当下游要构建实时数仓时，上游则可能需要实时的 Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数仓的整个链路中，可保证数据查询的时效性，形成实时数据采集、实时数据处理以及下游的实时 Query。 搜索引擎数据同步。搜索引擎这块以淘宝为例，当卖家上线新商品时，后台会实时产生消息流，该消息流经过 Flink 系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样当淘宝卖家上线新商品时，能在秒级或者分钟级实现搜索引擎的搜索。 实时监控。对用户行为或者相关事件进行实时监测和分析，基于风控规则进行预警。 实时大屏、实时报表。双11、双12等活动直播大屏，数据化运营等。 流式处理的演变传统事务处理架构传统的事务处理一般分为两层：计算层和存储层。计算层可以是我们的各个业务系统，计算层在数据计算时，可能需要用到一些外部数据，或者还要去修改这些外部数据，这样我们就需要跟传统的一些关系型数据库交互。所以我们的业务系统在收到请求时，需要去关系型数据库去查取数据，或者修改数据，然后进行计算，将计算结果包装成响应返回给用户。具体架构如下： 在使用过程中就会发现一些问题，因为数据量太大，数据来源多种多样，可能来自于mysql,sqlserver,orcale等多个关系型数据库，和一些非结构化的数据，比如前端埋点数据，可能为JSON或其他格式，一个单纯的关系型数据库根本无法处理。并且关系型数据库的数据存储会受到限制，存储的数据是有限的，所以我们一般只会存储一些与业务强关联的数据，但我们的一些埋点数据，我们是不需要把它存储起来的，或者说不需要存储到关系型数据库中，这个时候我们分析处理架构就诞生了。 离线分析处理架构将数据从业务数据库或业务日志复制到数仓，再进行分析查询。基本过程就是，将数据拉取经过ETL处理之后，放到Data Warehouse中，然后直接基于数仓进行计算，生成我们需要的报表，或者将计算结果返回给用户。具体架构如下： 这个过程就是我们大数据离线计算的基本应用流程，优点在于：我们能处理的数据更多更加的丰富。缺点在于：我们需要把所有的数据都提取出来放到数仓中，所以这个过程的实时性就无法保证，我们的传统事务处理架构，它是能保证处理结果的实时性的。基于这个问题，流式处理架构又产生了。 有状态的流式处理架构流式处理架构的产生主要是为了解决传统事务处理架构和离线分析处理架构存在的问题，我们为了保证处理的实时性，直接将数据保存在本地内存中，传统数据处理架构中，我们将数据存储在关系型数据库中，流式处理直接将数据存储在本地内存中，我们成为本地状态。优点在于内存数据的读取更快，当数据存储大小受限时，直接扩展集群即可。流式处理架构可以实现每一条请求或者数据到达，经过流式处理，都可以快速得到一条对应的响应。 流式处理像一个管道，数据从一边流入，进行处理后，从另一边流出，但后边如果像继续使用，也可以拿出来消费。 为了保证容错性，架构中还有check point机制，既定时周期性的将本地状态保存在远程的存储空间中，这样当出现故障我们可以通过远程存储中把它恢复出来。具体架构如下： 这样我们就解决了传统处理和离线处理架构中存储大和高延迟的问题。当数据量大时，可以做成分布式，但是分布式可能存在网络延迟带来的乱序问题，这时我们怎么保证数据处理的正确性呢，因为我们流式处理架构是集群架构，我们无法保证当因为网络或者其他问题导致数据延迟到达而产生的的问题（例如我们要统计截止当天结束的某些数据，但是有些数据因为延迟导致它在当天时间节点结束后才到达，这样就会导致计算出现误差甚至错误），此流式架构就无法解决此问题，所以流式处理架构有了一些演变，就产生了lambda架构。 Lambda架构为了解决数据的正确性问题，Lambda架构使用两套系统，同时保证低延迟和结果准确性（流处理保证数据的实时性，批处理保证数据的准确性）。 当有数据输入时，来一条处理一条，隔一段时间，采用批处理的方式，处理一批数据，保证结果的准确性，所以用户这边看到的情况是，数据实时显示，但隔一段时间后，数据可能还会变化。 具体架构如下： Batch Layer：该层主要利用分布式处理系统处理大批量的数据，在数据集上预先计算查询函数，并构建查询所对应的Batch View。即所谓的批处理，适合处理离线数据。 Speed Layer：该层的目的是提供低延时的 Real-time View，处理的都是实时的增量数据。 Serving Layer：Serving Layer 用于响应用户的查询请求，它将 Batch Views 和 Real-time Views 的结果进行了合并，得到最后的结果。 Lambda架构存在以下缺点: Lambda 架构需要在两个不同的API中对同样的业务逻辑进行两次编程：一次为批量计算的系统，一次为流式计算的系统。针对同一个业务问题产生了两个代码库，各有不同的漏洞。这种系统实际上非常难维护。 随着数据增量的增大，T+1 的批处理计算时间可能不够(当天的数据，一个晚上可能处理不完)。 实时与批量计算结果不一致引起的数据口径问题。 Kappa架构与 Lambda 架构不同的是，Kappa 架构去掉了批处理层这一体系结构，而只保留了Speed层。你只需要在业务逻辑改变又或者是代码更改的时候进行数据的重新处理。具体架构如下： 其核心思想就是，使用系统(例如：Kafka)保存历史消息数据, 然后通过回放数据，利用 Real-time Layer 这一层的流处理框架(Flink , Spark Streaming , Storm)来完成业务上的批处理需求。核心步骤如下: 数据需要可以被重放(重新处理)。例如, 用 Kafka 来保存数据，你需要几天的数据量就保存几天。 用新实例重新处理计算重放的数据。即当需要全量重新计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个新的结果存储中。 当新的实例做完后，停止老的流计算实例，并把老的一些结果删除。 需要说明的是：Flink实现了真正的流处理，并且做到了低延迟、高吞吐 和 exactly-once 语义；同时还支持有状态的计算(即使在发生故障时也能准确的处理计算状态) 和 基于事件时间的处理。可见，Flink不管是在 Lambda 架构还是 Kappa 架构中都能占有一席之地，特别是在Kappa 架构中，使用Flink是个很好的选择。","categories":[{"name":"Flink","slug":"Flink","permalink":"https://jjw-story.github.io/categories/Flink/"}],"tags":[{"name":"入门","slug":"入门","permalink":"https://jjw-story.github.io/tags/入门/"},{"name":"Flink","slug":"Flink","permalink":"https://jjw-story.github.io/tags/Flink/"}],"author":"JJW"},{"title":"MySQL-实践三","slug":"MySQL-实践三","date":"2020-08-27T09:07:16.000Z","updated":"2020-09-13T08:46:52.426Z","comments":true,"path":"2020/08/27/MySQL-实践三/","link":"","permalink":"https://jjw-story.github.io/2020/08/27/MySQL-实践三/","excerpt":"","text":"JOIN语句的执行原理我们通过创建两个表 t1 和 t2 来说明，这两个表的结构一模一样，有三个字段(id, a, b)，字段a建立普通索引，我们往t1表插入100条数据，t2插入1000条数据。 Index Nested-Loop Join（NLJ）我们执行如下SQL语句： 1select * from t1 straight_join t2 on (t1.a=t2.a); 如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，这样会影响我们分析 SQL 语句的执行过程。所以，为了便于分析执行过程中的性能问题，改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join。 在这条语句里，被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引，因此这个语句的执行流程是这样的： 从表 t1 中读入一行数据 R； 从数据行 R 中，取出 a 字段到表 t2 里去查找； 取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分； 重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。 在这个流程里： 对驱动表 t1 做了全表扫描，这个过程需要扫描 100 行； 而对于每一行 R，根据 a 字段去表 t2 查找，走的是树搜索过程。由于我们构造的数据都是一一对应的，因此每次的搜索过程都只扫描一行，也是总共扫描 100 行； 所以，整个执行流程，总扫描行数是 200。 如何选择驱动表？ 在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索。 假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 log2M（二分法），所以在被驱动表上查一行的时间复杂度是 2*log2M。 假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。因此整个执行过程，近似复杂度是 N + N2log2M。 显然，N 对扫描行数的影响更大，因此应该让小表来做驱动表。 结论使用 join 语句，性能比强行拆成多个单表执行 SQL 语句的性能要好；如果使用 join 语句的话，需要让小表做驱动表。 Simple Nested-Loop Join我们把查询的SQL改成如下这样: 1select * from t1 straight_join t2 on (t1.a=t2.b); 由于表 t2 的字段 b 上没有索引，因此在执行 取出 a 字段到表 t2 里去查找，就要做一次全表扫描。但是，这样算来，这个 SQL 请求就要扫描表 t2 多达 100 次，总共扫描 100*1000=10 万行。这还只是两个小表，如果数据量大，这个算法就太笨重了，因此，MySQL 也没有使用这个 Simple Nested-Loop Join 算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL。 Block Nested-Loop Join（BNL）上一节中查询语句，被驱动表上没有可用的索引，算法的流程是这样的： 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 可以看到，在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。如果使用 Simple Nested-Loop Join 算法进行查询，扫描行数也是 10 万行。因此，从时间复杂度上来说，这两个算法是一样的。但是，Block Nested-Loop Join 算法的这 10 万次判断是内存操作，速度上会快很多，性能也更好。 这个例子里表 t1 才 100 行，要是表 t1 是一个大表，join_buffer 放不下怎么办呢？ join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。 我把 join_buffer_size 改成 1200，再执行上述查询语句，执行流程就变成： 扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步； 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回； 清空 join_buffer； 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。 这个流程才体现出了这个算法名字中“Block”的由来，表示“分块去 join”。可以看到，这时候由于表 t1 被分成了两次放入 join_buffer 中，导致表 t2 会被扫描两次。虽然分成两次放入 join_buffer，但是判断等值条件的次数还是不变的，依然是 (88+12)*1000=10 万次。 如何选择驱动表？ 假设，驱动表的数据行数是 N，需要分 K 段才能完成算法流程，被驱动表的数据行数是 M。注意，这里的 K 不是常数，N 越大 K 就会越大，因此把 K 表示为λ*N，显然λ的取值范围是 (0,1)。 所以，在这个算法的执行过程中： 扫描行数是 N+λNM； 内存判断 N*M 次。 显然，内存判断次数是不受选择哪个表作为驱动表影响的。而考虑到扫描行数，在 M 和 N 大小确定的情况下，N 小一些，整个算式的结果会更小。join_buffer_size 越大，一次可以放入的行越多，分成的段数也就越少，对被驱动表的全表扫描次数就越少。 所以结论是，应该让小表当驱动表。 小结能不能使用 join 语句？ 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 在判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。 如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？ 结论：在任何情况下都应该使用小表作为驱动。更准确地说，在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 JOIN语句的优化Multi-Range Read 优化Multi-Range Read 优化 (MRR)。这个优化的主要目的是尽量使用顺序读盘。MRR优化主要是针对于回表的优化。回表是指，InnoDB 在普通索引 a 上查到主键 id 的值后，再根据一个个主键 id 的值到主键索引上去查整行数据的过程。 例如我们有一个表，表上有ID和a字段以及其他一些字段，ID是主键，a是一个普通索引字段，我们有一个查询语句是根据a字段来查询的范围查询，这时此查询语句会使用到a索引，然后回表去查询每一行的整体数据，但是这期间根据a的值递增顺序查询的话，id 的值就变成随机的，那么就会出现随机访问，性能相对较差。虽然“按行查”这个机制不能改，但是调整查询的顺序，还是能够加速的。因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。 这就是 MRR 优化的设计思路。此时，语句的执行流程变成了这样： 根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中； 将 read_rnd_buffer 中的 id 进行递增排序； 排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。 这里，read_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的。如果步骤 1 中，read_rnd_buffer 放满了，就会先执行完步骤 2 和 3，然后清空 read_rnd_buffer。之后继续找索引 a 的下个记录，并继续循环。 如果你想要稳定地使用 MRR 优化的话，需要设置set optimizer_switch=”mrr_cost_based=off”。（官方文档的说法，是现在的优化器策略，判断消耗的时候，会更倾向于不使用 MRR，把 mrr_cost_based 设置为 off，就是固定使用 MRR 了。） 使用MRR优化，我们会在explain结果中，看到extra字段多了 Using MRR，表示的是用上了 MRR 优化。 MRR 能够提升性能的核心在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。 Batched Key AccessBatched Key Access(BKA) 算法，其实是对 NLJ 算法的优化。 NLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。这时，MRR 的优势就用不上了。那怎么才能一次性地多传些值给表 t2 呢？方法就是，从表 t1 里一次性地多拿些行出来，一起传给表 t2。既然如此，我们就把表 t1 的数据取出来一部分，先放到一个临时内存。这个临时内存就是 join_buffer。 我们知道 join_buffer 在 BNL 算法里的作用，是暂存驱动表的数据。但是在 NLJ 算法里并没有用。那么，我们刚好就可以复用 join_buffer 到 BKA 算法中。 如果要使用 BKA 优化算法的话，你需要在执行 SQL 语句之前，先设置： 1set optimizer_switch=&apos;mrr=on,mrr_cost_based=off,batched_key_access=on&apos;; 其中，前两个参数的作用是要启用 MRR。这么做的原因是，BKA 算法的优化要依赖于 MRR。 BNL 算法的性能问题我们说到 InnoDB 的 LRU 算法的时候提到，由于 InnoDB 对 Bufffer Pool 的 LRU 算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在 old 区域。如果 1 秒之后这个数据页不再被访问了，就不会被移动到 LRU 链表头部，这样对 Buffer Pool 的命中率影响就不大。但是，如果一个使用 BNL 算法的 join 语句，多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部。这种情况对应的，是冷表的数据量小于整个 Buffer Pool 的 3/8，能够完全放入 old 区域的情况。如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入 young 区域。 大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。 为了减少这种影响，你可以考虑增大 join_buffer_size 的值，减少对被驱动表的扫描次数。也就是说，BNL 算法对系统的影响主要包括三个方面： 可能会多次扫描被驱动表，占用磁盘 IO 资源； 判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源； 可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。 我们执行语句之前，需要通过理论分析和查看 explain 结果的方式，确认是否要使用 BNL 算法。如果确认优化器会使用 BNL 算法，就需要做优化。优化的常见做法是，给被驱动表的 join 字段加上索引，把 BNL 算法转成 BKA 算法。 小结本节主要介绍Index Nested-Loop Join（NLJ）和 Block Nested-Loop Join（BNL）的优化方法。在这些优化方法中： BKA 优化是 MySQL 已经内置支持的，建议你默认使用； BNL 算法效率低，建议你都尽量转成 BKA 算法。优化的方向就是给被驱动表的关联字段加上索引； 基于临时表的改进方案，对于能够提前过滤出小数据的 join 语句来说，效果还是很好的； MySQL 目前的版本还不支持 hash join，但你可以配合应用端自己模拟出来，理论上效果要好于临时表的方案。 Join语句中的其他问题join 的写法注意点： explain一条 join 语句的 Extra 字段什么都没写的话，就表示使用的是 Index Nested-Loop Join（简称 NLJ）算法。 在 MySQL 里，NULL 跟任何值执行等值判断和不等值判断的结果，都是 NULL。这里包括， select NULL = NULL 的结果，也是返回 NULL。 这节我们主要解决两个问题： 如果用 left join 的话，左边的表一定是驱动表吗？ 如果两个表的 join 包含多个条件的等值匹配，是都要写到 on 里面呢，还是只把一个条件写到 on 里面，其他条件写到 where 部分？ 答： 使用 left join 时，左边的表不一定是驱动表。 如果需要 left join 的语义，就不能把被驱动表的字段放在 where 条件里面做等值判断或不等值判断，必须都写在 on 里面。 Simple Nested Loop Join 的性能问题BNL 算法的执行逻辑是： 首先，将驱动表的数据全部读入内存 join_buffer 中，这里 join_buffer 是无序数组； 然后，顺序遍历被驱动表的所有行，每一行数据都跟 join_buffer 中的数据进行匹配，匹配成功则作为结果集的一部分返回。 Simple Nested Loop Join 算法的执行逻辑是： 顺序取出驱动表中的每一行数据，到被驱动表去做全表扫描匹配，匹配成功则作为结果集的一部分返回。 那么问题是，Simple Nested Loop Join 算法，其实也是把数据读到内存里，然后按照匹配条件进行判断，为什么性能差距会这么大呢？ 解释这个问题，需要用到 MySQL 中索引结构和 Buffer Pool 的相关知识点： 在对被驱动表做全表扫描的时候，如果数据没有在 Buffer Pool 中，就需要等待这部分数据从磁盘读入；从磁盘读入数据到内存中，会影响正常业务的 Buffer Pool 命中率，而且这个算法天然会对被驱动表的数据做多次访问，更容易将这些数据页放到 Buffer Pool 的头部； 即使被驱动表数据都在内存中，每次查找“下一个记录的操作”，都是类似指针操作。而 join_buffer 中是数组，遍历的成本更低。所以说，BNL 算法的性能会更好。 所以说，BNL 算法的性能会更好。 distinct 和 group by 的性能如下两条语句： 12select a from t group by a order by null;select distinct a from t; 首先需要说明的是，这种 group by 的写法，并不是 SQL 标准的写法。标准的 group by 语句，是需要在 select 部分加一个聚合函数，比如： 1select a,count(*) from t group by a order by null; 这条语句的逻辑是：按照字段 a 分组，计算每组的 a 出现的次数。在这个结果里，由于做的是聚合计算，相同的 a 只出现一次。 没有了 count(*) 以后，也就是不再需要执行“计算总数”的逻辑时，第一条语句的逻辑就变成是：按照字段 a 做分组，相同的 a 的值只返回一行。而这就是 distinct 的语义，所以不需要执行聚合函数时，distinct 和 group by 这两条语句的语义和执行流程是相同的，因此执行性能也相同。 这两条语句的执行流程是下面这样的： 创建一个临时表，临时表有一个字段 a，并且在这个字段 a 上创建一个唯一索引； 遍历表 t，依次取数据插入临时表中：如果发现唯一键冲突，就跳过；否则插入成功； 遍历完成后，将临时表作为结果集返回给客户端。 MySQL临时表的内部机制临时表的特征有的人可能会认为，临时表就是内存表。但是，这两个概念可是完全不同的。 内存表，指的是使用 Memory 引擎的表，建表语法是 create table … engine=memory。这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表。 临时表，可以使用各种引擎类型 。如果是使用 InnoDB 引擎或者 MyISAM 引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用 Memory 引擎。 临时表主要有一下一些特点： 建表语法是 create temporary table …。 一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。 临时表可以与普通表同名。 session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。 show tables 命令不显示临时表。 由于临时表只能被创建它的 session 访问，所以在这个 session 结束的时候，会自动删除临时表。也正是由于这个特性，临时表就特别适合我们文章开头的 join 优化这种场景，原因主要包括以下两个方面： 不同 session 的临时表是可以重名的，如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。 不需要担心数据删除问题。如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。而临时表由于会自动回收，所以不需要这个额外的操作。 为什么临时表可以重名？同线程可以创建同名的临时表，这是怎么做到的呢？比如我们通过如下语句创建一个临时表： 1create temporary table temp_t(id int primary key)engine=innodb; 这个语句的时候，MySQL 要给这个 InnoDB 表创建一个 frm 文件保存表结构定义，还要有地方保存表数据。这个 frm 文件放在临时文件目录下，文件名的后缀是.frm，前缀是“#sql{进程 id}{线程 id} 序列号”。你可以使用 select @@tmpdir 命令，来显示实例的临时文件目录。从文件名的前缀规则，我们可以看到，其实创建一个叫作 t1 的 InnoDB 临时表，MySQL 在存储上认为我们创建的表名跟普通表 t1 是不同的，因此同一个库下面已经有普通表 t1 的情况下，还是可以再创建一个临时表 t1 的。 MySQL 维护数据表，除了物理上要有文件外，内存里面也有一套机制区别不同的表，每个表都对应一个 table_def_key。 一个普通表的 table_def_key 的值是由“库名 + 表名”得到的，所以如果你要在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现 table_def_key 已经存在了。 而对于临时表，table_def_key 在“库名 + 表名”基础上，又加入了“server_id+thread_id”。 也就是说，session A 和 sessionB 创建的两个临时表 t1，它们的 table_def_key 不同，磁盘文件名也不同，因此可以并存。 在实现上，每个线程都维护了自己的临时表链表。这样每次 session 内操作表的时候，先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表，如果没有再操作普通表；在 session 结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE + 表名”操作。 小结临时表建表语法create temporary table ，和普通的表不一样，和内存表也不一样。内存表数据保存到内存里，重启会丢失，临时表会写入到磁盘。临时表只对自己的session中可见，session结束后自动删除表结构和表数据。适用场景是分库分表，查询到的数据在临时表中做聚合。临时表可以重名，实际的存储文件名有线程id，在内存中表的命名有table_ref_key，是由库名加表名加serverid+线程id组成。bin log设置为row模式，临时表不会同步到备库中，设置为statement模式，会同步到备库中。 什么时候会使用内部临时表？union 执行流程例如我们执行下面这条语句： 1(select 1000 as f) union (select id from t1 order by id desc limit 2); 这个语句的执行流程是这样的： 创建一个内存临时表，这个临时表只有一个整型字段 f，并且 f 是主键字段。 执行第一个子查询，得到 1000 这个值，并存入临时表中。 执行第二个子查询：拿到第一行 id=1000，试图插入临时表中。但由于 1000 这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行；取到第二行 id=999，插入临时表成功。 从临时表中按行取出数据，返回结果，并删除临时表，结果中包含两行数据分别是 1000 和 999。 可以看到，这里的内存临时表起到了暂存数据的作用，而且计算过程还用上了临时表主键 id 的唯一性约束，实现了 union 的语义。如果把上面这个语句中的 union 改成 union all 的话，就没有了“去重”的语义。这样执行的时候，就依次执行子查询，得到的结果直接作为结果集的一部分，发给客户端。因此也就不需要临时表了。 group by 执行流程例如我们执行下面这条语句： 1select id%10 as m, count(*) as c from t1 group by m; 这个语句的执行流程是这样的： 创建内存临时表，表里有两个字段 m 和 c，主键是 m； 扫描表 t1 的索引 a，依次取出叶子节点上的 id 值，计算 id%10 的结果，记为 x；如果临时表中没有主键为 x 的行，就插入一个记录 (x,1); 如果表中有主键为 x 的行，就将 x 这一行的 c 值加 1； 遍历完成后，再根据字段 m 做排序，得到结果集返回给客户端。 explain 这条语句的结果，在 Extra 字段里面，我们可以看到三个信息： Using index，表示这个语句使用了覆盖索引，选择了索引 a，不需要回表； Using temporary，表示使用了临时表； Using filesort，表示需要排序。 如果你的需求并不需要对结果进行排序，那你可以在 SQL 语句末尾增加 order by null，也就是改成： 1select id%10 as m, count(*) as c from t1 group by m order by null; 这个例子里由于临时表只有 10 行，内存可以放得下，因此全程只使用了内存临时表。但是，内存临时表的大小是有限制的，参数 tmp_table_size 就是控制这个内存大小的，默认是 16M。 把内存临时表的大小限制为最大 1024 字节，并把语句改成 id % 100，这样返回结果里有 100 行数据。但是，这时的内存临时表大小不够存下这 100 行数据，也就是说，执行过程中会发现内存临时表大小到达了上限（1024 字节）。那么，这时候就会把内存临时表转成磁盘临时表，磁盘临时表默认使用的引擎是 InnoDB。 如果这个表 t1 的数据量很大，很可能这个查询需要的磁盘临时表就会占用大量的磁盘空间。 group by 优化方法 – 索引不论是使用内存临时表还是磁盘临时表，group by 逻辑都需要构造一个带唯一索引的表，执行代价都是比较高的。如果表的数据量比较大，上面这个 group by 语句执行起来就会很慢，group by 的语义逻辑，是统计不同的值出现的个数。但是，由于每一行的 id%100 的结果是无序的，所以我们就需要有一个临时表，来记录并统计结果。那么，如果扫描过程中可以保证出现的数据是有序的，是不是就简单了呢？ InnoDB 的索引，就可以满足这个输入有序的条件。 小结基于上面的 union、union all 和 group by 语句的执行过程的分析，我们来回答文章开头的问题：MySQL 什么时候会使用内部临时表？ 如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果； join_buffer 是无序数组，sort_buffer 是有序数组，临时表是二维表结构； 如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union 需要用到唯一索引约束， group by 还需要用到另外一个字段来存累积计数。 内部临时表一些使用的指导原则： 如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null； 尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort； 如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大 tmp_table_size 参数，来避免用到磁盘临时表； 如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果。 Memory引擎内存表的数据组织结构InnoDB 表的数据就放在主键索引树上，主键索引是 B+ 树。与 InnoDB 引擎不同，Memory 引擎的数据和索引是分开的。内存表的数据部分以数组的方式单独存放，而主键 id 索引里，存的是每个数据的位置。主键 id 是 hash 索引，可以看到索引上的 key 并不是有序的。InnoDB 和 Memory 引擎的数据组织方式是不同的： InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，我们称之为索引组织表（Index Organizied Table）。 而 Memory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表（Heap Organizied Table）。 从中我们可以看出，这两个引擎的一些典型不同： InnoDB 表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的； 当数据文件有空洞的时候，InnoDB 表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值； 数据位置发生变化的时候，InnoDB 表只需要修改主键索引，而内存表需要修改所有索引； InnoDB 表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的。 InnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。 由于内存表的这些特性，每个数据行被删除以后，空出的这个位置都可以被接下来要插入的数据复用。 hash 索引和 B-Tree 索引实际上，内存表也是支 B-Tree 索引的。在 id 列上创建一个 B-Tree 索引，SQL 语句可以这么写： 1alter table t1 add index a_btree_index using btree (id); 这样新增的这个 B-Tree 索引你看着就眼熟了，这跟 InnoDB 的 b+ 树索引组织形式类似。 其实，一般在我们的印象中，内存表的优势是速度快，其中的一个原因就是 Memory 引擎支持 hash 索引。当然，更重要的原因是，内存表的所有数据都保存在内存，而内存的读写速度总是比磁盘快。 内存表的锁内存表不支持行锁，只支持表锁。因此，一张表只要有更新，就会堵住其他所有在这个表上的读写操作。需要注意的是，这里的表锁跟 MDL 锁不同，但都是表级的锁。 跟行锁比起来，表锁对并发访问的支持不够好。所以，内存表的锁粒度问题，决定了它在处理并发事务的时候，性能也不会太好。 小结建议把普通内存表都用 InnoDB 表来代替，分析如下： 如果你的表更新量大，那么并发度是一个很重要的参考指标，InnoDB 支持行锁，并发度比内存表好； 能放到内存表的数据量都不大。如果你考虑的是读的性能，一个读 QPS 很高并且数据量不大的表，即使是使用 InnoDB，数据也是都会缓存在 InnoDB Buffer Pool 里的。因此，使用 InnoDB 表的读性能也不会差。 但是，有一个场景却是例外的，就是之前我们说的JOIN和GROUP等查询条件下，在数据量可控，不会耗费过多内存的情况下，你可以考虑使用内存表。内存临时表刚好可以无视内存表的两个不足，主要是下面的三个原因： 临时表不会被其他线程访问，没有并发性的问题； 临时表重启后也是需要删除的，清空数据这个问题不存在； 备库的临时表也不会影响主库的用户线程。 本节我们介绍了 Memory 引擎的几个特性。可以看到，由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉。因此，在生产上，我不建议你使用普通内存表。 自增主键为什么不是连续的？自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑。但是我们不能将业务设计依赖于自增主键的连续性，也就是说，这个设计假设自增主键是连续的。但实际上，这样的假设是错的，因为自增主键不能保证连续递增。 自增值保存在哪儿？不同的引擎对于自增值的保存策略不同： MyISAM 引擎的自增值保存在数据文件中。 InnoDB 引擎的自增值，其实是保存在了内存里，并且到了 MySQL 8.0 版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为 MySQL 重启前的值”，具体情况是： 在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。举例来说，如果一个表当前数据行里最大的 id 是 10，AUTO_INCREMENT=11。这时候，我们删除 id=10 的行，AUTO_INCREMENT 还是 11。但如果马上重启实例，重启后这个表的 AUTO_INCREMENT 就会变成 10。也就是说，MySQL 重启可能会修改一个表的 AUTO_INCREMENT 的值。 在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。 自增值修改机制在 MySQL 里面，如果字段 id 被定义为 AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下： 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT 值填到自增字段； 如果插入数据时 id 字段指定了具体的值，就直接使用语句里指定的值。 根据要插入的值和当前自增值的大小关系，自增值的变更结果也会有所不同。假设，某次要插入的值是 X，当前的自增值是 Y。 (如果X &lt; Y，那么这个表的自增值不变；如果 X ≥ Y，就需要把当前自增值修改为新的自增值。) 新的自增值生成算法是：从 auto_increment_offset 开始，以 auto_increment_increment 为步长，持续叠加，直到找到第一个大于 X 的值，作为新的自增值。其中，auto_increment_offset 和 auto_increment_increment 是两个系统参数，分别用来表示自增的初始值和步长，默认值都是 1。(备注：在一些场景下，使用的就不全是默认值。比如，双 M 的主备结构里要求双写的时候，我们就可能会设置成 auto_increment_increment=2，让一个库的自增 id 都是奇数，另一个库的自增 id 都是偶数，避免两个库生成的主键发生冲突。) 自增值的修改时机自增值在插入语句获取到自增值后就进行了修改，它并不会等语句或者事务执行完成，也不会回滚。 所以：唯一键冲突是导致自增主键 id 不连续的第一种原因。同样地，事务回滚也会产生类似的现象，这就是第二种原因。 自增值为什么不能回退假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。 假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。 事务 B 正确提交了，但事务 A 出现了唯一键冲突。 如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。 接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。 以上便是自增值不能回退的具体原因。 分区表分区表的创建方式如下示例：、 1234567891011121314# 创建表CREATE TABLE `t` ( `ftime` datetime NOT NULL, `c` int(11) DEFAULT NULL, KEY (`ftime`)) ENGINE=InnoDB DEFAULT CHARSET=latin1PARTITION BY RANGE (YEAR(ftime))(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB, PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB, PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB,PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);# 插入两条数据insert into t values(&apos;2017-4-1&apos;,1),(&apos;2018-4-1&apos;,1); 可以看到，这个表包含了一个.frm 文件和 4 个.ibd 文件，每个分区对应一个.ibd 文件。也就是说： 对于引擎层来说，这是 4 个表； 对于 Server 层来说，这是 1 个表。 分区表的引擎层行为上述示例中，我们在表中插入了两条数据，这两条记录的数据分别会落在两个分区，现在我们执行如下查询语句： 1select * from t where ftime = &apos;2017-5-1&apos; for update; 按照正常的逻辑，我们的这条查询语句，会为加上间隙锁 (2017-4-1, 2018-4-1)，但实际加锁会加在 p-2017分区上，间隙锁为(2017-4-1, supremum)，p-2018分区是不受影响的，也就是说关于2018的增删改查是不受影响的。 分区表的Server层行为如果从 server 层看的话，一个分区表就只是一个表，分区表有如下特性： MySQL 在第一次打开分区表的时候，需要访问所有的分区； 在 server 层，认为这是同一张表，因此所有分区共用同一个 MDL 锁； 在引擎层，认为这是不同的表，因此 MDL 锁之后的执行过程，会根据分区表规则，只访问必要的分区。 而关于“必要的分区”的判断，就是根据 SQL 语句中的 where 条件，结合分区规则来实现的。比如我们上面的例子中，where ftime=‘2018-4-1’，根据分区规则 year 函数算出来的值是 2018，那么就会落在 p_2018 这个分区。但是，如果这个 where 条件改成 where ftime&gt;=‘2018-4-1’，虽然查询结果相同，但是这时候根据 where 条件，就要访问 p_2018 和 p_others 这两个分区。 如果查询语句的 where 条件中没有分区 key，那就只能访问所有分区了。当然，这并不是分区表的问题。即使是使用业务分表的方式，where 条件中没有使用分表的 key，也必须访问所有的分表。 分区表的应用场景分区表的一个显而易见的优势是对业务透明，相对于用户分表来说，使用分区表的业务代码更简洁。还有，分区表可以很方便的清理历史数据。 如果一项业务跑的时间足够长，往往就会有根据时间删除历史数据的需求。这时候，按照时间分区的分区表，就可以直接通过 alter table t drop partition …这个语法删掉分区，从而删掉过期的历史数据。 这个 alter table t drop partition …操作是直接删除分区文件，效果跟 drop 普通表类似。与使用 delete 语句删除数据相比，优势是速度快、对系统影响小。 小结分区表跟用户分表比起来，有两个绕不开的问题：一个是第一次访问的时候需要访问所有分区，另一个是共用 MDL 锁。因此，如果要使用分区表，就不要创建太多的分区。我见过一个用户做了按天分区策略，然后预先创建了 10 年的分区。这种情况下，访问分区表的性能自然是不好的。这里有两个问题需要注意： 分区并不是越细越好。实际上，单表或者单分区的数据一千万行，只要没有特别大的索引，对于现在的硬件能力来说都已经是小表了。 分区也不要提前预留太多，在使用之前预先创建即可。比如，如果是按月分区，每年年底时再把下一年度的 12 个新分区创建上即可。对于没有数据的历史分区，要及时的 drop 掉。 至于分区表的其他问题，比如查询需要跨多个分区取数据，查询性能就会比较慢，基本上就不是分区表本身的问题，而是数据量的问题或者说是使用方式的问题了。 自增id用完怎么办？MySQL 里有很多自增的 id，每个自增 id 都是定义了初始值，然后不停地往上加步长。虽然自然数是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无符号整型 (unsigned int) 是 4 个字节，上限就是 2^32 - 1。 表定义自增值 id表定义的自增值达到上限后的逻辑是：再申请下一个 id 时，得到的值保持不变。 当第一个 insert 语句插入数据成功后，自增值拿到的是最大值，这个表的 AUTO_INCREMENT 没有改变（还是 4294967295），就样就会导致了第二个 insert 语句又拿到相同的自增 id 值，再试图执行插入语句，报主键冲突错误。 2^32-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成 8 个字节的 bigint unsigned。 InnoDB 系统自增 row_id如果你创建的 InnoDB 表没有指定主键，那么 InnoDB 会给你创建一个不可见的，长度为 6 个字节的 row_id。InnoDB 维护了一个全局的 dict_sys.row_id 值，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1。 实际上，在代码实现时 row_id 是一个长度为 8 字节的无符号长整型 (bigint unsigned)。但是，InnoDB 在设计时，给 row_id 留的只是 6 个字节的长度，这样写到数据表中时只放了最后 6 个字节，所以 row_id 能写到数据表中的值，就有两个特征： row_id 写入表中的值范围，是从 0 到 2^48-1； 当 dict_sys.row_id=2^48时，如果再有插入数据的行为要来申请 row_id，拿到以后再取最后 6 个字节的话就是 0。 也就是说，写入表的 row_id 是从 0 开始到 2^48-1。达到上限后，下一个值就是 0，然后继续循环。 当然，2^48-1 这个值本身已经很大了，但是如果一个 MySQL 实例跑得足够久的话，还是可能达到这个上限的。在 InnoDB 逻辑里，申请到 row_id=N 后，就将这行数据写入表中；如果表中已经存在 row_id=N 的行，新写入的行就会覆盖原有的行。 从这个角度看，我们还是应该在 InnoDB 表中主动创建自增主键。因为，表自增 id 到达上限后，再插入数据时报主键冲突错误，是更能被接受的。毕竟覆盖数据，就意味着数据丢失，影响的是数据可靠性； Xidredo log 和 binlog 相配合的时候，它们有一个共同的字段叫作 Xid。它在 MySQL 中是用来对应事务的。 MySQL 内部维护了一个全局变量 global_query_id，每次执行语句的时候将它赋值给 Query_id，然后给这个变量加 1。如果当前语句是这个事务执行的第一条语句，那么 MySQL 还会同时把 Query_id 赋值给这个事务的 Xid。而 global_query_id 是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实例中，不同事务的 Xid 也是有可能相同的。 而 global_query_id 是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实例中，不同事务的 Xid 也是有可能相同的。但是 MySQL 重启之后会重新生成新的 binlog 文件，这就保证了，同一个 binlog 文件里，Xid 一定是惟一的。 虽然 MySQL 重启不会导致同一个 binlog 里面出现两个相同的 Xid，但是如果 global_query_id 达到上限后，就会继续从 0 开始计数。从理论上讲，还是就会出现同一个 binlog 里面出现相同 Xid 的场景。因为 global_query_id 定义的长度是 8 个字节，这个自增值的上限是 264-1。要出现这种情况，必须是下面这样的过程： 执行一个事务，假设 Xid 是 A； 接下来执行 2^64次查询语句，让 global_query_id 回到 A； 再启动一个事务，这个事务的 Xid 也是 A。 不过，2^64这个值太大了，大到你可以认为这个可能性只会存在于理论上。 Innodb trx_idXid 是由 server 层维护的。InnoDB 内部使用 Xid，就是为了能够在 InnoDB 事务和 server 之间做关联。但是，InnoDB 自己的 trx_id，是另外维护的。trx_id就是讲事务可见性时，用到的事务 id（transaction id）。 InnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。InnoDB 数据可见性的核心思想是：每一行数据都记录了更新它的 trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的 trx_id 做对比。 注意：只读事务不分配 trx_id，这是因为： 一个好处是，这样做可以减小事务视图里面活跃事务数组的大小。因为当前正在运行的只读事务，是不影响数据的可见性判断的。所以，在创建事务的一致性视图时，InnoDB 就只需要拷贝读写事务的 trx_id。 另一个好处是，可以减少 trx_id 的申请次数。在 InnoDB 里，即使你只是执行一个普通的 select 语句，在执行过程中，也是要对应一个只读事务的。所以只读事务优化后，普通的查询语句不需要申请 trx_id，就大大减少了并发事务申请 trx_id 的锁冲突。 由于只读事务不分配 trx_id，一个自然而然的结果就是 trx_id 的增加速度变慢了。 但是，max_trx_id 会持久化存储，重启也不会重置为 0，那么从理论上讲，只要一个 MySQL 服务跑得足够久，就可能出现 max_trx_id 达到 248-1 的上限，然后从 0 开始的情况。这样就会出现脏读的情况。由于低水位值会持续增加，而事务 id 从 0 开始计数，就导致了系统在这个时刻之后，所有的查询都会出现脏读的。并且，MySQL 重启时 max_trx_id 也不会清 0，也就是说重启 MySQL，这个 bug 仍然存在。 假设一个 MySQL 实例的 TPS 是每秒 50 万，持续这个压力的话，在 17.8 年后，就会出现这个情况。如果 TPS 更高，这个年限自然也就更短了。但是，从 MySQL 的真正开始流行到现在，恐怕都还没有实例跑到过这个上限。不过，这个 bug 是只要 MySQL 实例服务时间够长，就会必然出现的。 thread_id线程 id 才是 MySQL 中最常见的一种自增 id。平时我们在查各种现场的时候，show processlist 里面的第一列，就是 thread_id。thread_id 的逻辑很好理解：系统保存了一个全局变量 thread_id_counter，每新建一个连接，就将 thread_id_counter 赋值给这个新连接的线程变量。 thread_id_counter 定义的大小是 4 个字节，因此达到 232-1 后，它就会重置为 0，然后继续增加。但是，你不会在 show processlist 里看到两个相同的 thread_id。是因为 MySQL 设计了一个唯一数组的逻辑，给新线程分配 thread_id 的时候，逻辑代码是这样的： 123do &#123; new_id= thread_id_counter++;&#125; while (!thread_ids.insert_unique(new_id).second); 小结每种自增 id 有各自的应用场景，在达到上限后的表现也不同： 表的自增 id 达到上限后，再申请时它的值就不会改变，进而导致继续插入数据时报主键冲突的错误。 row_id 达到上限后，则会归 0 再重新递增，如果出现相同的 row_id，后写的数据会覆盖之前的数据。 Xid 只需要不在同一个 binlog 文件中出现重复值即可。虽然理论上会出现重复值，但是概率极小，可以忽略不计。 InnoDB 的 max_trx_id 递增值每次 MySQL 重启都会被保存起来，所以我们文章中提到的脏读的例子就是一个必现的 bug，好在留给我们的时间还很充裕。 thread_id 是我们使用中最常见的，而且也是处理得最好的一个自增 id 逻辑了。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/tags/MySQL/"},{"name":"实践三","slug":"实践三","permalink":"https://jjw-story.github.io/tags/实践三/"}],"author":"JJW"},{"title":"Scala-高阶","slug":"Scala-高阶","date":"2020-08-06T07:22:56.000Z","updated":"2020-08-06T07:27:33.593Z","comments":true,"path":"2020/08/06/Scala-高阶/","link":"","permalink":"https://jjw-story.github.io/2020/08/06/Scala-高阶/","excerpt":"","text":"Scala-高阶123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428package story.jjwimport java.io.IOExceptionobject Two &#123; def main(args: Array[String]): Unit = &#123; // 1. 将函数赋值给变量 sayHelloFunc(&quot;JJW&quot;) // 2.匿名函数 sayHelloFuncA(&quot;JJW&quot;) // 3.高阶函数 greeting(sayHelloFunc, &quot;SS&quot;) // 4.scala常用高阶函数 // map: 对传入的每个元素都进行映射，返回一个处理后的元素 val arr = Array(1, 2, 3, 4, 5).map(2 * _) println(arr.toBuffer) // foreach: 对传入的每个元素都进行处理，但是没有返回值 arr.map(2 * _).foreach(println _) // filter: 对传入的每个元素都进行条件判断，如果对元素返回true，则保留该元素，否则过滤掉该元素 var arrA = arr.filter(_ % 3 == 0) println(arrA.toBuffer) // reduceLeft: 从左侧元素开始，进行reduce操作，即先对元素1和元素2进行处理，然后将结果与元素3处理，再将结果与元素4处理，依次类推，即为reduce；reduce操作必须掌握！spark编程的重点！！！ // 下面这个操作就相当于1 * 2 * 3 * 4 * 5 * 6 * 7 * 8 * 9 val sum = (1 to 9).reduceLeft(_ + _) println(sum) // sortWith: 对元素进行两两相比，进行排序 val sortArr = Array(3, 2, 5, 4, 10, 1).sortWith(_ - _ &gt; 0) println(sortArr.toBuffer) // 5.闭包 // 闭包最简洁的解释：函数在变量不处于其有效作用域时，还能够对变量进行访问，即为闭包 // 两次调用getGreetingFunc函数，传入不同的msg，并创建不同的函数返回 // 然而，msg只是一个局部变量，却在getGreetingFunc执行完之后，还可以继续存在创建的函数之中；greetingFuncHello(&quot;JJW&quot;)，调用时，值为&quot;hello&quot;的msg被保留在了函数体内部，可以反复的使用 // 这种变量超出了其作用域，还可以使用的情况，即为闭包 // Scala通过为每个函数创建对象来实现闭包，实际上对于getGreetingFunc函数创建的函数，msg是作为函数对象的变量存在的，因此每个函数才可以拥有不同的msg val greetingFuncHello = getGreetingFunc(&quot;hello&quot;) val greetingFuncHi = getGreetingFunc(&quot;hi&quot;) greetingFuncHello(&quot;JJW&quot;) greetingFuncHi(&quot;SS&quot;) // 6.Curring函数 // Curring函数，指的是，将原来接收两个参数的一个函数，转换为两个函数，第一个函数接收原先的第一个参数，然后返回接收原先第二个参数的第二个函数。 // 在函数调用的过程中，就变为了两个函数连续调用的形式 sum1(1, 1) sum2(1)(1) sum3(1)(1) // 7.集合 // Scala中的集合体系主要包括：Iterable、Seq、Set、Map。其中Iterable是所有集合trait的根trai // Scala中的集合是分成可变和不可变两类集合的，其中可变集合就是说，集合的元素可以动态修改，而不可变集合的元素在初始化之后，就无法修改了。分别对应scala.collection.mutable和scala.collection.immutable两个包。 // 7.1 List // List代表一个不可变的列表 // List的创建 val list = List(1, 2, 3, 4) // List有head和tail，head代表List的第一个元素，tail代表第一个元素之后的所有元素，list.head，list.tail // List有特殊的::操作符，可以用于将head和tail合并成一个List，0 :: list // 7.2 LinkedList // LinkedList代表一个可变的列表，使用elem可以引用其头部，使用next可以引用其尾部 // val l = scala.collection.mutable.LinkedList(1, 2, 3, 4, 5); l.elem; l.next // 7.3 Set // Set代表一个没有重复元素的集合，Set中的元素是乱序的 var s = Set(1, 2, 3) s += 4 println(s) // LinkedHashSet会用一个链表维护插入顺序 val ls = new scala.collection.mutable.LinkedHashSet[Int](); ls += 1; ls += 2; ls += 5 // SortedSet会自动根据key来进行排序 val ss = scala.collection.mutable.SortedSet(&quot;orange&quot;, &quot;apple&quot;, &quot;banana&quot;) // 7.4 Scala的高阶函数，Scala的集合类的map、flatMap、reduce、reduceLeft、foreach等这些函数，就是高阶函数，因为可以接收其他函数作为参数 // map案例实战：为List中每个元素都添加一个前缀 List(&quot;Leo&quot;, &quot;Jen&quot;, &quot;Peter&quot;, &quot;Jack&quot;).map(&quot;name is &quot; + _) // faltMap案例实战：将List中的多行句子拆分成单词 List(&quot;Hello World&quot;, &quot;You Me&quot;).flatMap(_.split(&quot; &quot;)) // foreach案例实战：打印List中的每个单词 List(&quot;I&quot;, &quot;have&quot;, &quot;a&quot;, &quot;beautiful&quot;, &quot;house&quot;).foreach(println(_)) // zip案例实战：对学生姓名和学生成绩进行关联 List(&quot;Leo&quot;, &quot;Jen&quot;, &quot;Peter&quot;, &quot;Jack&quot;).zip(List(100, 90, 75, 83)) // Set(1, 2, 3, 4) =&gt; Set((1,1), (2,1), (3,1), (4,1)) var sp = s.map((_, 1)) println(sp) var sp1 = sp.map(_._2) println(sp1) var sp2 = sp1.reduceLeft(_ + _) println(sp2) // 8.模式匹配 judgeGrade(&quot;JJW&quot;, &quot;dongdong&quot;) processException(new IOException(&quot;报错啦...&quot;)) matchArray(Array(&quot;Leo&quot;, &quot;JJW&quot;, &quot;SS&quot;)) matchList(List(&quot;Leo&quot;, &quot;JJW&quot;, &quot;SS&quot;)) // 9.泛型 // 泛型类 val st = new StudentD[Int](33) println(st.getSchoolId(52)) // 泛型函数 println(st.getCard[String](&quot;身份证&quot;)) // 上下边界 val steA = new StudentE(&quot;JJW&quot;) val steB = new StudentE(&quot;SS&quot;) val pa = new PartyA(steA, steB) pa.play val pb = new PartyB(steA, steB) pb.play // 10.协变和逆变 // enterMeetA(new CardA[Master](&quot;JJW&quot;)) 报错，因为方法形参定义的是CardA[Professional] CardA[+T]，所以在这里只能是比Professional更子级的类 enterMeetA(new CardA[Professional](&quot;JJW&quot;)) enterMeetB(new CardB[Master](&quot;JJW&quot;)) // enterMeetB(new CardB[Professional](&quot;JJW&quot;)) 报错，因为方法形参定义的是CardA[Master] CardA[-T]，所以在这里只能是比Master更父级的类 // 11.隐式转换 // Scala的隐式转换，其实最核心的就是定义隐式转换函数，即implicit conversion function。 // 定义的隐式转换函数，只要在编写的程序内引入，就会被Scala自动使用。Scala会根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回。 // 隐式转换的发生时机 // 1、调用某个函数，但是给函数传入的参数的类型，与函数定义的接收参数类型不匹配（案例：特殊售票窗口） // 2、使用某个类型的对象，调用某个方法，而这个方法并不存在于该类型时（案例：超人变身） // 3、使用某个类型的对象，调用某个方法，虽然该类型有这个方法，但是给方法传入的参数类型，与方法定义的接收参数的类型不匹配（案例：特殊售票窗口加强版） val stF = new StudentF(&quot;JJW&quot;) println(buySpecialTicket(stF)) // 隐式转换-装饰器模式 val leo = new Man(&quot;leo&quot;) leo.emitLaser // 隐式转换-隐式参数 // 定义隐式参数 implicit val signPen = new SignPen signForExam(&quot;JJW&quot;) // 12.Actor // Scala提供了Actor trait来让我们更方便地进行actor多线程编程，就Actor trait就类似于Java中的Thread和Runnable一样，是基础的多线程基类和接口。我们只要重写Actor trait的act方法，即可实现自己的线程执行体，与Java中重写run方法类似 // 使用start()方法启动actor；使用!符号，向actor发送消息；actor内部使用receive和模式匹配接收消息 // Scala的Actor模型与Java的多线程模型之间，很大的一个区别就是，Scala Actor天然支持线程之间的精准通信；即一个actor可以给其他actor直接发送消息。这个功能是非常强大和方便的。 val userManageActor = new UserManageActor userManageActor.start() userManageActor ! Register(&quot;JJW&quot;, &quot;5233&quot;) // 默认情况下，消息都是异步的；但是如果希望发送的消息是同步的，即对方接受后，一定要给自己返回结果，那么可以使用!?的方式发送消息。即val reply = actor !? message。 // 如果要异步发送一个消息，但是在后续要获得消息的返回值，那么可以使用Future。即!!语法。val future = actor !! message。val reply = future()。 &#125; // Scala中的函数是一等公民，可以独立定义，独立存在，而且可以直接将函数作为值赋值给变量 // Scala的语法规定，将函数赋值给变量时，必须在函数后面加上空格和下划线 def sayHello(name: String) &#123; println(&quot;Hello, &quot; + name) &#125; val sayHelloFunc = sayHello _ // 可以直接定义函数之后，将函数赋值给某个变量；也可以将直接定义的匿名函数传入其他函数之中 // Scala定义匿名函数的语法规则就是，(参数名: 参数类型) =&gt; 函数体 val sayHelloFuncA = (name: String) =&gt; println(&quot;Hello, &quot; + name) // 直接将某个函数传入其他函数，作为参数，这种函数叫做高阶函数 def greeting(func: (String) =&gt; Unit, name: String): Unit = &#123; func(name) &#125; // Scala通过为每个函数创建对象来实现闭包，实际上对于getGreetingFunc函数创建的函数，msg是作为函数对象的变量存在的，因此每个函数才可以拥有不同的msg def getGreetingFunc(msg: String) = (name: String) =&gt; println(msg + &quot;, &quot; + name) // Curring函数 def sum1(a: Int, b: Int) = a + b def sum2(a: Int) = (b: Int) =&gt; a + b def sum3(a: Int)(b: Int) = a + b // 模式匹配 // Scala的match case可以匹配各种情况，比如变量的类型、集合的元素、有值或无值 // match case中，只要一个case分支满足并处理了，就不会继续判断下一个case分支了 def judgeGrade(name: String, grade: String) &#123; grade match &#123; case &quot;A&quot; =&gt; println(name + &quot;, you are excellent&quot;) case &quot;B&quot; =&gt; println(name + &quot;, you are good&quot;) case &quot;C&quot; =&gt; println(name + &quot;, you are just so so&quot;) // 在case后的条件判断中，不仅仅只是提供一个值，而是可以在值后面再加一个if守卫，进行双重过滤 case _grade if name == &quot;leo&quot; =&gt; println(name + &quot;, you are a good boy, come on, your grade is &quot; + _grade) case _grade =&gt; println(&quot;you need to work harder, your grade is &quot; + _grade) // 如果值为下划线，则代表了不满足以上所有情况下的默认情况如何处理 case _ =&gt; println(&quot;you need to work harder&quot;) &#125; &#125; // 模式匹配-类型匹配 // 语法：case 变量: 类型 =&gt; 代码 def processException(e: Exception): Unit = &#123; e match &#123; case e1: IndexOutOfBoundsException =&gt; println(&quot;IndexOutOfBoundsException: &quot; + e1) case e2: IOException =&gt; println(&quot;IOException: &quot; + e2) case _e: Exception =&gt; println(&quot;Exception: &quot; + _e) &#125; &#125; // 对Array进行模式匹配，分别可以匹配带有指定元素的数组、带有指定个数元素的数组、以某元素打头的数组 def matchArray(arr: Array[String]): Unit = &#123; arr match &#123; case Array(&quot;Leo&quot;) =&gt; println(&quot;Hi, Leo!&quot;) case Array(girl1, girl2, girl3) =&gt; println(&quot;Hi, girls, nice to meet you. &quot; + girl1 + &quot; and &quot; + girl2 + &quot; and &quot; + girl3) case Array(&quot;Leo&quot;, _*) =&gt; println(&quot;Hi, Leo, please introduce your friends to me.&quot;) case _ =&gt; println(&quot;hey, who are you?&quot;) &#125; &#125; def matchList(list: List[String]): Unit = &#123; list match &#123; case &quot;Leo&quot; :: Nil =&gt; println(&quot;Hi, Leo!&quot;) case girl1 :: girl2 :: girl3 :: Nil =&gt; println(&quot;Hi, girls, nice to meet you. &quot; + girl1 + &quot; and &quot; + girl2 + &quot; and &quot; + girl3) case &quot;Leo&quot; :: tail =&gt; println(&quot;Hi, Leo, please introduce your friends to me.&quot;) case _ =&gt; println(&quot;hey, who are you?&quot;) &#125; &#125; // Scala有一种特殊的类型，叫做Option。Option有两种值，一种是Some，表示有值，一种是None，表示没有值 // Option通常会用于模式匹配中，用于判断某个变量是有值还是没有值，这比null来的更加简洁明了 val grades = Map((&quot;JJW&quot;, &quot;A&quot;), (&quot;SS&quot;, &quot;A&quot;)) def matchOption(name: String): Unit = &#123; val grade = grades.get(name) grade match &#123; // 有值 case Some(grade) =&gt; println(&quot;你的成绩是：&quot; + grade) case None =&gt; println(&quot;Sorry, your grade information is not in the system&quot;) &#125; &#125; // 斜边和逆变示例 def enterMeetA(card: CardA[Professional]) &#123; println(&quot;welcome to have this meeting!&quot;) &#125; def enterMeetB(card: CardB[Master]) &#123; println(&quot;welcome to have this meeting!&quot;) &#125; /** * 隐式转换函数 * * Scala默认会使用两种隐式转换，一种是源类型，或者目标类型的伴生对象内的隐式转换函数；一种是当前程序作用域内的可以用唯一标识符表示的隐式转换函数 * 如果隐式转换函数不在上述两种情况下的话，那么就必须手动使用import语法引入某个包下的隐式转换函数，比如import test._ */ implicit def object2SpecialPerson(obj: Object): SpecialPerson = &#123; if (obj.getClass == classOf[StudentF]) &#123; val stu = obj.asInstanceOf[StudentF]; new SpecialPerson(stu.name) &#125; else if (obj.getClass == classOf[Older]) &#123; val older = obj.asInstanceOf[Older]; new SpecialPerson(older.name) &#125; else Nil &#125; var ticketNumber = 0 def buySpecialTicket(p: SpecialPerson) = &#123; ticketNumber += 1 &quot;T-&quot; + ticketNumber &#125; /** * 隐式转换-装饰器模式 */ implicit def man2superman(man: Man): Superman = new Superman(man.name) /** * 隐式转换-隐式参数的使用示例 */ def signForExam(name: String) (implicit signPen: SignPen) &#123; signPen.write(name + &quot; come to exam in time.&quot;) &#125;&#125;// 泛型类class StudentD[T](val localId: T) &#123; def getSchoolId(id: T) = &quot;S-&quot; + id + &quot;-&quot; + localId // 泛型函数 def getCard[E](content: E) = &#123; if (content.isInstanceOf[Int]) &#123; &quot;card: 001, &quot; + content &#125; else if (content.isInstanceOf[String]) &#123; &quot;card: this is your card, &quot; + content &#125; else &#123; &quot;card: &quot; + content &#125; &#125;&#125;// 上边界 &lt;: 符号// 在指定泛型类型的时候，有时，我们需要对泛型类型的范围进行界定，而不是可以是任意的类型。class PersonF(val name: String) &#123; def sayHello = println(&quot;Hello, I&apos;m &quot; + name) def makeFriends(p: PersonF) &#123; sayHello p.sayHello &#125;&#125;class StudentE(name: String) extends PersonF(name)class PartyA[T &lt;: PersonF](p1: T, p2: T) &#123; def play = p1.makeFriends(p2)&#125;// 下边界 &gt;: 符号// 下边界，即指定泛型类型必须是某个类的父类class PartyB[T &gt;: StudentE](p1: T, p2: T) &#123; def play = &#123; if (p1.getClass == classOf[PersonF]) println(&quot;PersonF&quot;) else if (p1.getClass == classOf[StudentE]) println(&quot;StudentE&quot;) else println(&quot;error&quot;) &#125;&#125;/** * Scala的协变和逆变是非常有特色的！完全解决了Java中的泛型的一大缺憾！ * * 举例来说，Java中，如果有Professional是Master的子类，那么Card[Professionnal]是不是Card[Master]的子类？答案是：不是。因此对于开发程序造成了很多的麻烦。 */class Masterclass Professional extends Master// 大师以及大师级别以下的名片都可以进入会场class CardA[+T](val name: String)// 只要专家级别的名片就可以进入会场，如果大师级别的过来了，当然可以了！class CardB[-T](val name: String)/** * 隐式转换 * * 基础示例 */class SpecialPerson(val name: String)class StudentF(val name: String)class Older(val name: String)/** * 隐式转换-增强类型（装饰器模式） */class Man(val name: String)class Superman(val name: String) &#123; def emitLaser = println(&quot;emit a laster!&quot;)&#125;/** * 隐式参数 */class SignPen &#123; def write(content: String) = println(content)&#125;/** * Actor * 在scala中，通常建议使用样例类，即case class来作为消息进行发送。然后在actor接收消息之后，可以使用scala强大的模式匹配功能来进行不同消息的处理。 */case class Login(username: String, password: String)case class Register(username: String, password: String)class UserManageActor extends Actor &#123; def act(): Unit = &#123; while (true) &#123; receive &#123; case Login(username, password) =&gt; println(&quot;login, username is &quot; + username + &quot;, password is &quot; + password) case Register(username, password) =&gt; println(&quot;Register, username is &quot; + username + &quot;, password is &quot; + password) &#125; &#125; &#125;&#125;/** * 两个Actor之间要互相收发消息 * 一个actor向另外一个actor发送消息时，同时带上自己的引用；其他actor收到自己的消息时，直接通过发送消息的actor的引用，即可以给它回复消息。 */case class Message(content: String, sender: Actor)class JJWTelephoneActor extends Actor &#123; def act(): Unit = &#123; while (true) &#123; receive &#123; case Message(content, sender) =&gt; &#123; println(&quot;JJW telephone: &quot; + content) sender ! &quot;I&apos;m JJW, please call me after 10 minutes.&quot; &#125; &#125; &#125; &#125;&#125;class SSTelephoneActor(val jjwTelephoneActor: Actor) extends Actor &#123; def act(): Unit = &#123; jjwTelephoneActor ! Message(&quot;Hello JJW, , I&apos;m SS.&quot;, this) receive &#123; case resp: String =&gt; println(&quot;ss telephone: &quot; + resp) &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://jjw-story.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://jjw-story.github.io/tags/Scala/"},{"name":"高阶","slug":"高阶","permalink":"https://jjw-story.github.io/tags/高阶/"}],"author":"JJW"},{"title":"Scala-语法","slug":"Scala-语法","date":"2020-08-06T07:22:56.000Z","updated":"2020-08-06T07:27:53.218Z","comments":true,"path":"2020/08/06/Scala-语法/","link":"","permalink":"https://jjw-story.github.io/2020/08/06/Scala-语法/","excerpt":"","text":"Scala-语法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662package story.jjwimport java.io.IOExceptionimport scala.beans.BeanPropertyimport scala.collection.mutable.ArrayBufferimport scala.util.control.Breaks/** * 除了自己实现main方法之外，还可以继承App Trait，然后将需要在main方法中运行的代码，直接作为object的constructor代码；而且用args可以接受传入的参数 * * App Trait的工作原理为：App Trait继承自DelayedInit Trait，scalac命令进行编译时，会把继承App Trait的object的constructor代码都放到DelayedInit Trait的delayedInit方法中执行 */object HelloWorld extends App &#123; if (args.length &gt; 0) &#123; println(&quot;hello, &quot; + args(0)) &#125; else &#123; println(&quot;Hello World!!!&quot;) &#125;&#125;object One &#123; /** * 就如同java中，如果要运行一个程序，必须编写一个包含main方法类一样；在scala中，如果要运行一个应用程序，那么必须有一个main方法，作为入口 * scala中的main方法定义为def main(args: Array[String])，而且必须定义在object中 */ def main(args: Array[String]): Unit = &#123; // 1. while循环 var n = 10 while (n &gt; 0) &#123; println(n) n = n - 1 &#125; // 2.for循环 val n1 = 10 for (i &lt;- 0 to n1) &#123; println(&quot;n1..&quot; + i) &#125; // 3.for循环遍历字符串 val n2 = &quot;Hello World&quot; for (i &lt;- n2) &#123; println(i) &#125; // 4.跳出循环语句,scala没有提供类似于java的break语句，使用Breaks提供的函数实现 Breaks.breakable &#123; var n = 10 for (i &lt;- &quot;Hello World&quot;) &#123; if (n == 5) &#123; Breaks.break &#125; println(i) n -= 1 &#125; &#125; // 5.多重for循环 for (i &lt;- 1 to 9; j &lt;- 1 to 9) &#123; if (j == 9) &#123; println(i * j) &#125; else &#123; print(i * j + &quot; &quot;) &#125; &#125; // 6.if守卫，取偶数 for (i &lt;- 0 to 9 if i % 2 == 0) &#123; println(i) &#125; // 7.for推导式：构造集合 val vector = for (i &lt;- -1 to 9) yield i println(vector) // 8.函数示例 println(sayHello(&quot;JJW&quot;, 27)) // 9.函数调用默认参数示例 println(sayHello(&quot;SS&quot;)) // 10.斐波那契数列 println(fab(9)) // 11.变长参数 println(sum(1, 2, 3, 4, 5)) // 12.使用序列调用变长参数 println(sum(1 to 5: _*)) // 13.lazy值，如果将一个变量声明为lazy，则只有在第一次使用该变量时，变量对应的表达式才会发生计算 val valueA = &#123; println(&quot;开始初始化valueA&quot;); 1 + 1 &#125; lazy val valueB = &#123; println(&quot;开始初始化valueB&quot;); 1 + 1 &#125; println(valueA) println(valueB) // 14.异常捕获 exceptionProcess(new IOException(&quot;IO 异常&quot;)) // 15.Array 和 ArrayBuffer val arrA = new Array[Int](10) arrA(0) = 10 println(arrA(0)) val arrB = Array(&quot;hello&quot;, &quot;world&quot;) println(arrB(1)) val ab = ArrayBuffer[Int]() ab += 10 ab += (11, 12) ab ++= Array(13, 14) println(ab) ab.trimEnd(3) // 从头截取指定个元素 println(ab) // 指定索引位置插入元素 ab.insert(0, 15) ab.insertAll(2, Array(7, 8, 9)) println(ab) // 删除指定位置元素 ab.remove(1) ab.remove(1, 3) // Array 与 ArrayBuffer 相互转换 println(arrB.toBuffer) println(ab.toArray) // 普通遍历 for (i &lt;- 0 until arrA.length) &#123; println(arrA(i)) &#125; // 跳跃遍历 for (i &lt;- 0 until(ab.length, 2)) &#123; println(ab(i)) &#125; // 增强for for (e &lt;- ab) &#123; println(e) &#125; // 数组一些api println(ab.sum) println(ab.max) scala.util.Sorting.quickSort(arrA) println(ab.mkString(&quot;, &quot;)) println(ab.mkString(&quot;(&quot;, &quot;, &quot;, &quot;)&quot;)) // 使用yield和函数式编程转换数组 // val arrC = Array(1, 2, 3, 4, 5) val arrC = ArrayBuffer(1, 2, 3, 4, 5) val arrD = for (ele &lt;- arrC) yield &#123; ele * 3 &#125; println(arrD) val arrE = arrC.filter(_ % 2 == 1).map(_ * 3) val arrF = arrC.filter &#123; _ % 2 == 0 &#125;.map &#123; _ * 2 &#125; println(&quot;arrE &quot; + arrE) println(&quot;arrF &quot; + arrF) // 16.Map和Tuple // 创建不可变Map val mapA = Map(&quot;JJW&quot; -&gt; 20, &quot;SS&quot; -&gt; 20) // mapA(&quot;JJW&quot;) = 18 map不可变，报错 val mapB = Map((&quot;Leo&quot;, 30), (&quot;Jen&quot;, 25), (&quot;Jack&quot;, 23)) // 创建可变Map val mapC = scala.collection.mutable.Map(&quot;Leo&quot; -&gt; 30, &quot;Jen&quot; -&gt; 25, &quot;Jack&quot; -&gt; 23) mapC(&quot;JJW&quot;) = 18 println(mapC) // Map取值 val age = mapC(&quot;JJW&quot;) val leoAge = if (mapC.contains(&quot;leo&quot;)) mapC(&quot;leo&quot;) else 0 val jenAge = mapC.getOrElse(&quot;Jen&quot;, 0) // 上述简化 // 更新元素 mapC(&quot;JJW&quot;) = 20 // 增加多个元素, 不可变Map元素不可变，不可修改和添加元素，但是可以通过操作得到新的Map mapC += (&quot;Mike&quot; -&gt; 35, &quot;Tom&quot; -&gt; 40) // 移除元素 mapC -= &quot;Mike&quot; // 更新不可变的map val mapD = mapB + (&quot;Mike&quot; -&gt; 36, &quot;Tom&quot; -&gt; 40) // 移除不可变map的元素 val mapE = mapB - &quot;Jack&quot; println(mapC) println(mapB) println(mapD) println(mapE) // 创建空的HashMap val hashMap = new scala.collection.mutable.HashMap[String, String] // 遍历Map // 普通遍历 for ((key, value) &lt;- mapD) &#123; println(key + &quot; &quot; + value) &#125; // 遍历map的key for (key &lt;- mapD.keySet) &#123; println(key) &#125; // 遍历map的value for (value &lt;- mapD.values) &#123; println(value) &#125; // 生成新map，反转key和value val mapF = for ((key, value) &lt;- mapD) yield &#123; (value, key) &#125; println(mapF) // SortedMap 和 LinkedHashMap // SortedMap可以自动对Map的key的排序 val sortedMap = scala.collection.immutable.SortedMap(20 -&gt; &quot;JJW&quot;, 10 -&gt; &quot;SS&quot;, 30 -&gt; &quot;DD&quot;) println(sortedMap) // LinkedHashMap是有序的，根据插入数据的顺序 val linkedHashMap = new scala.collection.mutable.LinkedHashMap[String, Int] linkedHashMap(&quot;leo&quot;) = 30 linkedHashMap(&quot;alice&quot;) = 15 linkedHashMap(&quot;jen&quot;) = 25 println(linkedHashMap) // Tuple // 创建 val tuple2 = (&quot;JJW&quot;, 20) // 访问Tuple tuple2._1 tuple2._2 // zip操作 val names = Array(&quot;leo&quot;, &quot;jack&quot;, &quot;mike&quot;) val ages = Array(30, 24, 26) val nameAges = names.zip(ages) for ((name, age) &lt;- nameAges) &#123; println(name + &quot;: &quot; + age) &#125; // 17.类 // 创建类的对象，并调用其方法 val helloWorld = new HelloWorld helloWorld.sayHello() // 也可以不加括号，如果定义方法时不带括号，则调用方法时也不能带括号 print(helloWorld.getName) // 使用scala自己生成的get set方法 helloWorld.age = 27 println(helloWorld.age) // val修饰的，只生成get方法 println(helloWorld.gender) // 自定义get set方法 helloWorld.myHobby = &quot;玩&quot; println(helloWorld.myHobby) // java式的 get set helloWorld.stature = &quot;178&quot; println(helloWorld.stature) helloWorld.setStature(&quot;180&quot;) println(helloWorld.stature) // Java式的构造函数 val s1 = new Student(&quot;JJW&quot;) val s2 = new Student(&quot;JJW&quot;, 20) // 主constructor创建对象 val t = new Teacher(&quot;JJW&quot;, &quot;学习&quot;, 27) // object定义的类会在第一次使用的时候初始化构造函数，下载再使用的时候构造函数中的内容不会执行 // println(Person.getEyeNum) println(&quot;++++++++++&quot;) val p = Person new Person(&quot;JJW&quot;, 20).sayHello // 使用object class 类的 apply构造伴生对象（此方法很重要，多处都是这样用的） Person(&quot;SS&quot;, 18).sayHello // 抽象类实例化 val pc: PersonC = new StudentC(&quot;JJW&quot;) pc.sayHello // 18.枚举 println(Season(0)) println(Season.withName(&quot;summer&quot;)) for (ele &lt;- Season.values) println(ele) // 19.类型判断 // isInstanceOf和asInstanceOf // 需要使用isInstanceOf判断对象是否是指定类的对象，如果是的话，则可以使用asInstanceOf将对象转换为指定类型 val pa: PersonA = new StudentA var sa: StudentA = null // 判断类型 if (pa.isInstanceOf[StudentA]) &#123; // 转换类型 sa = pa.asInstanceOf[StudentA] &#125; println(sa) // getClass和classOf // isInstanceOf只能判断出对象是否是指定类以及其子类的对象，而不能精确判断出，对象就是指定类的对象 // 如果要求精确地判断对象就是指定类的对象，那么就只能使用getClass和classOf了 println(pa.getClass == classOf[PersonA]) // false println(pa.getClass == classOf[StudentA]) // true // 模式匹配判断，isInstanceOf 式的判断，并不是精确判断，既如果是子类依然会匹配成功 pa match &#123; case per: PersonA =&gt; println(&quot;it&apos;s PersonA&apos;s object&quot;) case _ =&gt; println(&quot;unknown type&quot;) &#125; val pd = new PersonD(&quot;PZ&quot;, &quot;好好学习&quot;) pd.eyeNum = 0 pd.sayHello(&quot;BQ&quot;) // 创建类的对象时，指定该对象混入某个trait val pdd = new PersonD(&quot;PZ&quot;, &quot;好好学习&quot;) with MyLogger pdd.myLog(&quot;为实例混入trait&quot;) // trait一个接口，多个实现，一个类又继承了此多个实现，然后我们调用接口的一个方法，这样会从右到左依次执行每个实现中的这个方法 // 重点：责任链模式 val pe = new PersonE(&quot;JJW&quot;) pe.sayHello &#125; def sayHello(name: String, age: Int = 20) = &#123; if (age &gt; 18) &#123; printf(&quot;你好， %s， 你成年了： %s \\n&quot;, name, age) &#125; else &#123; printf(&quot;你好， %s， 你未成年： %s \\n&quot;, name, age) &#125; age &#125; def fab(n: Int): Int = &#123; if (n &lt;= 1) &#123; 1 &#125; else &#123; fab(n - 1) + fab(n - 2) &#125; &#125; def sum(nums: Int*): Int = &#123; var res = 0 for (num &lt;- nums) &#123; res += num &#125; res &#125; def sum2(nums: Int*): Int = &#123; if (nums.length == 0) &#123; 0 &#125; else &#123; nums.head + sum2(nums.tail: _*) &#125; &#125; def exceptionProcess(err: Exception): Unit = &#123; try &#123; throw err &#125; catch &#123; case e1: IllegalArgumentException =&gt; println(&quot;illegal argument&quot;) case e2: IOException =&gt; println(&quot;io exception&quot;) &#125; &#125;&#125;/** * 定义类 */class HelloWorld &#123; // 如果不希望生成setter和getter方法，则将field声明为private[this] // 定义不带private的var field，JVM会提供public的getter和setter方法 var age = 20; // 而如果使用private修饰field，则生成的getter和setter也是private的 private var name = &quot;leo&quot; // 如果定义val field，则只会生成getter方法 val gender = &quot;男&quot; // 由于field是private的，所以setter和getter都是private，对外界没有暴露；自己可以实现修改field值的方法；自己可以覆盖getter方法 private var hobby = &quot;学习&quot; // Scala的getter和setter方法的命名与java是不同的，是field和field_=的方式 // 如果要让scala自动生成java风格的getter和setter方法，只要给field添加@BeanProperty注解即可 // 此时会生成4个方法，stature: String、stature_=(newValue: String): Unit、getStature(): String、setStature(newValue: String): Unit @BeanProperty var stature: String = _ def myHobby = hobby def myHobby_=(newValue: String) &#123; print(&quot;自定义set方法&quot;) hobby = newValue &#125; def sayHello() &#123; println(&quot;Hello, &quot; + name) &#125; def getName = name&#125;class Student &#123; private var name = &quot;&quot; private var age = 0 def this(name: String) &#123; this() this.name = name &#125; def this(name: String, age: Int) &#123; this(name) this.age = age &#125;&#125;// 主constructor中还可以通过使用默认参数，来给参数默认的值// 如果主constrcutor传入的参数什么修饰都没有，比如name: String，那么如果类内部的方法使用到了，则会声明为private[this] name；否则没有该field，就只能被constructor代码使用而已class Teacher(val name: String, gender: String, val age: Int = 30) &#123; println(&quot;your name is &quot; + name + &quot;, your age is &quot; + age + &quot;, gender is &quot; + gender)&#125;/** * 内部类示例 */class Class &#123; class Student(val name: String) val students = new ArrayBuffer[Student] def getStudent(name: String) = &#123; new Student(name) &#125;&#125;/** * object，相当于class的单个实例，通常在里面放一些静态的field或者method * 第一次调用object的方法时，就会执行object的constructor，也就是object内部不在method中的代码；但是object不能定义接受参数的constructor * 注意，object的constructor只会在其第一次被调用时执行一次，以后再次调用就不会再次执行constructor了 * object通常用于作为单例模式的实现，或者放class的静态成员，比如工具方法 */object Person &#123; private var eyeNum = 2 /** * 通常在伴生对象中实现apply方法，并在其中实现构造伴生类的对象的功能 * 而创建伴生类的对象时，通常不会使用new Class的方式，而是使用Class()的方式，隐式地调用伴生对象得apply方法，这样会让对象创建更加简洁 */ def apply(name: String, age: Int) = new Person(name, age) println(&quot;this Person object!&quot;) // def getEyeNum = eyeNum&#125;/** * 伴生对象 * * 如果有一个class，还有一个与class同名的object，那么就称这个object是class的伴生对象，class是object的伴生类 * 伴生类和伴生对象必须存放在一个.scala文件之中 * /伴生类和伴生对象，最大的特点就在于，互相可以访问private field */class Person(val name: String, val age: Int) &#123; def sayHello = println(&quot;Hi, &quot; + name + &quot;, I guess you are &quot; + age + &quot; years old!&quot; + &quot;, and usually you must have &quot; + Person.eyeNum + &quot; eyes.&quot;)&#125;/** * object继承类示例 * * @param message */abstract class Hello(var message: String) &#123; def sayHello(name: String): Unit&#125;object HelloImpl extends Hello(&quot;hello&quot;) &#123; override def sayHello(name: String) = &#123; println(message + &quot;, &quot; + name) &#125;&#125;/** * scala枚举 */object Season extends Enumeration &#123; val SPRING = Value(0, &quot;spring&quot;) val SUMMER = Value(1, &quot;summer&quot;) val AUTUMN = Value(2, &quot;autumn&quot;) val WINTER = Value(3, &quot;winter&quot;)&#125;/** * Scala中，让子类继承父类，与Java一样，也是使用extends关键字 * 继承就代表，子类可以从父类继承父类的field和method；然后子类可以在自己内部放入父类所没有，子类特有的field和method；使用继承可以有效复用代码 * 子类可以覆盖父类的field和method；但是如果父类用final修饰，field和method用final修饰，则该类是无法被继承的，field和method是无法被覆盖的 */class PersonA &#123; private var name = &quot;leo&quot; val age: Int = 20 def getName = name&#125;class StudentA extends PersonA &#123; private var score = &quot;A&quot; def getScore = score // 重写父类 filed override val age: Int = 27 // 重写父类getName方法 override def getName = &quot;Hi, I&apos;m &quot; + super.getName&#125;/** * 通过子类的主构造函数来调用父类的构造函数(与 Kotlin 一样) * 注意！如果是父类中接收的参数，比如name和age，子类中接收时，就不要用任何val或var来修饰了，否则会认为是子类要覆盖父类的field */class PersonB(val name: String, val age: Int)class StudentB(name: String, age: Int, var score: Double) extends PersonB(name, age) &#123; def this(name: String) &#123; this(name, 0, 0) &#125; def this(age: Int) &#123; this(&quot;leo&quot;, age, 0) &#125;&#125;/** * 子类必须覆盖field，以定义自己的具体field，并且覆盖抽象field，不需要使用override关键字 * * 而一个类中如果有一个抽象方法，那么类就必须用abstract来声明为抽象类，此时抽象类是不可以实例化的，在子类中覆盖抽象类的抽象方法时，不需要使用override关键字 * * 与 Kotlin 一样 */abstract class PersonC(val name: String) &#123; val age: Int def sayHello: Unit&#125;class StudentC(name: String) extends PersonC(name) &#123; val age: Int = 20 def sayHello: Unit = println(&quot;Hello, &quot; + name)&#125;/** * 在Scala中，trait是没有接收参数的构造函数的，这是trait与class的唯一区别 * * Scala中的Triat是一种特殊的概念 * 首先我们可以将Trait作为接口来使用，此时的Triat就与Java中的接口非常类似,在triat中可以定义抽象方法，就与抽象类中的抽象方法一样，只要不给出方法的具体实现即可 * 类可以使用extends关键字继承trait，注意，这里不是implement,在scala中没有implement的概念，无论继承类还是trait，统一都是extends * 类继承trait后，必须实现其中的抽象方法，实现时不需要使用override关键字 * scala不支持对类进行多继承，但是支持多重继承trait，使用with关键字即可 * * Scala中的Triat可以不是只定义抽象方法，还可以定义具体方法 * * Scala中的Triat可以定义具体field，此时继承trait的类就自动获得了trait中定义的field * 但是这种获取field的方式与继承class是不同的：如果是继承class获取的field，实际是定义在父类中的；而继承trait获取的field，就直接被添加到了类中 * * Scala中的Triat可以定义抽象field，而trait中的具体方法则可以基于抽象field来编写 * 但是继承trait的类，则必须覆盖抽象field，提供具体的值 * * 有时我们可以在创建类的对象时，指定该对象混入某个trait，这样，就只有这个对象混入该trait的方法，而类的其他对象则没有 * * 在Scala中，trait也是有构造代码的，也就是trait中的，不包含在任何方法中的代码 * 而继承了trait的类的构造机制如下：1、父类的构造函数执行；2、trait的构造代码执行，多个trait从左到右依次执行；3、构造trait时会先构造父trait，如果多个trait继承同一个父trait，则父trait只会构造一次；4、所有trait构造完毕之后，子类的构造函数执行 * * 重点：在Scala中，trait也可以继承自class，此时这个class就会成为所有继承该trait的类的父类（trait不能直接实例化，跟接口一样） */trait Hellotrait &#123; // 定义具体field var eyeNum: Int = 2 // 定义抽象字段 val msg: String // 定义抽象方法 def sayHello(name: String) // 定义具体方法（类似于Java8 中接口有默认实现一样） def log(message: String) = println(message)&#125;trait MakeFriendsTrait &#123; def makeFriends(name: String)&#125;trait MyLogger&#123; def myLog(msg: String) &#123; println(&quot;log: &quot; + msg) &#125;&#125;class PersonD(val name: String, override val msg: String) extends Hellotrait with MakeFriendsTrait with Serializable &#123; override def sayHello(name: String): Unit = &#123; println(&quot;Hello &quot; + name + &quot; I have &quot; + eyeNum + &quot; eyes; &quot; + msg) &#125; override def makeFriends(name: String): Unit = &#123; println(&quot;Hello friend &quot; + name) log(&quot;log&quot; + name) &#125;&#125;/** * Scala中支持让类继承多个trait后，依次调用多个trait中的同一个方法，只要让多个trait的同一个方法中，在最后都执行super.方法即可 * 类中调用多个trait中都有的这个方法时，首先会从最右边的trait的方法开始执行，然后依次往左执行，形成一个调用链条 * 这种特性非常强大，其实就相当于设计模式中的责任链模式的一种具体实现依赖 * * 注意：要实现责任链这种模式，每种实现中必须要调用 super.(抽象函数) */trait Handler &#123; def handle(data: String) &#123;&#125;&#125;trait DataValidHandler extends Handler &#123; override def handle(data: String) &#123; println(&quot;check data: &quot; + data) super.handle(data) &#125;&#125;trait SignatureValidHandler extends Handler &#123; override def handle(data: String) &#123; println(&quot;check signature: &quot; + data) super.handle(data) &#125;&#125;class PersonE(val name: String) extends SignatureValidHandler with DataValidHandler &#123; def sayHello = &#123; println(&quot;Hello, &quot; + name); handle(name) &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://jjw-story.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://jjw-story.github.io/tags/Scala/"},{"name":"语法","slug":"语法","permalink":"https://jjw-story.github.io/tags/语法/"}],"author":"JJW"},{"title":"MySQL-实践二","slug":"MySQL-实践二","date":"2020-07-31T08:17:46.000Z","updated":"2020-08-27T09:13:56.087Z","comments":true,"path":"2020/07/31/MySQL-实践二/","link":"","permalink":"https://jjw-story.github.io/2020/07/31/MySQL-实践二/","excerpt":"","text":"binlog 和 redo log详解binlog 的写入机制binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。 系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。每个线程有自己 binlog cache，但是共用同一份 binlog 文件。 binlog cache写入binlog也要经历两步： write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。 write 和 fsync 的时机，是由参数 sync_binlog 控制的： sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。 redo log 的写入机制事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。redo log 可能存在的三种状态： 存在 redo log buffer 中，物理上是在 MySQL 进程内存中； 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面； 持久化到磁盘，对应的是 hard disk。 日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。 为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值： 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。 InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。 注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。 还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中: 一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。 通常我们说 MySQL 的双1配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。 日志逻辑序列号（log sequence number，LSN）LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。 组提交例如我们有三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160，等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160；trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；这时候 trx2 和 trx3 就可以直接返回了。 所以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。 为了让一次 fsync 带的组员更多，MySQL 有一个很有趣的优化：拖时间，具体流程就是： 1.redo log perpare write -&gt; 2.binlog write -&gt; 3.redo log perpare fsync -&gt; 4.binlog fsync -&gt; 5.redo log commit write; 注意-重点：上述流程都是在redo log 和 binlog 已经写入对应的 cache 中，并且在事务的提交阶段发生的流程，redo log perpare也是在提交阶段。 这么一来，binlog 也可以组提交了。在执行第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。通常情况下第 3 步执行得会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。 如果你想提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来实现。 binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync; binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。 这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。所以，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。 MySQL IO 性能瓶颈提升方法 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志。 将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据。 不建议你把 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。 总结为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？回答：MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。而 redo log 并没有这个要求，中间有生成的日志可以写到 redo log buffer 中。redo log buffer 中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中。 事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？回答：不会。因为这时候 binlog 也还在 binlog cache 里，没发给备库。crash 以后 redo log 和 binlog 都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。（对应上述组提交流程） 什么情况下会使用双非1配置？ 业务高峰期。一般如果有预知的高峰期，DBA 会有预案，把主库设置成“非双 1”。 备库延迟，为了让备库尽快赶上主库。 批量导入数据的时候。 MySQL主备一致原理主备原理M-S结构假设MySQL集群有两个节点，节点A和节点B。客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。需要切换的时候，我们就将客户端读写访问的节点改为 B，而节点 A 是 B 的备库。 这就是主库和备库的使用原理。 一般情况下：我们都将备库设置为只读readonly）模式，原因有如下几点： 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作； 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致； 可以用 readonly 状态，来判断节点的角色。 备库设置成只读，还怎么跟主库保持同步更新呢？readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。 双M结构双 M 结构和 M-S 结构，其实区别只是多了一条线，即：节点 A 和 B 之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。（区别就是没有备库，不需要将其中一个节点设置为 readonly模式）。 但是，双 M 结构还有一个问题需要解决，业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。那么，如果节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了。这个要怎么解决呢？这里归结为循环复制的问题，答案在下面小节解释。 主备数据同步的内部流程主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog。 备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。 注意：由于多线程复制方案的引入，sql_thread 演化成为了多个线程 binlog的三种格式statement当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文。 你可以用 show binlog events in ‘master.000001’; 命令看 binlog 中的内容。 statement 格式下，记录到 binlog 里的是语句原文，这样写是有风险的。因为我们执行一条语句，在主库和备库上执行结果又可能是不一样的（例如我们之前学的，MySQL为什么会选错索引，删除的时候用 limit = 1 限制，就会删除的数据不一样），这样就会出现问题。 rowrow格式的binlog记录了非常详细的内容，有server id：事务是在 server_id=xxx 的这个库上执行的。Table_map event：用于说明要操作的表。以及我们插入更新删除数据的所有字段信息，字段值信息，新旧数据信息等，非常全面。 这样就保证了，我们每操作一个数据，读不会出现操作错误的问题，因为我们有数据的所有信息。 mixed为什么会有 mixed 这种 binlog 格式的存在场景？ 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。 所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。 也就是说，mixed 格式可以利用 statment 格式的优点，同时又避免了数据不一致的风险。 现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。原因如下： 比如我们现在执行了一个删除语句，因为row 格式的 binlog 也会把被删掉的行的整行信息保存起来。所以，如果你在执行完一条 delete 语句以后，发现删错数据了，可以直接把 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去就可以恢复了。 如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了 update 语句的话，只需要把这个 event 前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了。 上述循环复制问题解决MySQL 在 binlog 中记录了这个命令第一次执行时所在实例的 server id。因此，我们可以用下面的逻辑，来解决两个节点间的循环复制的问题： 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系； 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog； 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。 按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样： 从节点 A 更新的事务，binlog 里面记的都是 A 的 server id； 传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id； 再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。 MySQL是如何保证高可用的主备延迟主备切换可能是一个主动运维动作，比如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电。 先说明一个概念，即“同步延迟”。与数据同步有关的时间点主要包括以下三个： 主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1; 之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2; 备库 B 执行完成这个事务，我们把这个时刻记为 T3。 所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1。 可以在备库上执行 show slave status 命令，它的返回结果里面会显示 seconds_behind_master，用于表示当前备库延迟了多少秒。 seconds_behind_master 的计算方法是这样的： 每个事务的 binlog 里面都有一个时间字段，用于记录主库上写入的时间； 备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，得到 seconds_behind_master。 可以看到，其实 seconds_behind_master 这个参数计算的就是 T3-T1。所以，我们可以用 seconds_behind_master 来作为主备延迟的值，这个值的时间精度是秒。在网络正常的时候，日志从主库传给备库所需的时间是很短的。 所以，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。 主备延迟的来源 备库所在机器的性能要比主库所在的机器性能差。 备库的压力大（由于主库直接影响业务，大家使用起来会比较克制，反而忽视了备库的压力控制。结果就是，备库上的查询耗费了大量的 CPU 资源，影响了同步速度，造成主备延迟。这种情况我们可以使用一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力）。 大事务。因为主库上必须等事务执行完成才会写入 binlog，再传给备库。所以，如果一个主库上的语句执行 10 分钟，那这个事务很可能就会导致从库延迟 10 分钟。（一次性地用 delete 语句删除太多数据，这就是一个大事务典型场景。另一种典型的大事务场景，就是大表 DDL） 备库的并行复制能力（下一章节的重点内容）。 可靠性优先策略在双M的架构下，我们可以这样保证数据的可靠性： 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步； 把主库 A 改成只读状态，即把 readonly 设置为 true； 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止； 把备库 B 改成可读写状态，也就是把 readonly 设置为 false； 把业务请求切到备库 B。 这个切换流程中是有不可用时间的。因为在步骤 2 之后，主库 A 和备库 B 都处于 readonly 状态，也就是说这时系统处于不可写状态，直到步骤 5 完成后才能恢复。试想如果一开始主备延迟就长达 30 分钟，而不先做判断直接切换的话，系统的不可用时间就会长达 30 分钟，这种情况一般业务都是不可接受的。 可用性优先策略如果强行把上面可靠性优先步骤 4、5 调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库 B 可以读写，那么系统几乎就没有不可用时间了。我们把这个切换流程，暂时称作可用性优先流程。这个切换流程的代价，就是可能出现数据不一致的情况。 大多数情况下，都建议使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。 备库并行复制能力为什么要有多线程复制呢？这是因为单线程复制的能力全面低于多线程复制，对于更新压力较大的主库，备库是可能一直追不上主库的。从现象上看就是，备库上 seconds_behind_master 的值越来越大。 日志在备库上的执行，就是主备数据同步流程中备库上 sql_thread 更新数据 (DATA) 的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟。在官方的 5.6 版本之前，MySQL 只支持单线程复制，由此在主库并发高、TPS 高时就会出现严重的主备延迟问题。 MariaDB 的并行复制策略MariaDB 的并行复制策略利用的是 redo log 组提交 (group commit) 优化 的特性： 能够在同一组里提交的事务，一定不会修改同一行；（重点理解） 主库上可以并行执行的事务，备库上也一定是可以并行执行的。 在实现上，MariaDB 是这么做的： 在一组里面一起提交的事务，有一个相同的 commit_id，下一组就是 commit_id+1； commit_id 直接写到 binlog 里面； 传到备库应用的时候，相同 commit_id 的事务分发到多个 worker 执行； 这一组全部执行完成后，coordinator 再去取下一批。 但是，这个策略有一个问题，它并没有实现“真正的模拟主库并发度”这个目标。在主库上，一组事务在 commit 的时候，下一组事务是同时处于“执行中”状态的。在备库上执行的时候，要等第一组事务完全执行完成后，第二组事务才能开始执行，这样系统的吞吐量就不够。另外，这个方案很容易被大事务拖后腿，例如有三个事务（trx1， trx2， trx3），这一组事务需要全部执行都完成后，coordinator 再去取下一批，假设 trx2 是一个超大事务，那么在备库应用的时候，trx1 和 trx3 执行完成后，就只能等 trx2 完全执行完成，下一组才能开始执行。这段时间，只有一个 worker 线程在工作，是对资源的浪费。 MySQL 5.7 的并行复制策略在 MariaDB 并行复制实现之后，官方的 MySQL5.7 版本也提供了类似的功能，由参数 slave-parallel-type 来控制并行复制策略： 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库并行策略； 配置为 LOGICAL_CLOCK，表示的就是类似 MariaDB 的策略。不过，MySQL 5.7 这个策略，针对并行度做了优化。这个优化的思路也很有趣儿。 同时处于“执行状态”的所有事务，是不是可以并行？答案是，不能。因为，这里面可能有由于锁冲突而处于锁等待状态的事务。如果这些事务在备库上被分配到不同的 worker，就会出现备库跟主库不一致的情况。而上面提到的 MariaDB 这个策略的核心，是“所有处于 commit”状态的事务可以并行。事务处于 commit 状态，表示已经通过了锁冲突的检验了。 其实，不用等到 commit 阶段，只要能够到达 redo log prepare 阶段，就表示事务已经通过锁冲突的检验了。因此，MySQL 5.7 并行复制策略的思想是： 同时处于 prepare 状态的事务，在备库执行时是可以并行的； 处于 prepare 状态的事务，与处于 commit 状态的事务之间，在备库执行时也是可以并行的。 MySQL 5.7.22 的并行复制策略在 2018 年 4 月份发布的 MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制。相应地，新增了一个参数 binlog-transaction-dependency-tracking，用来控制是否启用这个新策略。这个参数的可选值有以下三种： COMMIT_ORDER，表示的就是前面介绍的，根据同时进入 prepare 和 commit 来判断是否可以并行的策略。 WRITESET，表示的是对于事务涉及更新的每一行，计算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。（为了唯一标识，这个 hash 值是通过“库名 + 表名 + 索引名 + 值”计算出来的。如果一个表上除了有主键索引外，还有其他唯一索引，那么对于每个唯一索引，insert 语句对应的 writeset 就要多增加一个 hash 值。） WRITESET_SESSION，是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。 MySQL 官方的这个实现还是有很大的优势： writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容（event 里的行数据），节省了很多计算量； 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个 worker，更省内存； 由于备库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也是可以的。 主备切换今天我们要讨论的就是，在一主多从架构下，主库故障后的主备切换问题。 假设我们有一个MySQL集群，主备关系： A 和 A’互为主备， 从库 B、C、D 指向的是主库 A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。 基于位点的主备切换当我们把节点 B 设置成节点 A’的从库的时候，需要执行一条 change master 命令： 1234567CHANGE MASTER TOMASTER_HOST=$host_nameMASTER_PORT=$portMASTER_USER=$user_nameMASTER_PASSWORD=$passwordMASTER_LOG_FILE=$master_log_nameMASTER_LOG_POS=$master_log_pos 这条命令有这么 6 个参数：MASTER_HOST、MASTER_PORT、MASTER_USER 和 MASTER_PASSWORD 四个参数，分别代表了主库 A’的 IP、端口、用户名和密码。最后两个参数 MASTER_LOG_FILE 和 MASTER_LOG_POS 表示，要从主库的 master_log_name 文件的 master_log_pos 这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。 节点 B 要设置成 A’的从库，就要执行 change master 命令，就不可避免地要设置位点的这两个参数，但是这两个参数到底应该怎么设置呢？原来节点 B 是 A 的从库，本地记录的也是 A 的位点。但是相同的日志，A 的位点和 A’的位点是不同的。因此，从库 B 要切换的时候，就需要先经过“找同步位点”这个逻辑。这个位点很难精确取到，只能取一个大概位置。 考虑到切换过程中不能丢数据，所以我们找位点的时候，总是要找一个“稍微往前”的，然后再通过判断跳过那些在从库 B 上已经执行过的事务。例如我们找到的位点，但是我们需要稍微往前一点，正好这里面包了了A’库和B库都执行过了的一条插入语句，这样B库在执行的时候，就会把插入 R 这一行数据的 binlog 又同步到从库 B 去执行。这时候，从库 B 的同步线程就会报告 Duplicate entry ‘id_of_R’ for key ‘PRIMARY’ 错误，提示出现了主键冲突，然后停止同步。 通常情况下，我们在切换任务的时候，要先主动跳过这些错误，有两种常用的方法： 主动跳过一个事务。跳过命令的写法是： 12set global sql_slave_skip_counter=1;start slave; 因为切换过程中，可能会不止重复执行一个事务，所以我们需要在从库 B 刚开始接到新主库 A’时，持续观察，每次碰到这些错误就停下来，执行一次跳过命令，直到不再出现停下来的情况，以此来跳过可能涉及的所有事务. 通过设置 slave_skip_errors 参数，直接设置跳过指定的错误: 121062 错误是插入数据时唯一键冲突；1032 错误是删除数据时找不到行 这里需要注意的是，这种直接跳过指定错误的方法，针对的是主备切换时，由于找不到精确的同步位点，所以只能采用这种方法来创建从库和新主库的主备关系。等到主备间的同步关系建立完成，并稳定执行一段时间之后，我们还需要把这个参数设置为空，以免之后真的出现了主从数据不一致，也跳过了。 GTID通过 sql_slave_skip_counter 跳过事务和通过 slave_skip_errors 忽略错误的方法，虽然都最终可以建立从库 B 和新主库 A’的主备关系，但这两种操作都很复杂，而且容易出错。所以，MySQL 5.6 版本引入了 GTID，彻底解决了这个困难。 GTID 的全称是 Global Transaction Identifier，也就是全局事务 ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。它由两部分组成，格式是：GTID=server_uuid:gno server_uuid 是一个实例第一次启动时自动生成的，是一个全局唯一的值； gno 是一个整数，初始值是 1，每次提交事务的时候分配给这个事务，并加 1。 GTID 模式的启动也很简单，我们只需要在启动一个 MySQL 实例的时候，加上参数 gtid_mode=on 和 enforce_gtid_consistency=on 就可以了。 在 GTID 模式下，每个事务都会跟一个 GTID 一一对应。这个 GTID 有两种生成方式，而使用哪种方式取决于 session 变量 gtid_next 的值（重点）。 如果 gtid_next=automatic，代表使用默认值。这时，MySQL 就会把 server_uuid:gno 分配给这个事务。a. 记录 binlog 的时候，先记录一行 SET @@SESSION.GTID_NEXT=‘server_uuid:gno’; b. 把这个 GTID 加入本实例的 GTID 集合。 如果 gtid_next 是一个指定的 GTID 的值，比如通过 set gtid_next=’current_gtid’指定为 current_gtid，那么就有两种可能：a. 如果 current_gtid 已经存在于实例的 GTID 集合中，接下来执行的这个事务会直接被系统忽略；b. 如果 current_gtid 没有存在于实例的 GTID 集合中，就将这个 current_gtid 分配给接下来要执行的事务，也就是说系统不需要给这个事务生成新的 GTID，因此 gno 也不用加 1。 这样，每个 MySQL 实例都维护了一个 GTID 集合，用来对应“这个实例执行过的所有事务”。 基于 GTID 的主备切换在 GTID 模式下，备库 B 要设置为新主库 A’的从库的语法如下： 123456CHANGE MASTER TOMASTER_HOST=$host_nameMASTER_PORT=$portMASTER_USER=$user_nameMASTER_PASSWORD=$passwordmaster_auto_position=1 其中，master_auto_position=1 就表示这个主备关系使用的是 GTID 协议。可以看到，前面让我们头疼不已的 MASTER_LOG_FILE 和 MASTER_LOG_POS 参数，已经不需要指定了。 我们把现在这个时刻，实例 A’的 GTID 集合记为 set_a，实例 B 的 GTID 集合记为 set_b。接下来，我们就看看现在的主备切换逻辑。我们在实例 B 上执行 start slave 命令，取 binlog 的逻辑是这样的： 实例 B 指定主库 A’，基于主备协议建立连接。 实例 B 把 set_b 发给主库 A’。 实例 A’算出 set_a 与 set_b 的差集，也就是所有存在于 set_a，但是不存在于 set_b 的 GTID 的集合，判断 A’本地是否包含了这个差集需要的所有 binlog 事务。a. 如果不包含，表示 A’已经把实例 B 需要的 binlog 给删掉了，直接返回错误；b. 如果确认全部包含，A’从自己的 binlog 文件里面，找出第一个不在 set_b 的事务，发给 B； 之后就从这个事务开始，往后读文件，按顺序取 binlog 发给 B 去执行。 由于不需要找位点了，所以从库 B、C、D 只需要分别执行 change master 命令指向实例 A’即可。其实，严谨地说，主备切换不是不需要找位点了，而是找位点这个工作，在实例 A’内部就已经自动完成了。但由于这个工作是自动的，所以对 HA 系统的开发人员来说，非常友好。 GTID 和在线 DDL业务高峰期的慢查询性能问题时，分析到如果是由于索引缺失引起的性能问题，我们可以通过在线加索引来解决。但是，考虑到要避免新增索引对主库性能造成的影响，我们可以先在备库加索引，然后再切换。 假设，这两个互为主备关系的库还是实例 X 和实例 Y，且当前主库是 X，并且都打开了 GTID 模式。这时的主备切换流程可以变成下面这样： 在实例 X 上执行 stop slave。 在实例 Y 上执行 DDL 语句。注意，这里并不需要关闭 binlog。 执行完成后，查出这个 DDL 语句对应的 GTID，并记为 server_uuid_of_Y:gno。 到实例 X 上执行以下语句序列： 12345set GTID_NEXT=&quot;server_uuid_of_Y:gno&quot;;begin;commit;set gtid_next=automatic;start slave; 这样做的目的在于，既可以让实例 Y 的更新有 binlog 记录，同时也可以确保不会在实例 X 上执行这条更新。 接下来，执行完主备切换，然后照着上述流程再执行一遍即可。 读写分离的主从延迟读写分离的主要目标就是分摊主库的压力。一种架构是：客户端（client）主动做负载均衡，这种模式下一般会把数据库的连接信息放在客户端的连接层。也就是说，由客户端来选择后端数据库进行查询。第二种架构是：在 MySQL 和客户端之间有一个中间代理层 proxy，客户端只连接 proxy， 由 proxy 根据请求类型和上下文决定请求的分发路由。 客户端直连和带 proxy 的读写分离架构，各有哪些特点： 客户端直连方案，因为少了一层 proxy 转发，所以查询性能稍微好一点儿，并且整体架构简单，排查问题更方便。但是这种方案，由于要了解后端部署细节，所以在出现主备切换、库迁移等操作的时候，客户端都会感知到，并且需要调整数据库连接信息。你可能会觉得这样客户端也太麻烦了，信息大量冗余，架构很丑。其实也未必，一般采用这样的架构，一定会伴随一个负责管理后端的组件，比如 Zookeeper，尽量让业务端只专注于业务逻辑开发。 带 proxy 的架构，对客户端比较友好。客户端不需要关注后端细节，连接维护、后端信息维护等工作，都是由 proxy 完成的。但这样的话，对后端维护团队的要求会更高。而且，proxy 也需要有高可用架构。因此，带 proxy 架构的整体就相对比较复杂。 不论使用哪种架构，都会碰到问题：由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。“在从库上会读到系统的一个过期状态”的现象，称为过期读。 处理过期读的方案处理过期读包括以下6种方案： 强制走主库方案； sleep 方案； 判断主备无延迟方案； 配合 semi-sync 方案； 等主库位点方案； 等 GTID 方案 强制走主库方案强制走主库方案其实就是，将查询请求做分类。通常情况下，我们可以将查询请求分为这么两类： 对于必须要拿到最新结果的请求，强制将其发到主库上。 对于可以读到旧数据的请求，才将其发到从库上。 这个方案最大的问题在于，有时候你会碰到“所有查询都不能是过期读”的需求，比如一些金融类的业务。这样的话，你就要放弃读写分离，所有读写压力都在主库，等同于放弃了扩展性。 Sleep 方案主库更新后，读从库之前先 sleep 一下。具体的方案就是，类似于执行一条 select sleep(1) 命令。这个方案的假设是，大多数情况下主备延迟在 1 秒之内，做一个 sleep 可以有很大概率拿到最新的数据。 这个方案比较麻瓜，个人觉得还是不要用了。 判断主备无延迟方案要确保备库无延迟，通常有三种做法： 判断seconds_behind_master 参数的值确保无延迟:每次从库执行查询请求前，先判断 seconds_behind_master 是否已经等于 0。如果还不等于 0 ，那就必须等到这个参数变为 0 才能执行查询请求(seconds_behind_master 的单位是秒，存在精度不够的问题)。 对比位点确保主备无延迟： 12Master_Log_File 和 Read_Master_Log_Pos，表示的是读到的主库的最新位点；Relay_Master_Log_File 和 Exec_Master_Log_Pos，表示的是备库执行的最新位点。 如果 Master_Log_File 和 Relay_Master_Log_File、Read_Master_Log_Pos 和 Exec_Master_Log_Pos 这两组值完全相同，就表示接收到的日志已经同步完成。 对比 GTID 集合确保主备无延迟： 123Auto_Position=1，表示这对主备关系使用了 GTID 协议。Retrieved_Gtid_Set，是备库收到的所有日志的 GTID 集合；Executed_Gtid_Set，是备库所有已经执行完成的 GTID 集合。 如果这两个集合相同，也表示备库接收到的日志都已经同步完成。可见，对比位点和对比 GTID 这两种方法，都要比判断 seconds_behind_master 是否为 0 更准确。 我们上面判断主备无延迟的逻辑，是备库收到的日志都执行完成了。但是，从 binlog 在主备之间状态的分析中，不难看出还有一部分日志，处于客户端已经收到提交确认，而备库还没收到日志的状态。这种情况下，我们去备库做查询，按照我们上面的逻辑，从库认为已经没有同步延迟，但还是没有查到主库上最新的事务，严格地说，就是出现了过期读。所以还是有有几率出现问题。 配合 semi-sync引入半同步复制，也就是 semi-sync replication。semi-sync 做了这样的设计： 事务提交的时候，主库把 binlog 发给从库； 从库收到 binlog 以后，发回给主库一个 ack，表示收到了； 主库收到这个 ack 以后，才能给客户端返回“事务完成”的确认。 也就是说，如果启用了 semi-sync，就表示所有给客户端发送过确认的事务，都确保了备库已经收到了这个日志。这样，semi-sync 配合前面关于位点的判断，就能够确定在从库上执行的查询请求，可以避免过期读。 但是，semi-sync+ 位点判断的方案，只对一主一备的场景是成立的。在一主多从场景中，主库只要等到一个从库的 ack，就开始给客户端返回确认。这时，在从库上执行查询请求，就有两种情况： 如果查询是落在这个响应了 ack 的从库上，是能够确保读到最新数据； 但如果是查询落到其他从库上，它们可能还没有收到最新的日志，就会产生过期读的问题。 判断同步位点的方案还有另外一个潜在的问题，即：如果在业务更新的高峰期，主库的位点或者 GTID 集合更新很快，那么上面的两个位点等值判断就会一直不成立，很可能出现从库上迟迟无法响应查询请求的情况，主库事务一直在发生，GTIO一直在更新，导致从库一直得不到ack结束的状态。 到这里，我们小结一下，semi-sync 配合判断主备无延迟的方案，存在两个问题： 一主多从的时候，在某些从库执行查询请求会存在过期读的现象； 在持续延迟的情况下，可能出现过度等待的问题。 等主库位点方案要理解等主库位点方案，需要介绍一条命令：select master_pos_wait(file, pos[, timeout]); 这条命令的逻辑如下：它是在从库执行的；参数 file 和 pos 指的是主库上的文件名和位置；timeout 可选，设置为正整数 N 表示这个函数最多等待 N 秒。 这个命令正常返回的结果是一个正整数 M，表示从命令开始执行，到应用完 file 和 pos 表示的 binlog 位置，执行了多少事务。除了正常返回一个正整数 M 外，这条命令还会返回一些其他结果，包括： 如果执行期间，备库同步线程发生异常，则返回 NULL； 如果等待超过 N 秒，就返回 -1； 如果刚开始执行的时候，就发现已经执行过这个位置了，则返回 0。 此方案我们执行一个事务然后执行查询语句的具体逻辑如下： trx1 事务更新完成后，马上执行 show master status 得到当前主库执行到的 File 和 Position； 选定一个从库执行查询语句； 在从库上执行 select master_pos_wait(File, Position, 1)； 如果返回值是 &gt;=0 的正整数，则在这个从库执行查询语句； 否则，到主库执行查询语句。 这里我们假设，这条 select 查询最多在从库上等待 1 秒。那么，如果 1 秒内 master_pos_wait 返回一个大于等于 0 的整数，就确保了从库上执行的这个查询结果一定包含了 trx1 的数据。步骤 5 到主库执行查询语句，是这类方案常用的退化机制。因为从库的延迟时间不可控，不能无限等待，所以如果等待超时，就应该放弃，然后到主库去查。你可能会说，如果所有的从库都延迟超过 1 秒了，那查询压力不就都跑到主库上了吗？确实是这样。 GTID 方案MySQL 中同样提供了一个类似的命令：select wait_for_executed_gtid_set(gtid_set, 1); 这条命令的逻辑是：等待，直到这个库执行的事务中包含传入的 gtid_set，返回 0；超时返回 1。 在前面等位点的方案中，我们执行完事务后，还要主动去主库执行 show master status。而 MySQL 5.7.6 版本开始，允许在执行完更新类事务后，把这个事务的 GTID 返回给客户端，这样等 GTID 的方案就可以减少一次查询。这时，等 GTID 的执行流程就变成了： trx1 事务更新完成后，从返回包直接获取这个事务的 GTID，记为 gtid1； 选定一个从库执行查询语句； 在从库上执行 select wait_for_executed_gtid_set(gtid1, 1)； 如果返回值是 0，则在这个从库执行查询语句； 否则，到主库执行查询语句。 跟等主库位点的方案一样，等待超时后是否直接到主库查询，需要业务开发同学来做限流考虑。 MySQL 在执行事务后，返回包中带上 GTID，只需要将参数 session_track_gtids 设置为 OWN_GTID，然后通过 API 接口 mysql_session_track_get_first 从返回包解析出 GTID 的值即可。 小结上述几种方案中，有的方案看上去是做了妥协，有的方案看上去不那么靠谱儿，但都是有实际应用场景的，你需要根据业务需求选择。即使是最后等待位点和等待 GTID 这两个方案，虽然看上去比较靠谱儿，但仍然存在需要权衡的情况。如果所有的从库都延迟，那么请求就会全部落到主库上，这时候会不会由于压力突然增大，把主库打挂了呢？ 在实际应用中，这几个方案是可以混合使用的。比如，先在客户端对请求做分类，区分哪些请求可以接受过期读，而哪些请求完全不能接受过期读；然后，对于不能接受过期读的语句，再使用等 GTID 或等位点的方案。 判断主库是否出问题的策略主备切换有两种场景，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由 HA 系统发起的。 外部检测外部检测都需要定时轮询，所以系统可能已经出问题了，但是却需要等到下一个检测发起执行语句的时候，我们才有可能发现问题。而且，如果你的运气不够好的话，可能第一次轮询还不能发现，这就会导致切换慢的问题。以下是一些外部检测的常用方案。 select 1 判断select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题。 并发连接和并发查询：并发连接和并发查询，并不是同一个概念。你在 show processlist 的结果里，看到的几千个连接，指的就是并发连接。而“当前正在执行”的语句，才是我们所说的并发查询。并发连接数达到几千个影响并不大，就是多占一些内存而已。我们应该关注的是并发查询，因为并发查询太高才是 CPU 杀手。这也是为什么我们需要设置 innodb_thread_concurrency 参数的原因。在线程进入锁等待以后，并发线程的计数会减一，也就是说等行锁（也包括间隙锁）的线程是不算在innodb_thread_concurrency 里面的。 查表判断为了能够检测 InnoDB 并发线程数过多导致的系统不可用情况，我们需要找一个访问 InnoDB 的场景。一般的做法是，在系统库（mysql 库）里创建一个表，比如命名为 health_check，里面只放一行数据，然后定期执行查询操作。使用这个方法，我们可以检测出由于并发线程过多导致的数据库不可用的情况。但是，空间满了以后，这种方法又会变得不好使。 我们知道，更新事务要写 binlog，而一旦 binlog 所在磁盘的空间占用率达到 100%，那么所有的更新语句和事务提交的 commit 语句就都会被堵住。但是，系统这时候还是可以正常读数据的。所以，此种判断逻辑，还是存在问题的。 更新判断既然要更新，就要放个有意义的字段，常见做法是放一个 timestamp 字段，用来表示最后一次执行检测的时间。节点可用性的检测都应该包含主库和备库。如果用更新来检测主库的话，那么备库也要进行更新检测。但，备库的检测也是要写 binlog 的。由于我们一般会把数据库 A 和 B 的主备关系设计为双 M 结构，所以在备库 B 上执行的检测命令，也要发回给主库 A。如果主库 A 和备库 B 都用相同的更新命令，就可能出现行冲突，也就是可能会导致主备同步停止。所以，现在看来 mysql.health_check 这个表就不能只有一行数据了。为了让主备之间的更新不产生冲突，我们可以在 mysql.health_check 表上存入多行数据，并用 A、B 的 server_id 做主键。由于 MySQL 规定了主库和备库的 server_id 必须不同（否则创建主备关系的时候就会报错），这样就可以保证主、备库各自的检测命令不会发生冲突。 更新判断是一个相对比较常用的方案了，不过依然存在一些问题。比如 判断慢：首先，所有的检测逻辑都需要一个超时时间 N。执行一条 update 语句，超过 N 秒后还不返回，就认为系统不可用。你可以设想一个日志盘的 IO 利用率已经是 100% 的场景。这时候，整个系统响应非常慢，已经需要做主备切换了。但是你要知道，IO 利用率 100% 表示系统的 IO 是在工作的，每个请求都有机会获得 IO 资源，执行自己的任务。而我们的检测使用的 update 命令，需要的资源很少，所以可能在拿到 IO 资源的时候就可以提交成功，并且在超时时间 N 秒未到达之前就返回给了检测系统。检测系统一看，update 命令没有超时，于是就得到了“系统正常”的结论。 内部统计针对磁盘利用率这个问题，如果 MySQL 可以告诉我们，内部每一次 IO 请求的时间，那我们判断数据库是否出问题的方法就可靠得多了。其实，MySQL 5.6 版本以后提供的 performance_schema 库，就在 file_summary_by_event_name 表里统计了每次 IO 请求的时间。因为我们每一次操作数据库，performance_schema 都需要额外地统计这些信息，所以我们打开这个统计功能是有性能损耗的。 可以通过 MAX_TIMER 的值来判断数据库是否出问题了。比如，你可以设定阈值，单次 IO 请求时间超过 200 毫秒属于异常，然后使用类似下面这条语句作为检测逻辑: 1mysql&gt; select event_name,MAX_TIMER_WAIT FROM performance_schema.file_summary_by_event_name where event_name in (&apos;wait/io/file/innodb/innodb_log_file&apos;,&apos;wait/io/file/sql/binlog&apos;) and MAX_TIMER_WAIT&gt;200*1000000000; 发现异常后，取到你需要的信息，再通过下面这条语句： 1mysql&gt; truncate table performance_schema.file_summary_by_event_name; 把之前的统计信息清空。这样如果后面的监控中，再次出现这个异常，就可以加入监控累积值了。 误删数据的处理方案误删行误删行的恢复措施如果是使用 delete 语句误删了数据行，可以用 Flashback 工具通过闪回把数据恢复回来。Flashback 恢复数据的原理，是修改 binlog 的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保 binlog_format=row 和 binlog_row_image=FULL。 具体恢复数据时，对单个事务做如下处理： 对于 insert 语句，对应的 binlog event 类型是 Write_rows event，把它改成 Delete_rows event 即可； 同理，对于 delete 语句，也是将 Delete_rows event 改为 Write_rows event； 而如果是 Update_rows 的话，binlog 里面记录了数据行修改前和修改后的值，对调这两行的位置即可。 误删行的预防措施 把 sql_safe_updates 参数设置为 on。这样一来，如果我们忘记在 delete 或者 update 语句中写 where 条件，或者 where 条件里面没有包含索引字段的话，这条语句的执行就会报错。 代码上线前，必须经过 SQL 审计。 误删库 / 表误删库 / 表的恢复措施这种情况下，要想恢复数据，就需要使用全量备份，加增量日志的方式了。这个方案要求线上有定期的全量备份，并且实时备份 binlog。 在这两个条件都具备的情况下，假如有人中午 12 点误删了一个库，恢复数据的流程如下： 取最近一次全量备份，假设这个库是一天一备，上次备份是当天 0 点； 用备份恢复出一个临时库； 从日志备份里面，取出凌晨 0 点之后的日志； 把这些日志，除了误删除数据的语句外，全部应用到临时库。 注意：在应用日志的时候，需要跳过 12 点误操作的那个语句的 binlog：如果实例使用了 GTID 模式，假设误操作命令的 GTID 是 gtid1，那么只需要执行 set gtid_next=gtid1;begin;commit; 先把这个 GTID 加到临时实例的 GTID 集合，之后按顺序执行 binlog 的时候，就会自动跳过误操作的语句。 这种方式恢复数据还是不够快，主要原因有两个： 如果是误删表，最好就是只恢复出这张表，也就是只重放这张表的操作，但是 mysqlbinlog 工具并不能指定只解析一个表的日志； 用 mysqlbinlog 解析出日志应用，应用日志的过程就只能是单线程。我们之前学习的并行复制的方法这里都用不上。 延迟复制备库一般的主备复制结构存在的问题是，如果主库上有个表被误删了，这个命令很快也会被发给所有从库，进而导致所有从库的数据表也都一起被误删了。 延迟复制的备库是一种特殊的备库，通过 CHANGE MASTER TO MASTER_DELAY = N 命令，可以指定这个备库持续保持跟主库有 N 秒的延迟。 比如你把 N 设置为 3600，这就代表了如果主库上有数据被误删了，并且在 1 小时内发现了这个误操作命令，这个命令就还没有在这个延迟复制的备库执行。这时候到这个备库上执行 stop slave，再通过之前介绍的方法，跳过误操作命令，就可以恢复出需要的数据。这样的话，你就随时可以得到一个，只需要最多再追 1 小时，就可以恢复出数据的临时实例，也就缩短了整个数据恢复需要的时间。 误删库 / 表的预防措施第一条建议是，账号分离。这样做的目的是，避免写错命令。比如： 我们只给业务开发同学 DML 权限，而不给 truncate/drop 权限。而如果业务开发人员有 DDL 需求的话，也可以通过开发管理系统得到支持。 即使是 DBA 团队成员，日常也都规定只使用只读账号，必要的时候才使用有更新权限的账号。 第二条建议是，制定操作规范。这样做的目的，是避免写错要删除的表名。比如： 在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响以后再删除这张表。 改表名的时候，要求给表名加固定的后缀（比如加 _to_be_deleted)，然后删除表的动作必须通过管理系统执行。并且，管理系删除表的时候，只能删除固定后缀的表。 rm 删除数据对于一个有高可用机制的 MySQL 集群来说，最不怕的就是 rm 删除数据了。只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。这时，你要做的就是在这个节点上把数据恢复回来，再接入整个集群。 为什么还有kill不掉的语句？在 MySQL 中有两个 kill 命令：一个是 kill query + 线程 id，表示终止这个线程中正在执行的语句；一个是 kill connection + 线程 id，这里 connection 可缺省，表示断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的。 有的时候我们发现，使用了 kill 命令，却没能断开这个连接。再执行 show processlist 命令，看到这条语句的 Command 列显示的是 Killed。 收到 kill 以后，线程做什么？kill 并不是马上停止的意思，而是告诉执行线程说，这条语句已经不需要继续执行了，可以开始“执行停止的逻辑了”。其实，这跟 Linux 的 kill 命令类似，kill -N pid 并不是让进程直接停止，而是给进程发一个信号，然后进程处理这个信号，进入终止逻辑。只是对于 MySQL 的 kill 命令来说，不需要传信号量参数，就只有“停止”这个命令。 当用户执行 kill query + 线程 id 命令时，MySQL 里处理 kill 命令的线程做了两件事： 把 线程(session) 的运行状态改成 THD::KILL_QUERY(将变量 killed 赋值为 THD::KILL_QUERY)； 给 线程(session) 的执行线程发一个信号。 为什么要发信号呢？ session处于锁等待状态，如果只是把 session 的线程状态设置 THD::KILL_QUERY，线程并不知道这个状态变化，还是会继续等待。发一个信号的目的，就是让 session 退出等待，来处理这个 THD::KILL_QUERY 状态。 上面的分析中，隐含了这么三层意思： 一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是 THD::KILL_QUERY，才开始进入语句终止逻辑； 如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处； 语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。 到这里你就知道了，原来不是“说停就停的”。 示例：一个 kill 不掉的例子，首先，执行 set global innodb_thread_concurrency=1，将 InnoDB 的并发线程上限数设置为 1；然后执行两条sql语句： 12345session A:select sleep(100) from t;session B:select * from t; 我们先执行sessionA，然后执行sessionB，这时，sessionB是被阻塞住的，因为我们设置了并发线程为1，所以只有sessionA在执行。 这时候我们通过sessionC执行：kill query B; 发现是没有什么效果的。为什么呢？ 在这个例子里，sessionB 线程的等待逻辑是这样的：每 10 毫秒判断一下是否可以进入 InnoDB 执行，如果不行，就调用 nanosleep 函数进入 sleep 状态。也就是说，虽然 sessionB 线程的状态已经被设置成了 KILL_QUERY，但是在这个等待进入 InnoDB 的循环过程中，线程没有被空闲出来，并没有去判断线程的状态，因此根本不会进入终止逻辑阶段。 我们可以通过sessionD执行：kill B；这个时候我们发现断开了 sessionB 的连接，提示“Lost connection to MySQL server during query”， 但是这时候，如果在 sessionD 中执行 show processlist，发现 id=sessionB 这个线程的 Commnad 列显示的是 Killed。也就是说，客户端虽然断开了连接，但实际上服务端上这条语句还在执行过程中。 sessionD 执行 kill connection 命令时，是这么做的： 把 12 号线程状态设置为 KILL_CONNECTION； 关掉 12 号线程的网络连接。因为有这个操作，所以你会看到，这时候 session C 收到了断开连接的提示。 那为什么执行 show processlist 的时候，会看到 Command 列显示为 killed 呢？其实，这就是因为在执行 show processlist 的时候，有一个特别的逻辑：如果一个线程的状态是KILL_CONNECTION，就把Command列显示成Killed。 所以其实，即使是客户端退出了，这个线程的状态仍然是在等待中。那这个线程什么时候会退出呢？答案是，只有等到满足进入 InnoDB 的条件后，session C 的查询语句继续执行，然后才有可能判断到线程状态已经变成了 KILL_QUERY 或者 KILL_CONNECTION，再进入终止逻辑阶段。 kill无效的情况总结 线程没有执行到判断线程状态的逻辑。由于 IO 压力过大，读写 IO 的函数一直无法返回，导致不能及时判断线程的状态。或者上述示例中，线程被占满，无法释放出来。 终止逻辑耗时较长。时候，从 show processlist 结果上看也是 Command=Killed，需要等到终止逻辑完成，语句才算真正完成。这类情况，比较常见的场景有以下几种： 1231. 超大事务执行期间被 kill。这时候，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长。2. 大查询回滚。如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长。3. DDL 命令执行到最后阶段，如果被 kill，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久。 如果直接在客户端通过 Ctrl+C 命令，是不是就可以直接终止线程呢？答案是，不可以。其实在客户端的操作只能操作到客户端的线程，客户端和服务端只能通过网络交互，是不可能直接操作服务端线程的。 关于客户端的误解如果库里面的表特别多，连接就会很慢有些线上的库，会包含很多表（我见过最多的一个库里有 6 万个表）。这时候，你就会发现，每次用客户端连接都会卡住很长时间，但是我们之前学习过，个客户端在和服务端建立连接的时候，需要做的事情就是 TCP 握手、用户校验、获取权限。但这几个操作，显然跟库里面表的个数无关。实际上，当使用默认参数连接的时候，MySQL 客户端会提供一个本地库名和表名补全的功能。为了实现这个功能，客户端在连接成功后，需要多做一些操作： 执行 show databases； 切到 db1 库，执行 show tables； 把这两个命令的结果用于构建一个本地的哈希表。 在这些操作中，最花时间的就是第三步在本地构建哈希表的操作。所以，当一个库中的表个数非常多的时候，这一步就会花比较长的时间。也就是说，我们感知到的连接过程慢，其实并不是连接慢，也不是服务端慢，而是客户端慢。 如果在连接命令中加上 -A，就可以关掉这个自动补全的功能，然后客户端就可以快速返回了。 除了加 -A 以外，加–quick(或者简写为 -q) 参数，也可以跳过这个阶段。但是，这个–quick 是一个更容易引起误会的参数，也是关于客户端常见的一个误解。实际上，设置了这个参数可能会降低服务端的性能。为什么这么说呢？ MySQL 客户端发送请求后，接收服务端返回结果的方式有两种： 一种是本地缓存，也就是在本地开一片内存，先把结果存起来。如果你用 API 开发，对应的就是 mysql_store_result 方法。 另一种是不缓存，读一个处理一个。如果你用 API 开发，对应的就是 mysql_use_result 方法。 MySQL 客户端默认采用第一种方式，而如果加上–quick 参数，就会使用第二种不缓存的方式。 采用不缓存的方式时，如果本地处理得慢，就会导致服务端发送结果被阻塞，因此会让服务端变慢。但是这个参数存在的作用还是有的，使用这个参数可以达到以下三点效果： 就是前面提到的，跳过表名自动补全功能。 mysql_store_result 需要申请本地内存来缓存查询结果，如果查询结果太大，会耗费较多的本地内存，可能会影响客户端本地机器的性能； 是不会把执行命令记录到本地的命令历史文件。 所以，–quick 参数的意思，是让客户端变得更快。 查询对数据库内存影响如果我们的数据库主机内存只有 100G，现在要对一个 200G 的大表做全表扫描，会不会把数据库主机的内存用光了？但是反过来想想，逻辑备份的时候，可不就是做整库扫描吗？如果这样就会把内存吃光，逻辑备份不是早就挂了？所以说，对大表做全表扫描，看来应该是没问题的。 全表扫描对 server 层的影响假设，我们现在要对一个 200G 的 InnoDB 表 db1. t，执行一个全表扫描。 InnoDB 的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表 t 的主键索引。这条查询语句由于没有其他的判断条件，所以查到的每一行都可以直接放到结果集里面，然后返回给客户端。那么，这个“结果集”存在哪里呢？实际上，服务端并不需要保存一个完整的结果集。取数据和发数据的流程是这样的： 获取一行，写到 net_buffer 中。这块内存的大小是由参数 net_buffer_length 定义的，默认是 16k。 重复获取行，直到 net_buffer 写满，调用网络接口发出去。 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer。 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。 从这个流程中，你可以看到： 一个查询在发送过程中，占用的 MySQL 内部的内存最大就是 net_buffer_length 这么大，并不会达到 200G； socket send buffer 也不可能达到 200G（默认定义 /proc/sys/net/core/wmem_default），如果 socket send buffer 被写满，就会暂停读数据的流程。 也就是说，MySQL 是“边读边发的”，这个概念很重要。这就意味着，如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间变长。 如果我们在服务端执行 show processlist 看到的结果有 State 的值一直处于“Sending to client”，就表示服务器端的网络栈写满了（上一节中曾提到，如果客户端使用–quick 参数，会使用 mysql_use_result 方法。这个方法是读一行处理一行。你可以想象一下，假设有一个业务的逻辑比较复杂，每读一行数据以后要处理的逻辑如果很慢，就会导致客户端要过很久才会去取下一行数据，就会出现这种情况）。 因此，对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用 mysql_store_result 这个接口，直接把查询结果保存到本地内存。 实际上，一个查询语句的状态变化是这样的（这里略去了其他无关的状态）： MySQL 查询语句进入执行阶段后，首先把状态设置成“Sending data”； 然后，发送执行结果的列相关的信息（meta data) 给客户端； 再继续执行语句的流程； 执行完成后，把状态设置成空字符串。 也就是说，“Sending data”并不一定是指“正在发送数据”，而可能是处于执行器过程中的任意阶段。比如，你可以构造一个锁等待的场景，就能看到 Sending data 状态。 仅当一个线程处于“等待客户端接收结果”的状态，才会显示”Sending to client”；而如果显示成“Sending data”，它的意思只是“正在执行”。 全表扫描对 InnoDB 的影响WAL 机制中InnoDB 内存的一个作用，是保存更新的结果，再配合 redo log，就避免了随机写盘。内存的数据页是在 Buffer Pool (BP) 中管理的，在 WAL 里 Buffer Pool 起到了加速更新的作用。而实际上，Buffer Pool 还有一个更重要的作用，就是加速查询。当事务提交的时候，磁盘上的数据页是旧的，这时候马上有一个查询要来读这个数据页，这时候查询根本不需要读磁盘，直接从内存拿结果，速度是很快的。所以说，Buffer Pool 还有加速查询的作用。 Buffer Pool 对查询的加速效果，依赖于一个重要的指标，即：内存命中率。InnoDB Buffer Pool 的大小是由参数 innodb_buffer_pool_size 确定的，一般建议设置成可用物理内存的 60%~80%。 你可以在 show engine innodb status 结果中，查看一个系统当前的 BP 命中率。一般情况下，一个稳定服务的线上系统，要保证响应时间符合要求的话，内存命中率要在 99% 以上。 如果所有查询需要的数据页都能够直接从内存得到，那是最好的，对应的命中率就是 100%。但，这在实际生产上是很难做到的。innodb_buffer_pool_size 小于磁盘的数据量是很常见的。如果一个 Buffer Pool 满了，而又要从磁盘读入一个数据页，那肯定是要淘汰一个旧数据页的。InnoDB 内存管理用的是最近最少使用 (Least Recently Used, LRU) 算法，这个算法的核心就是淘汰最久未使用的数据。 在 InnoDB 实现上，按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域。 LRU 算法执行流程变成了下面这样： 扫描过程中，需要新插入的数据页，都被放到 old 区域 ; 一个数据页里面有多条记录，这个数据页会被多次访问到，但由于是顺序扫描，这个数据页第一次被访问和最后一次被访问的时间间隔不会超过 1 秒，因此还是会被保留在 old 区域； 再继续扫描后续的数据，之前的这个数据页之后也不会再被访问到，于是始终没有机会移到链表头部（也就是 young 区域），很快就会被淘汰出去。 可以看到，这个策略最大的收益，就是在扫描这个大表的过程中，虽然也用到了 Buffer Pool，但是对 young 区域完全没有影响，从而保证了 Buffer Pool 响应正常业务的查询命中率。 小结由于 MySQL 采用的是边算边发的逻辑，因此对于数据量很大的查询结果来说，不会在 server 端保存完整的结果集。所以，如果客户端读结果不及时，会堵住 MySQL 的查询过程，但是不会把内存打爆。 而对于 InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/tags/MySQL/"},{"name":"实践二","slug":"实践二","permalink":"https://jjw-story.github.io/tags/实践二/"}],"author":"JJW"},{"title":"Spark-入门","slug":"Spark-入门","date":"2020-07-20T09:21:43.000Z","updated":"2020-08-14T02:21:53.327Z","comments":true,"path":"2020/07/20/Spark-入门/","link":"","permalink":"https://jjw-story.github.io/2020/07/20/Spark-入门/","excerpt":"","text":"Spark介绍Spark，是一种通用的大数据计算框架，正如传统大数据技术Hadoop的MapReduce、Hive引擎，以及Storm流式实时计算引擎等。 Spark使用Spark RDD、Spark SQL、Spark Streaming、MLlib、GraphX成功解决了大数据领域中，离线批处理、交互式查询、实时流计算、机器学习与图计算等最重要的任务和问题。 Spark主要用于大数据的计算，而Hadoop以后主要用于大数据的存储（比如HDFS、Hive、HBase等），以及资源调度（Yarn）。Spark+Hadoop的组合，是未来大数据领域最热门的组合，也是最有前景的组合！ Spark除了一站式的特点之外，另外一个最重要的特点，就是基于内存进行计算，从而让它的速度可以达到MapReduce、Hive的数倍甚至数十倍！ Spark特点 速度快：Spark基于内存进行计算（当然也有部分计算基于磁盘，比如shuffle）。 容易上手开发：Spark的基于RDD的计算模型，比Hadoop的基于Map-Reduce的计算模型要更加易于理解，更加易于上手开发，实现各种复杂功能，比如二次排序、topn等复杂操作时，更加便捷。 超强的通用性：Spark提供了Spark RDD、Spark SQL、Spark Streaming、Spark MLlib、Spark GraphX等技术组件，可以一站式地完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见的任务。 集成Hadoop：Spark并不是要成为一个大数据领域的“独裁者”，一个人霸占大数据领域所有的“地盘”，而是与Hadoop进行了高度的集成，两者可以完美的配合使用。Hadoop的HDFS、Hive、HBase负责存储，YARN负责资源调度；Spark复杂大数据计算。实际上，Hadoop+Spark的组合，是一种“double win”的组合。Spark本身并不提供大数据的存储。 极高的活跃度：Spark目前是Apache基金会的顶级项目，全世界有大量的优秀工程师是Spark的committer。并且世界上很多顶级的IT公司都在大规模地使用Spark。 Spark与Hadoop对比Spark，是分布式计算平台，是一个用scala语言编写的计算框架，基于内存的快速、通用、可扩展的大数据分析引擎。 Hadoop，是分布式管理、存储、计算的生态系统；包括HDFS，Hive，HBase（存储）、MapReduce（计算）、Yarn（资源调度） MapReduce：我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。 Hadoop和Spark都是并行计算，两者都是用MR模型进行计算。Hadoop一个作业称为一个Job，Job里面分为Map Task和Reduce Task阶段，每个Task都在自己的进程中运行，当Task结束时，进程也会随之结束；Spark用户提交的任务称为application，一个application对应一个SparkContext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGScheduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset，由TaskScheduler分发到各个executor中执行；executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 ps：一个Application -&gt; 多个job -&gt;一个job多个stage -&gt; 一个stage多个task 两者的各方面比较 Spark对标于Hadoop中的计算模块MR，但是速度和效率比MR要快得多； Spark没有提供文件管理系统，所以，它必须和其他的分布式文件系统进行集成才能运作，它只是一个计算分析框架，专门用来对分布式存储的数据进行计算处理，它本身并不能存储数据； Spark可以使用Hadoop的HDFS或者其他云数据平台进行数据存储，但是一般使用HDFS； Spark可以使用基于HDFS的HBase数据库，也可以使用HDFS的数据文件，还可以通过jdbc连接使用Mysql数据库数据；Spark可以对数据库数据进行修改删除，而HDFS只能对数据进行追加和全表删除； Spark数据处理速度秒杀Hadoop中MR； Spark处理数据的设计模式与MR不一样，Hadoop是从HDFS读取数据，通过MR将中间结果写入HDFS；然后再重新从HDFS读取数据进行MR，再刷写到HDFS，这个过程涉及多次落盘操作，多次磁盘IO，效率并不高；而Spark的设计模式是读取集群中的数据后，在内存中存储和运算，直到全部运算完毕后，再存储到集群中； Spark是由于Hadoop中MR效率低下而产生的高效率快速计算引擎，批处理速度比MR快近10倍，内存中的数据分析速度比Hadoop快近100倍（源自官网描述）； Spark中RDD一般存放在内存中，如果内存不够存放数据，会同时使用磁盘存储数据；通过RDD之间的血缘连接、数据存入内存中切断血缘关系等机制，可以实现灾难恢复，当数据丢失时可以恢复数据；这一点与Hadoop类似，Hadoop基于磁盘读写，天生数据具备可恢复性； Spark引进了内存集群计算的概念，可在内存集群计算中将数据集缓存在内存中，以缩短访问延迟，对7的补充； Spark中通过DAG图可以实现良好的容错。 部分基本组件对比与MapReduce对比MapReduce能够完成的各种离线批处理功能，以及常见算法（比如二次排序、topn等），基于Spark RDD的核心编程，都可以实现，并且可以更好地、更容易地实现。而且基于Spark RDD编写的离线批处理程序，运行速度是MapReduce的数倍，速度上有非常明显的优势。Spark速度快的原因是：MapReduce的计算模型太死板，必须是map-reduce模式，有时候即使完成一些诸如过滤之类的操作，也必须经过map-reduce过程，这样就必须经过shuffle过程。而MapReduce的shuffle过程是最消耗性能的，因为shuffle中间的过程必须基于磁盘来读写。而Spark的shuffle虽然也要基于磁盘，但是其大量transformation操作，比如单纯的map或者filter等操作，可以直接基于内存进行pipeline操作，速度性能自然大大提升。Spark的劣势是：由于Spark基于内存进行计算，虽然开发容易，但是真正面对大数据的时候（比如一次操作针对10亿以上级别），在没有进行调优的情况下，可能会出现各种各样的问题，比如OOM内存溢出等等。导致Spark程序可能都无法完全运行起来，就报错挂掉了，而MapReduce即使是运行缓慢，但是至少可以慢慢运行完。 Spark SQL与Hive 对比Spark SQL实际上并不能完全替代Hive，因为Hive是一种基于HDFS的数据仓库，并且提供了基于SQL模型的，针对存储了大数据的数据仓库，进行分布式交互查询的查询引擎。严格的来说，Spark SQL能够替代的，是Hive的查询引擎，而不是Hive本身，实际上即使在生产环境下，Spark SQL也是针对Hive数据仓库中的数据进行查询，Spark本身自己是不提供存储的，自然也不可能替代Hive作为数据仓库的这个功能。 Spark SQL的一个优点，相较于Hive查询引擎来说，就是速度快，同样的SQL语句，可能使用Hive的查询引擎，由于其底层基于MapReduce，必须经过shuffle过程走磁盘，因此速度是非常缓慢的。很多复杂的SQL语句，在hive中执行都需要一个小时以上的时间。而Spark SQL由于其底层基于Spark自身的基于内存的特点，因此速度达到了Hive查询引擎的数倍以上。而Spark SQL相较于Hive的另外一个优点，就是支持大量不同的数据源，包括hive、json、parquet、jdbc等等。此外，Spark SQL由于身处Spark技术堆栈内，也是基于RDD来工作，因此可以与Spark的其他组件无缝整合使用，配合起来实现许多复杂的功能。比如Spark SQL支持可以直接针对hdfs文件执行sql语句！ Spark Streaming与Storm对比Spark Streaming与Storm都可以用于进行实时流计算。但是他们两者的区别是非常大的。其中区别之一，就是，Spark Streaming和Storm的计算模型完全不一样，Spark Streaming是基于RDD的，因此需要将一小段时间内的，比如1秒内的数据，收集起来，作为一个RDD，然后再针对这个batch的数据进行处理。而Storm却可以做到每来一条数据，都可以立即进行处理和计算。因此，Spark Streaming实际上严格意义上来说，只能称作准实时的流计算框架；而Storm是真正意义上的实时计算框架。 此外，Storm支持的一项高级特性，是Spark Streaming暂时不具备的，即Storm支持在分布式流式计算程序（Topology）在运行过程中，可以动态地调整并行度，从而动态提高并发处理能力。而Spark Streaming是无法动态调整并行度的。但是Spark Streaming也有其优点，首先Spark Streaming由于是基于batch进行处理的，因此相较于Storm基于单条数据进行处理，具有数倍甚至数十倍的吞吐量。 此外，Spark Streaming由于也身处于Spark生态圈内，因此Spark Streaming可以与Spark Core、Spark SQL，甚至是Spark MLlib、Spark GraphX进行无缝整合。流式处理完的数据，可以立即进行各种map、reduce转换操作，可以立即使用sql进行查询，甚至可以立即使用machine learning或者图计算算法进行处理。这种一站式的大数据处理功能和优势，是Storm无法匹敌的。 对于Storm来说，如果仅仅要求对数据进行简单的流式计算处理，那么选择storm或者spark streaming都无可厚非。但是如果需要对流式计算的中间结果（RDD），进行复杂的后续处理，则使用Spark更好，因为Spark本身提供了很多原语，比如map、reduce、groupByKey、filter等等。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://jjw-story.github.io/categories/Spark/"}],"tags":[{"name":"入门","slug":"入门","permalink":"https://jjw-story.github.io/tags/入门/"},{"name":"Spark","slug":"Spark","permalink":"https://jjw-story.github.io/tags/Spark/"}],"author":"JJW"},{"title":"MySQL-实践一","slug":"MySQL-实践一","date":"2020-07-12T03:09:01.000Z","updated":"2020-11-26T09:11:00.557Z","comments":true,"path":"2020/07/12/MySQL-实践一/","link":"","permalink":"https://jjw-story.github.io/2020/07/12/MySQL-实践一/","excerpt":"","text":"普通索引和唯一索引的选择普通索引和唯一索引的查询过程对比例如：我们查询一个 where k=5 条件的SQL语句，这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，然后可以认为数据页内部通过二分法来定位记录。 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。 结论：这个索引不同查询带来的性能差距会有多少呢？答案是，微乎其微。 普通索引和唯一索引的更新过程对比change buffer当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。 唯一索引和普通索引对于change buffer的应用特点对比对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如我们要插入一条数据，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。 因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 change buffer 的使用场景因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。对于写少读多的业务，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。 普通索引和唯一索引的选择结论：这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。 change buffer 和 redo logredo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。(这块还是有点不太理解，个人理解就是：WAL是数据页已经在内存中了，这时我们修改或插入此数据页数据，直接更新内存中的数据，然后写入redo log，系统在空闲的时候进行将操作更新到磁盘。 而 change buffer 主要解决的是数据页本身就不在内存中，这时候我们更新或者添加数据，就直接写入到change buffer中，等到需要读此数据页的数据时，才将此数据读入内存中，然后合并 change buffer中的修改) MySQL为什么有时候会选错索引1：MySQL选错索引，啥意思？ 我们认为使用K索引检索的速度会更快的，但是MySQL没有使用，决定使用什么索引是由Server层的优化器来决定的，她也是想选择最佳的方案来检索数据的，不过他也是人写的程序也是存在bug的。 2：MySQL为啥会选错索引？ 优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。优化器认为使用那个索引检索数据的速度比较快是一个需要各种因素综合评估的事情，比如：是否使用临时表、是否排序、扫描的行数多少、回表的次数等。 索引的创建是非常的耗时的，因为需要真正的建索引的过程，但是删除索引却不需要耗费太多时间，因为是标记删除，这个是以空间换时间的思路。优化器采用采样评估出现误差的原因也在于，索引的标记删除影响的。 3：mysql如何判断一个查询的扫描行数? MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。我们可以使用 show index 方法，看到一个索引的基数。 4：索引基数如何计算? 为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。 5：可以重新统计索引信息的命令是什么? analyze table t 命令，可以用来重新统计索引信息 6： 索引选择异常的问题可以有哪几种处理方式? 强制指定使用某个索引，不常用不推荐用 调整SQL语句，使优化器选择的和我们想的一样，不具有通用性 新建更合适的索引或者删除不合适的索引，是一个思路 使用analyze table可以解决索引统计信息不准确导致的索引选错的问题 怎么给字符串字段加索引MySQL 是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。 前缀索引使用示例： 12345# 正常默认索引-包含了每个记录的整个字符串alter table SUser add index index1(email);# 前缀索引-对于每个记录都是只取前 6 个字节alter table SUser add index index2(email(6)); 两种索引的数据结构和存储区别前缀索引因为只取整个字段的前几个字节，所以占用空间会更少，这就是使用前缀索引的优势，这同时带来的损失是，可能会增加额外的记录扫描次数，具体来说主要是使用索引检索的过程如下进行分析： 如果我们用的是整个字符串的索引，首先我们直接根据索引定位要查询的字符串，然后回表，取出数据，然后检索对比下一条，发现不满足条件，就查询结束。 如果使用前缀索引，因为前缀相同的记录可能会有多条，这样的情况也很正常，它就需要每一条都扫描，然后回表去查询具体的数据来进行对比，一直找到前缀不能匹配的记录，所以这就是前缀索引会增加额外的扫描次数的原因。 但是：如果我们通过分析业务，使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本（通过前缀就能很快的区分出大部分的数据）。 前缀索引对覆盖索引的影响如下一条查询语句： 1select id,email from SUser where email=&apos;zhangssxyz@xxx.com&apos;; 如果我们直接使用普通索引（email），我们因为有覆盖索引的机理，所以一次就能返回所有的数据而不需要进行回表，但是如果我们使用了覆盖索引，每一次都需要回表判断email字段是不是对应的具体记录，这样就增加了回表的逻辑，而事实上，即使我们的前缀索引长度设置的足够长，能包含所有的字段长度，它还是需要回表进行判断，因为系统并不知道前缀索引的定义是否截断了完整的信息。 综上所述：使用前缀索引就用不上覆盖索引对查询性能的优化了 前缀索引使用的其他方式对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？要知道，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。 方式一：倒序存储：倒序存储就是针对一些前缀区别度不大，但是后缀区别比较大的字段类型，我们可以使用倒序存储来创建前缀索引。 方式二：使用hash字段：你可以在表上再创建一个整数字段，来保存你所要的业务字段的校验码，同时在这个字段上创建校验码。 使用倒序存储和使用 hash 字段这两种方法的异同点: 相同点是，都不支持范围查询 不同点： 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。 总结 直接创建完整索引，这样可能比较占用空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题； 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。 为什么MySQL会“抖一下”抖一下：一条SQL语句正常执行的时候非常的快，但有时突然就会变得特别慢，而且很难复现，它不止是随机的，而且持续时间很短。 脏页当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志），在更新内存写完 redo log 后，就返回给客户端，本次更新成功（WAL）。 内存里的数据总是要找时间写入磁盘的，这个过程术语就是 flush。在这个 flush 操作执行之前，内存中的数据跟磁盘上的数据是不一致的，因为还没有将redo log中的记录刷新到磁盘上。 所以我们就能够想象，我们的MySQL突然抖一下，就是因为在刷脏页，平时执行很快的更新操作，其实就是写了内存和日志。 数据库flush过程的触发点 InnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写（环形链表结构）。checkpoint 可不是随便往前修改一下位置就可以的。它需要将两个点之间的日志，对应的所有脏页都 flush 到磁盘上。 系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。为什么刷脏页一定会写盘，这是为了保证每个数据页只有两种状态： 一种是内存里存在，内存里就肯定是正确的结果，直接返回； 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样的效率最高。 MySQL 认为系统“空闲”的时候。当然MySQL及时忙的时候，也会见缝插针的找时间，只要有机会就会刷一点脏页。 MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 触发点的具体分析我们着重分析第一种和第二种情况，因为第三种和第四种都是正常的情况，我们不需要关注它的性能问题。 第一种是“redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。 第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。 所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长； 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。 InnoDB 刷脏页的控制策略 首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。因为没能正确地设置 innodb_io_capacity 参数，而导致的性能问题也比比皆是。 innodb_flush_neighbors：MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。SSD的时代，建议设置为0，MySQL8 已经默认设置为0了。 总结WAL这个机制后续需要的刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。 重点的重点淘汰的时候，刷脏页过程不用动redo log文件的，直接将脏页数据刷到磁盘中就完成了。而在redo log重放的过程中，有个额外的保证，如果一个数据页已经是刷过的，会识别出来并跳过。这个保证的具体实现机制是：innodb的每个数据页头部有LSN，8字节，每次修改都会变大，对比这个LSN跟checkpoint 的LSN，比checkpoint小的一定是干净页。这样解决的问题是：当内存不够用了，要将脏页写到磁盘，会有一个数据页淘汰机制（最久不使用），假设淘汰的是脏页，则此时脏页所对应的redo log的位置是随机的，当有多个不同的脏页需要刷，则对应的redo log可能在不同的位置，这样就需要把redo log的多个不同位置刷掉，这样对于redo log的处理不是就会很麻烦（合并间隙，移动位置） 现在我们对 WAL、change buffer、脏页、flush串起来一下，首先我们来了一条更新数据的语句，如果此数据所在页已经在内存中，那么MySQL直接更新内存数据，然后写redo log，如果此数据页没有再内存中，那么MySQL会写change buffer（前提不是添加数据，或者修改的列没有建立唯一索引，如果建立了，就需要将数据页加载到内存中进行判断唯一，这样就直接更新内存了），然后写redo log，这样我们的更新就完成了。下面我们说flush的过程，1. 在我们内存满了后需要淘汰脏页，这是就会将脏页的数据在内存中直接刷到磁盘上，WAL进行redo log重放的时候，会判断此数据是否被之前已有脏页刷过盘了，如果刷过了就跳过，没有刷过就重放更新磁盘数据。2. change buffer中的数据加载是看数据有没有被读取过，如果数据被读取，那么数据就会从磁盘读取到内存中，然后从change buffer中取出更改更新到内存中，那么此时内存中就是脏页，脏页在淘汰或者刷盘中就走了正常逻辑，如果没有被读取，change buffer 的后台也会有自己的合并行为，在一定的时机就会将change buffer中的数据刷新到磁盘中（这里重点注意：如果change buffer中的数据被丢了，那么它还是可以通过 redo log 进行重放进行恢复）。3. redo log 的重放，重放会将我们所有的更改持久化到磁盘数据中，这里重放的时候会判断当前数据有没有被脏页刷盘过，有没有被change buffer的后台自动持久化过，如果有，就跳过，如果没有，就通过redo log进行重放（LSN）。 redo log 的重放：redo log 并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由 redo log 更新过去”的情况。 如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与 redo log 毫无关系。 在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让 redo log 更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。 数据库表的空间回收一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小。 参数 innodb_file_per_table表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的： 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。 建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。 数据删除流程InnoDB 里的数据都是用 B+ 树的结构组织的，假设我们有一个数据页PageA，数据页中存的数据ID有（3、5、6），现在我们把 ID为5 的这条记录删除，InnoDB 引擎只会把 5 这个记录标记为删除，如果之后要再插入一个 ID 在 3 和 6 之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。如果我们删掉了一个数据页上的所有记录，那么整个数据页就可以被复用了，但是，数据页的复用跟记录的复用是不同的。记录的复用，只限于符合范围条件的数据，比如上面这个例子，我们删除 5 记录之后，如果插入一个 ID 为8 的记录，就不能复用这个位置了。而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID=50 的记录需要使用新页的时候，page A 是可以被复用的。 如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。 进一步地，如果我们用 delete 命令把整个表的数据删除，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。 空洞上述我们了解到，delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。 插入数据也可以产生空洞：如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。如果我们某个Page页满了，向此页中插入数据就不得不再申请一个新的数据页，来保存数据，页分裂完成后，旧页的末尾就留下了空洞。 更新索引上的值也可以产生空洞：更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的（比如将一个索引的值 10 改为 1000）。 重建表经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。而重建表，就可以达到这样的目的。 可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程大致是： 1新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。 由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。 MySQL 会自动完成转存数据、交换表名、删除旧表的操作。 Online DDL上述流程中，显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。在 MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。下面描述一下引入了 Online DDL 之后，重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件； 用临时文件替换表 A 的数据文件。 此过程与上述重建表过程不同的是，由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。 Online 和 inplaceinplace：上述我们在讲MySQL 5.5 以前的版本重建表的流程，我们把表 A 中的数据导出来的存放位置叫作 tmp_table。这是一个临时表，是在 server 层创建的，而5.5之后，根据表 A 重建出来的数据是放在“tmp_file”里的，这个临时文件是 InnoDB 在内部创建出来的。整个 DDL 过程都在 InnoDB 内部完成。对于 server 层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。 inplace重建表的语句： 1alter table t engine=innodb,ALGORITHM=inplace; (ALGORITHM=inplace 可以不加，因为它是默认的) copy重建表的语句： 1alter table t engine=innodb,ALGORITHM=copy; 当你使用 ALGORITHM=copy 的时候，表示的是强制拷贝表，对应的就是类似上述5.5版本重建表的逻辑流程。 Online 和 inplace的区别 DDL 过程如果是 Online 的，就一定是 inplace 的； 反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。 optimize table、analyze table 和 alter table 这三种方式重建表的区别 从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是上面 Online重建表的 流程了； analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁； optimize table t 等于 recreate+analyze。 count函数应用剖析count(*) 的实现方式在不同的 MySQL 引擎中，count(*) 有不同的实现方式： MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高； 而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 这里需要注意的是，我们在这篇文章里讨论的是没有过滤条件的 count(*)，如果加了 where 条件的话，MyISAM 表也是不能返回得这么快的。 为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。 InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。 小结 MyISAM 表虽然 count(*) 很快，但是不支持事务； show table status 命令虽然返回很快，但是不准确； InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。 不同的 count 用法count() 的语义：count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的（这里这个判断虽然是多余的，主键不可能为空，但是MySQL代码确实是这么做的），就按行累加。 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 对于 count(字段) 来说： 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 对于 count(*) 来说：并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 结论：按照效率排序的话，count(字段) &lt; count(主键 id) &lt; count(1) ≈ count()，所以我建议你，尽量使用 count(\\)。 order by 的工作原理这里用一条SQL语句举例说明： select city,name,age from t where city=’杭州’ order by name limit 1000; （city字段建立了索引） 全字段排序MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。通常情况下，全字段排序上述语句执行流程如下： 初始化 sort_buffer，确定放入 name、city、age 这三个字段； 从索引 city 找到第一个满足 city=’杭州’ 条件的主键 id； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到 city 的值不满足查询条件为止； 对 sort_buffer 中的数据按照字段 name 做快速排序； 按照排序结果取前 1000 行返回给客户端。 排序这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序，外部排序一般使用归并排序算法。（一般是将数据分为多个小临时文件排好顺序，然后归并排序到一个大文件中） rowid 排序在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。 我们可以通过配置参数max_length_for_sort_data，控制用于排序的行数据的长度，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。使用如下： 1SET max_length_for_sort_data = 16; city、name、age 这三个字段的定义总长度是 36，我把 max_length_for_sort_data 设置为 16，新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id，这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回，具体执行流程如下： 初始化 sort_buffer，确定放入两个字段，即 name 和 id； 从索引 city 找到第一个满足 city=’杭州’条件的主键 id； 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到不满足 city=’杭州’条件为止； 对 sort_buffer 中的数据按照字段 name 进行排序； 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。 对比全字段排序，rowid 排序多访问了一次表 t 的主键索引，就是步骤 7。 全字段排序 VS rowid 排序如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。如果内存够，就要多利用内存，尽量减少磁盘访问。 不用排序算法的 order by 场景如果我们对上述查询语句的 city 和 name 建立联合索引，这时，对于 city 字段的值相同的行来说，还是按照 name 字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了： 从索引 (city,name) 找到第一个满足 city=’杭州’条件的主键 id； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回； 从索引 (city,name) 取下一个记录主键 id； 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=’杭州’条件时循环结束。 在我们这个例子里，只需要扫描 1000 次，不需要经历排序算法。 我们还可以建立(city,name,age)的联合索引，这样上述步骤中的的回表操作就可以完全省略，因为在索引中已经包含了我们需要的所有字段（覆盖索引的原理），这样性能会更快。 当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。 explain 中 Extra 字段返回结果Using filesort：表示的就是需要排序 Using temporary表示的是需要使用临时表 order by内部会使用归并排序，根据sort buffer size决定是否需要使用外部（磁盘）排序，根据max_length_for_sort_data决定使用全字段排序还是rowid排序，不同点是rowid排序，只使用排序字段和主键，会在原有的基础上，多进行回表查询，多了磁盘操作，为此可以使用复合查询，这样从索引中查询出来的数据，就是有序的，可以直接进行回表，返回result，也可以考虑是否使用覆盖索引，直接返回值，如果order by后面加上limit num，num是小值，在5.6以上会使用优先队列进行排序。 rowid详解如果把一个 InnoDB 表的主键删掉，是不是就没有主键，就没办法回表了？其实不是的。 如果你创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。这也就是排序模式里面，rowid 名字的来历。实际上它表示的是： 每个引擎用来唯一标识数据行的信息。对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID； 对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的； MEMORY 引擎不是索引组织表，你可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。 本章节注意：内存临时表、磁盘临时表的概念 SQL语句执行性能分析条件字段函数操作对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 例如，对于一个查询语句，t_modified字段类型为datetime类型： 1select count(*) from tradelog where month(t_modified)=7; 上述语句就不会走索引，因为对索引字段做了month()函数操作 例如，对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。 隐式类型转换例如如下查询语句，tradeid 这个字段上建立了索引，字段类型是 varchar(32)，但是此语句却没有走索引： 1mysql&gt; select * from tradelog where tradeid=110717; 这里的原因就是，tradeid 的字段类型是 varchar(32)，而输入的参数却是整型，所以需要做类型转换，所以对于优化器来说，上述语句相当于： 1select * from tradelog where CAST(tradid AS signed int) = 110717; 也就是说，这条语句触发了我们上面说到的规则：对索引字段做函数操作，优化器会放弃走树搜索功能。 注意重点：当字符串和数字比较时会把字符串转化为数字 隐式字符编码转换例如我们有如下查询语句，此两个表的tradeid 这个字段上都建立了索引 1select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; 在这条语句中，tradelog 为驱动表，trade_detail为被驱动表，具体的查询逻辑如下： 我们首先通过tradelog驱动表的主键索引找到id = 2的记录； 然后找出tradeid字段的值； 然后根据 tradeid 值到 trade_detail 表中查找条件匹配的行。 这里我们通过explain结果集发现，trade_detail 的 tradeid 值匹配并没有走索引，这是因为：这两个表的字符集不同，一个是 utf8mb4，一个是 utf8，所以做表连接查询的时候用不上关联字段的索引。所以我们在做上述第三步的时候，具体查询逻辑类似如下SQL： 123select * from trade_detail where tradeid=$L2.tradeid.value;也就是：select * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; $L2.tradeid.value 的字符集是 utf8mb,。字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。 这个设定很好理解，utf8mb4 是 utf8 的超集。类似地，在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。 所以优化此查询语句，就可以修改为： 1select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 这里，我主动把 l.tradeid 转成 utf8，就避免了被驱动表上的字符编码转换，从 explain 结果可以看到，这次索引走对了。 本节总结索引字段不能进行函数操作，但是索引字段的参数可以玩函数，一言以蔽之。 SQL语句执行慢分析查询长时间不返回等 MDL 锁例如我们执行一条很简单的语句：select * from t where id=1; 结果长时间不返回，一般出现这种情况大概率是表被锁住了，分析时我们一般执行 show processlist 命令，查看当前语句处于什么状态。 使用 show processlist 命令查看语句出现 Waiting for table metadata lock 的状态，说明现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。场景是:session A 通过 lock table 命令持有表 t 的 MDL 写锁，而 session B 的查询需要获取 MDL 读锁。所以，session B 进入等待状态。这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。(通过查询 sys.schema_table_lock_waits 这张表，我们就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可。) 等 flush我们使用select * from information_schema.processlist查询结果，发现状态有 Waiting for table flush，这个状态表示的是，现在有一个线程正要对表 t 做 flush 操作。MySQL 里面对表做 flush 操作的用法，一般有以下两个： flush tables t with read lock; flush tables with read lock; 这两个 flush 语句，如果指定表 t 的话，代表的是只关闭表 t；如果没有指定具体的表名，则表示关闭 MySQL 里所有打开的表。但是正常这两个语句执行起来都很快，除非它们也被别的线程堵住了。所以，出现 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句，这个别的语句可能是一个耗时很长的查询操作，或者耗时很长的更新操作。 等行锁例如：一条更新语句占用了写锁，并且事务一直没有提交，另一条查询语句如下 select * from t where id=1 lock in share mode; 或是使用了for update都表示需要当前读，这样就需要给当前数据加写锁，这时因为上一条执行事务没有提交，锁一直没有释放，就导致本条查询语句一直被阻塞。 我们可以通过SQL语句查看具体是被什么被锁住了：select * from t sys.innodb_lock_waits where locked_table=’[schema.tablename]’。查找到 blocking_pid，然后 kill [pid] 即可完成释放。 查询慢扫描行数多例如有如下语句：select * from t where c=50000 limit 1; 这条语句慢是因为字段c上没有索引，所以只能走主键ID顺序扫描，然后取出行记录进行对比，因此需要扫描50000行。扫描行数越多，执行越慢 一致性读 undo log 回放比如我们会遇到一种现象，执行SQL语句 select * from t where id=1； 这条SQL只扫描一行，但是执行时间确很长，如果我们执行语句是 select * from t where id=1 lock in share mode，执行时扫描行数也是 1 行，执行时间确非常短，看上去非常奇怪，按理说 lock in share mode 还要加锁，时间应该更长才对啊，具体说明如下，如下SQL流程： session A session B start transaction with consistent snapshot; update t set c = c + 1 where id = 1;// 执行100万次 select * from t where id = 1; select * from t where id = 1 lock in share mode; 你看到了，session A 先用 start transaction with consistent snapshot 命令启动了一个事务，之后 session B 才开始执行 update 语句，session B 更新完 100 万次，生成了 100 万个回滚日志 (undo log)。带 lock in share mode 的 SQL 语句，是当前读，因此会直接读到 1000001 这个结果，所以速度很快；而 select * from t where id=1 这个语句，是一致性读，因此需要从 1000001 开始，依次执行 undo log，执行了 100 万次以后，才将 1 这个结果返回。所以这就是上述现象的原因。 幻读和间隙锁幻读什么是幻读？ 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。 上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。 幻读带来的问题？ 对行锁语义的破坏 破坏了数据一致性 为啥会出现幻读？ 行锁只能锁定存在的行，针对新插入的操作没有限定 间隙锁产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。顾名思义，间隙锁，锁的就是两个值之间的空隙。 注意：间隙锁是在可重复读隔离级别（RR）下才会生效的。 数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样。跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们有一个表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 next-key lock，分别是 (-∞, id1]、(id2, id3]、… 、(idn, +supremum]。supremum从哪儿来的呢？这是因为 +∞是开区间。实现上，InnoDB 给每个索引加了一个不存在的最大值 supremum，这样才符合我们前面说的“都是前开后闭区间”。 注意：间隙锁本身是前开后开的区间锁，next-key lock 才是前开后闭区间。 间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。 非索引字段update或者delete对于非索引字段进行update或select .. for update操作，代价极高。所有记录上锁，以及所有间隔的锁。对于索引字段进行上述操作，代价一般。只有索引字段本身和附近的间隔会被加锁。（个人理解：锁是加在主键索引上的） MySQL加锁规则MySQL加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 注意： 只有访问到的对象才会加锁，比如我们访问一个普通索引的字段，使用覆盖索引就可以查询出来数据，这时我们是不会给主键索引来加锁的。（lock in share mode 只锁覆盖索引，但是如果是 for update 就不一样了。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。） 申请锁的顺序是 先申请间隙锁，然后再申请行锁。（有的时候我们一条更新语句加锁的时候我们先加了间隙锁，然后加行锁的时候发现行被占用，因为间隙锁之间不是互斥的，这是行锁申请失败被阻塞住等待其他事务释放行锁，但间隙锁已经加成功，也会导致一些其他的操作被此间隙锁所阻塞住）。 在删除数据的时候尽量加 limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。 “有行”才会加行锁。如果查询条件没有命中行，那就加 next-key lock。 范围查询：无论是否是唯一索引，范围查询都需要访问到不满足条件的第一个值为止。 例如有一个表有数据如下： 1(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 对应字段为(id, c, d)，c 建立了普通索引 示例一我们有如下查询语句： 12beginselect * from t where t.c &gt;= 15 and c &lt; 20 order by c desc lock in share mode; 这里的加锁逻辑如下: 由于是 order by c desc，第一个要定位的是索引 c 上“最右边的”c=20 的行，所以会加上间隙锁 (20,25) 和 next-key lock (15,20]。 在索引 c 上向左遍历，要扫描到 c=10 才停下来，所以 next-key lock 会加到 (5,10]。 注意这里是重点：为什么会加(5,10]，因为我们的desc向左扫描的时候，我们需要扫描到10才能停下来，而我们的 next-key lock 是前开后闭区间，为了保证10这行本身也被锁住，所以我们需要正向的加 (5,10] 的 next-key lock，所以这就是为什么这个锁的范围这么广的原因。而如果我们是asc，这样就不会存在这种问题，就是正常的加锁逻辑，不会多出来一个锁区间。 在扫描过程中，c=20、c=15、c=10 这三行都存在值，由于是 select *，所以会在主键 id 上加三个行锁。 先定位索引c上最右边c=20的行，所以第一个等值查询会扫描到c=25，然后通过优化2，next-key lock退化为间隙锁，则会加上间隙锁（20，25），紧接着再向左遍历，会加 next-key lock (15, 20], (10, 15], 因为要扫描到c=10才停下来，所以也会加next-key lock (5,10] 示例二如上数据库表结构还有经典加锁示例如下语句： sessionA: 12begin;select * from t where c &gt; 5 lock in share mode; sessionB: 1234update t set c = 1 where c = 5;(Query OK)update t set c = 5 where c = 1;(blocked) 上述语句同时执行，sessionB的第二条语句会被阻塞住。原因是：首先sessionA的加锁范围是: 索引 c 上的 (5,10]、(10,15]、(15,20]、(20,25]和 (25,supremum]。 注意：根据 c&gt;5 查到的第一个记录是 c=10，因此不会加 (0,5]这个 next-key lock。 之后 session B 的第一个 update 语句，要把 c=5 改成 c=1，你可以理解为两步： 插入 (c=1, id=5) 这个记录； 删除 (c=5, id=5) 这个记录。 索引 c 上 (5,10) 间隙是由这个间隙右边的记录，也就是 c=10 定义的。所以通过这个操作，session A 的加锁范围变成了 (1,10]、(10,15] … 接下来 session B 要执行 update t set c = 5 where c = 1 这个语句了，一样地可以拆成两步： 插入 (c=5, id=5) 这个记录； 删除 (c=1, id=5) 这个记录。 第一步试图在已经加了间隙锁的 (1,10) 中插入数据，所以就被堵住了。 示例三如上数据库表结构还有经典加锁示例如下语句： 12begin;select * from t where id&gt;9 and id&lt;12 order by id desc for update; 这个语句的加锁范围是主键索引上的 (0,5]、(5,10]和 (10, 15)。也就是说，id=15 这一行，并没有被加上行锁。为什么呢？ 当试图去找 “第一个id &lt; 12的值”的时候，用的还是从左往右的遍历（因为用到了优化2），也就是说，当去找第一个等值的时候（通过树搜索去定位记录的时候），即使order by desc，但用的还是向右遍历，当找到了第一个等值的时候（例子中的id=15），然后根据order by desc，再向左遍历。 意向锁 为什么没有意向锁的话，表锁和行锁不能共存？ 举个粟子（此时假设行锁和表锁能共存）： 事务A锁住表中的一行（写锁）。事务B锁住整个表（写锁）。 但你就会发现一个很明显的问题，事务A既然锁住了某一行，其他事务就不可能修改这一行。这与”事务B锁住整个表就能修改表中的任意一行“形成了冲突。所以，没有意向锁的时候，行锁与表锁共存就会存在问题！ 意向锁是如何让表锁和行锁共存的？ 有了意向锁之后，前面例子中的事务A在申请行锁（写锁）之前，数据库会自动先给事务A申请表的意向排他锁。当事务B去申请表的写锁时就会失败，因为表上有意向排他锁之后事务B申请表的写锁时会被阻塞。 所以，意向锁的作用就是： 当一个事务在需要获取资源的锁定时，如果该资源已经被排他锁占用，则数据库会自动给该事务申请一个该表的意向锁。如果自己需要一个共享锁定，就申请一个意向共享锁。如果需要的是某行（或者某些行）的排他锁定，则申请一个意向排他锁。 意向锁是表锁还是行锁？ 首先可以肯定的是，意向锁是表级别锁。意向锁是表锁是有原因的。 当我们需要给一个加表锁的时候，我们需要根据意向锁去判断表中有没有数据行被锁定，以确定是否能加成功。如果意向锁是行锁，那么我们就得遍历表中所有数据行来判断。如果意向锁是表锁，则我们直接判断一次就知道表中是否有数据行被锁定了。 注：意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/tags/MySQL/"},{"name":"实践一","slug":"实践一","permalink":"https://jjw-story.github.io/tags/实践一/"}],"author":"JJW"},{"title":"MySQL-基础","slug":"MySQL-基础","date":"2020-07-10T12:37:48.000Z","updated":"2020-07-28T03:27:38.833Z","comments":true,"path":"2020/07/10/MySQL-基础/","link":"","permalink":"https://jjw-story.github.io/2020/07/10/MySQL-基础/","excerpt":"","text":"SQL查询语句的执行过程MySQL 可以分为 Server 层和存储引擎层两部分 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。我们可以在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。 不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。 连接器连接器负责跟客户端建立连接、获取权限、维持和管理连接。一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它，如果其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了.怎么解决这个问题呢: 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，如下查询语句： 1select SQL_CACHE * from T where ID=10; 需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 分析器如果没有命中查询缓存，就要开始真正执行语句了。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 优化器经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。 具体的执行查询会一行一行的扫描，扫描到满足条件的行就返回，如果没有就直到扫描到最后一行，对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 SQL更新语句的执行过程可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。 你执行语句前要先连接数据库，这是连接器的工作。在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log 和 binlog redo logWAL 技术:Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，类似于循环链表的一个数据结构。write pos 是当前记录的位置的一个指针，一边写一边后移，checkpoint 是当前要擦除的位置的指针，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 redo log buffer开启一个事务有多个操作，我们将里面的各种操作一个一个执行完后日志写入到redo log buffer，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。 redo log buffer 就是一块内存，用来先存 redo 日志的。 binlog前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的 redo log 是 InnoDB 引擎特有的日志（重做日志），而 Server 层也有自己的日志，称为 binlog（归档日志），他们的不同之处有如下三点: redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 一条更新语句的流程如下： 1234567891. 取ID=2这一行2. 数据是否在内存中2 -&gt; true: 返回行数据； 2 -&gt; false:从磁盘读入内存3. 内存中的行数据C值+14. 写入新行5. 新行更新到内存6. 写入redo log - 处于prepare阶段7. 写binlog8. 提交事务，处于commit状态，同时提交 redo log。（两阶段提交） 最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。 两阶段提交上述我们记录演示了redo log的两阶段提交，但是我们为什么要使用两阶段提交，具体如下。 binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。怎样让数据库恢复到半个月内任意一秒的状态: 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库。 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 如果我们执行一条更新语句，不使用两阶段提交，会出现什么情况： 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 两阶段提交日志相关问题 在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象，如果 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理： 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交； 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：a. 如果是，则提交事务；b. 否则，回滚事务。 MySQL 怎么知道 binlog 是完整的： 一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event。 redo log 和 binlog 是怎么关联起来的? 它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log： 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交； 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。 总结redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 事务隔离事务就是要保证一组数据库操作，要么全部成功，要么全部失败。提到事务，我们会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 SQL 标准的事务隔离级别包括： 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 隔离级别相关SQL： 12345# 查看当前事务隔离级别show variables like &apos;transaction_isolation&apos;;# 修改隔离级别将启动参数 transaction-isolation 的值设置成 对应的级别 如：READ-COMMITTED 隔离级别的实现原理实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 这里展开说明”可重复读”：在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从 1 被按顺序改成了 2、3、4，他们创建的视图分别是read-view A、read-view B、read-view C，在视图 A、B、C 里面，这一个记录的值分别是 1、2、3，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 事务回滚日志只有再在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 怎么避免使用长事务： 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间 事务的启动方式 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 查询长事务的SQL语句： 12# 查找持续时间超过 60s 的事务select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 索引索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。 索引的常见模型 哈希表：哈希表这种结构适用于只有等值查询的场景，但是对于区间查找，因为Hash表示无序的，这样就需要全部都扫描一遍进行对比。 有序数组：在等值查询和范围查询场景中的性能就都非常优秀。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。 二叉搜索树：二叉树为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log(N))。你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间，这样就会使用很长的时间。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。 InnoDB 的索引模型在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。 基于主键索引和普通索引的查询有什么区别？基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 索引维护B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。B+树每个节点维护的都是数据页，当我们需要插入一条记录的时候，只需要将这条记录找到索引应该对应的数据页，放入行中即可，但是我们插入的位置需要挪动数据页相对位置后面的数据的时候，就比较麻烦了（比如：数据页中原数据是 …3,5… ，但是我们插入的数据是4，则需要将5以及5后面的内容都移动一下空出位置），而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 哪些场景应该使用自增主键，哪些场景不应该使用： 使用自增主键的好处：插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 从存储空间来考虑：设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 适合用业务字段作为主键的场景： 只有一个索引; 该索引必须是唯一索引; 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。 普通索引的查询逻辑普通索引使用的查询逻辑是，先在普通索引上找到符合要求的索引节点，从节点拿到主键的值，然后我们通过主键的值去主键索引上拿到所有数据的值，这个过程叫做回表。 由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？ 覆盖索引如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 很多时候，我们建立联合索引就是利用的覆盖索引这一个概念，比如我们要查询 a &gt; 0 AND b &gt; 1，这时候我们如果只对a建立索引，那么b的值判断我们就需要通过回表的方式，去主键索引中拿到b的值，然后判断，这就耗费了大量的时间，但是我们建立了联合索引，就可以直接在此联合索引上先判断a的值，然后判断b的值，不需要通过回表，能大量提升效率。 最左前缀原则B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录，包括联合索引。 示例： （如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是”where name like ‘张 %’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。） 在建立联合索引的时候，如何安排索引内的字段顺序： 上一节覆盖索引的示例中，我们建立的a b联合索引，如果单独只查询b，这样是不走索引的，因为最左匹配原则 第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 索引下推索引下推其实就是我们上述联合索引覆盖索引的具体应用原理，就是联合索引 a b，然后查询a &gt; 0 AND b &gt; 1，减少回表次数，这就是索引下推的原理。 锁数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 全局锁全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。 但是让整库都只读，听上去就很危险： 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆。 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 但是我们在全库备份的时候，如果不加全局锁，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 一致性读是好，但前提是引擎要支持这个隔离级别，如果是一些不支持这个隔离级别的引擎，就还是需要全局锁来实现全库备份，所以，single-transaction 方法只适用于所有的表使用事务引擎的库。 既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因： 一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。 二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 表级锁MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁：语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 元数据锁：MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 虽然 MDL 锁是系统默认会加的，但却是你不能忽略的一个机制。所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。 表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是： 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎； 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。 Online DDL: Online DDL的过程是这样的： 拿MDL写锁 降级成MDL读锁 真正做DDL 升级成MDL写锁 释放MDL锁 1、2、4、5如果没有锁冲突，执行时间非常短。第3步占用了DDL绝大部分时间，这期间这个表可以正常读写数据，是因此称为“online ” 行锁MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。 两阶段锁的概念：在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。所以我们在使用锁的时候需要注意：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 死锁的解决策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁，这个负担在高并发的时候是很大的 事务和行锁的关系事务的启动时机begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 第一种启动方式，一致性视图是在执行第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。 一致性视图：是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 “快照”在 MVCC 里的工作原理在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的，实际上，这个快照并不需要拷贝出全库的数据，它的实现如下： InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。 注意：*事务启动的时候还要保存“现在正在执行的所有事物ID列表”，如果一个row trx_id在这列表中，也要不可见，这就保证了一个事务的row trx_id比较大，还有比它小的row trx_id还未提交，这样比它小的row trx_id的数据同样不可见。 按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view），这个视图数组把所有的 row trx_id 分成了几种不同的情况。 已提交事务 - （低水位） - 未提交事务集合 - （高水位） - 未开始事务 InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。（具体来说就是，一致性读会读取到当前数据的真实值，然后通过这个水位，找到当前事务之后提交的数据的 undo log，然后回放 undo log 获取到当时的数据，这样来保证一致性读） 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见； 更新逻辑一个非常非常重要的概念更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。意味着我们更新数据的时候，读的数据就不能从事务视图中拿了，必须直接拿当前的值，否则其他事务所更新的数据就有可能被丢失。 基于此逻辑，我们的写锁就与事务结合了起来：如果我们在写数据的时候不通过当前读，那就会导致每个事务都只更新自己的数据，还要锁干什么，只有通过当前读，我们才会对当前内存中的数据进行加锁，这样保证每个事务在更新数据的时候都是串行的，两阶段锁协议保证每个事务的修改只有再提交或者回滚后，下一个事务的更新操作才能获取到锁，才能进行更新数据，每个事务更新的数据才不会丢失，到这里，我们把一致性读、当前读和行锁就串起来了。 事务的可重复读的能力是怎么实现的？可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/categories/MySQL/"}],"tags":[{"name":"入门","slug":"入门","permalink":"https://jjw-story.github.io/tags/入门/"},{"name":"MySQL","slug":"MySQL","permalink":"https://jjw-story.github.io/tags/MySQL/"}],"author":"JJW"},{"title":"Kebernetes-进阶","slug":"Kebernetes-进阶","date":"2020-06-30T12:12:20.000Z","updated":"2020-07-10T12:50:14.601Z","comments":true,"path":"2020/06/30/Kebernetes-进阶/","link":"","permalink":"https://jjw-story.github.io/2020/06/30/Kebernetes-进阶/","excerpt":"","text":"Kubectl连接K8s集群配置（Cluster配置）在我们搭建好K8s集群后，需要通过Kubectl 客户端连接集群并进行相关集群管控操作，一般我们只需要安装kubectl工具，然后将集群中生成的config文件，拷贝到我们需要连接集群的电脑的 ${HOME}/.kube/config 目录下，这样就可以操作K8s集群了。 但是有的时候我们一台客户端电脑需要连接多个K8s集群，这时我们就需要将不同的config文件合并在一起，以方便我们操作。 本节核心命令： kubectl config get-contexts:获取客户端链接的所有上下文 kubectl config get-clusters:获取客户端连接的所有配置cluster kubectl config use-context [context的名称]：切换当前集群的连接 kubectl get node -o wide:查看集群节点的详细信息 kubectl describe node [上述命令查询出的节点名称]:查看某个节点的详细信息 例如我们原有的minikube集群中的config文件如下： 12345678910111213141516171819apiVersion: v1clusters:- cluster: certificate-authority: /home/jjw/.minikube/ca.crt server: https://172.17.0.3:8443 name: minikubecontexts:- context: cluster: minikube user: minikube name: minikubecurrent-context: minikubekind: Configpreferences: &#123;&#125;users:- name: minikube user: client-certificate: /home/jjw/.minikube/profiles/minikube/client.crt client-key: /home/jjw/.minikube/profiles/minikube/client.key 我们现在合并另外一个集群的配置文件如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 指定版本apiVersion: v1# 配置clusters，这里就是可以配置连接多个K8s的集群clusters:- cluster: certificate-authority: /home/jjw/.minikube/ca.crt server: https://172.17.0.3:8443 name: minikube- cluster: certificate-authority-data: DATA+OMITTED server: https://172.19.8.113:6443 name: jplocal# 配置context，contexts:# 第一个- context: cluster: minikube user: minikube name: minikube# 第二个- context: cluster: jplocal user: kube-admin-local name: local# 配置默认的连接current-context: minikubekind: Configpreferences: &#123;&#125;# 配置连接集群的uers信息，包含身份认证信息等users:- name: minikube user: client-certificate: /home/jjw/.minikube/profiles/minikube/client.crt client-key: /home/jjw/.minikube/profiles/minikube/client.key- name: kube-admin-local user: client-certificate-data: REDACTED client-key-data: REDACTED 配置完成后我们就可以通过命令查看我们的配置上下文等信息，如下： 12345678910# 注意：带*号表示是我们当前使用连接的contextjjw@jjw-PC:~/.kube$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE local jplocal kube-admin-local * minikube minikube minikube jjw@jjw-PC:~/.kube$ kubectl config get-clustersNAMEminikubejplocal 切换连接到名称为local的集群： 123456789# 切换jjw@jjw-PC:~/.kube$ kubectl config use-context localSwitched to context &quot;local&quot;.# 查看切换结果，发现切换成功jjw@jjw-PC:~/.kube$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* local jplocal kube-admin-local minikube minikube minikube 这样我们就可以操作我们的K8s集群了，例如完整如下： 123456789101112# 获取当前local集群的节点信息，获取不到是因为这个配置是我瞎写的jjw@jjw-PC:~/.kube$ kubectl get nodeerror: tls: failed to find any PEM data in certificate input# 切换成之前的minikube集群jjw@jjw-PC:~/.kube$ kubectl config use-context minikubeSwitched to context &quot;minikube&quot;.# 能够获取到节点信息jjw@jjw-PC:~/.kube$ kubectl get nodeNAME STATUS ROLES AGE VERSIONminikube Ready master 15d v1.18.3 K8s集群网络（Cluster Networking）K8s集群间网络现象及原理当我们搭建了一套K8s集群后，我们在节点中创建Pod，并且查询出此Pod的IP地址，这时我们在任何一个节点或者任何一个节点的Pod中，我们ping这个IP地址，我们发现是都可以ping通的，这就是K8s集群中我们要讲的网络。 在之前的Swarm集群中，我们也有同样的现象，我们知道原理是它们通过一个Overlay的Network实现的，其实K8s集群也是这样，只不过这里是通过某个插件来实现的Overlay网络，远离同样是将Pod的网络代理到机器网关上，类似eth1，然后通过此网关来解析代理。 具体的插件有很多种（例如flannal网络插件），我们可以去K8s官网上查看，这些插件都需要遵循K8s网络的规定或者说协议： 所有的容器跟其他所有的容器之间可以直接通信，并且不需要NAT的转化 所有的Node都可以直接与其他Node的容器进行直接通信，并且不需要NAT的转化 我们的这个容器的IP地址是什么，那么其他容器或者节点就可以直接通过这个IP来访问这个容器 本节核心命令： kubectl get svc:获取集群上所有的Services信息 kubectl get pods –show-labels:查看集群中所有Pod的Label kubectl expose pods [pod名称]:将已有Pod导出创建Service kubectl expose depolyment [depolyment名称]：将已有depolyment导出创建为Service kubectl create -f [service的yml文件名称]：通过yml文件创建service kubectl edit depolyment [depolyment名称]：创建service后，直接编辑此depolymet升级此depolyment kubectl apply -f [新的depolyment yml文件] –record:创建service滚动升级depolyment kubectl delete svc/[service名称]:删除service kubectl delete -f [service的yml文件名称]:通过文件删除service K8s集群中应用对外提供网络Service在上述现象中，我们所描述的Pod提供的IP都只是在K8s集群中可以互访，但是我们部署应用后，需要对外界提供服务，这样我们就需要将网络暴露给外界，这时应该怎么做呢？ 首先我们看看在K8s集群中直接使用和管理Pod的缺点： 当我们使用ReplicaSet或者ReplicationController做水平扩展scale的时候，Pods就有可能被终止掉 当我们使用Depolyment的时候，我们去更新Docker Image Version ，旧的Pod就会被终止，然后创建新的Pod 当我们使用上述两种方式做扩展，如果我们直接管理的是Pod，对外提供的是Pod的IP，那么我们在扩展后，Pod的IP地址就会发生变化，这时我们外界再通过之前的IP就不能访问我们的Pod中提供的服务了。或者是说我们在Pod中部署了数据库服务，其他集群中的Pod可以访问，但是我们对数据库进行升级的时候，这样其他集群中的服务就不能访问数据库了，因为在升级的过程中，数据库的IP地址就发生了变化 为了解决上述问题，我们同样可以使用之前学习过的Service来解决，K8s中管理最常用的也是Service，使用Service我们可以管理多个Pod，并且将这些Pod集中管理起来，可以随意扩展或降级升级，并且保证我们对外提供的IP地址是一个，且不发生变化，而且有负载均衡的作用。 Service主要有三种类型： ClusterIP：这个IP地址是Cluster可以访问的，及K8s集群中任何一个节点都可以访问，但是外界不能访问，这个IP是不会变的，是Service所对应的IP地址，它会代理到我们service中的Pod上，Pod的IP地址是可以变化的，一般应用与只在系统间内部可以访问，比如数据库服务 NodePort：这个Service是对外提供IP地址，这是与ClusterIP类型的Service最大的不同 LoadBalancer：这个一般需要云服务商提供，我们通过这个LoadBalancer就可以访问集群中的Pod，并且提供负载均衡 Service的创建方式： 通过kubectl expose 命令，会给我们的Pod创建一个service，供外部访问 定义一个yml文件，我们在此文件中描述Service的具体信息及资源 创建ClusterIP类型Servcice示例 通过Pod直接创建Service 1234567891011121. 首先创建好Podkubectl create -f [yml文件的路径名称]2. 查看我们创建的Pod的具体信息，包括pod的名称和IP等kubectl get pods -o wide3. 根据pod的名称，创建servicekubectl expose pods [pod名称]4. 查看我们创建的service的具体信息，这里会查看到我们创建的service的IP地址和端口，这里我们查看到的Service的IP地址是不会变化的，而上面我们查看到的Pod的IP地址是会随着服务的扩展升级等发生变化注意：这里查询出的ServiceIP只能在K8s集群内部访问，因为我们创建的是一个ClusterIP类型的Servicekubectl get svc 通过Depolyment创建Service，测试升级或者扩展此应用，Service IP不发生变化 12345678910111213141516171819202122232425262728293031323334353637381. 首先我们创建一个Depolyment，此Depolyment中Pod的个数可以是多个kubectl create -f [deployment的yml文件]jjw@jjw-PC:~$ kubectl create -f deployment_nginx.yml deployment.apps/nginx-deployment created2. 查看我们创建好的pod的具体信息kubectl get pods -o widejjw@jjw-PC:~$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-deployment-5cc6c7559b-j5v57 1/1 Running 0 16s 172.18.0.5 minikube &lt;none&gt; &lt;none&gt;nginx-deployment-5cc6c7559b-t5z92 1/1 Running 0 16s 172.18.0.4 minikube &lt;none&gt; &lt;none&gt;nginx-deployment-5cc6c7559b-wfbts 1/1 Running 0 16s 172.18.0.6 minikube &lt;none&gt; &lt;none&gt;3. 查看我们此depolyment的具体信息，包括depolyment的名称kubectl get depolyment4. 通过此depolyment创建Servicekubectl expose depolyment [depolyment名称]jjw@jjw-PC:~$ kubectl expose deployment nginx-deploymentservice/nginx-deployment exposed5. 查看我们创建的service，这时，我们的Service的IP信息也就被查看到了kubectl get svcjjw@jjw-PC:~$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 19dnginx-deployment ClusterIP 10.96.161.226 &lt;none&gt; 80/TCP 26s6. 然后我们访问此IP，发现是可以访问到我们Pod提供的服务的，并且它还实现了负载均衡的功能7. 升级此depoluyment，我们可以直接编辑此depolyment来实现升级，使用命令kubectl edit depolyment [depolyment名称]8. 编辑完此depolyment要升级的内容后，我们的服务就会自动升级9. 查看现在的Pod信息，发现pod已经不是我们之前创建的了，而是新的，但是这时我们的Service IP还是没有变化，还是可以通过此IP访问到服务kubectl get pods -o wide 这里我们的升级它不是零宕机的升级，它还是会中断一会，我们可以通过其他方法实现滚动升级，即优雅上下线升级 滚动升级的实现其实就是我们重新定义一个yml，及针对与之前depolyment的yml的升级后的yml，然后通过命令： kubectl apply -f [新的depolyment yml文件] –record 这样就可以实现滚动升级，如果不加 –record 参数，则旧的depolymeng不会被删除，它还会存在，只是Pod数变成了0 创建NodePort类型Servcice示例 通过Pod直接创建Service 1234567891011121. 首先创建好Podkubectl create -f [yml文件的路径名称]2. 查看我们创建的Pod的具体信息，包括pod的名称和IP等kubectl get pods -o wide3. 根据pod的名称，指定参数，创建NodePort类型Servicekubectl expose pods [pod名称] --type=NodePort4. 查看我们创建的service的具体信息注意：这里也会查询出ClusterIP，但是在端口信息上，与ClusterIP类型的Service不同，它会指定一个IP映射（80:31404/TCP），这里就表示我们将Pod的端口映射到了主机上，我们可以通过此映射及Port通过主机的IP来访问此Pod服务。这样如果我们的主机是有公网IP的，那么我们这个服务就可以直接暴露给外界来使用kubectl get svc 通过Depolyment创建NodePort类型的Service 创建方式与上述创建ClusterIP类型的Service一样，不同的是在创建的命令中指定–type=NodePort即可 kubectl expose depolyment [depolyment名称] –type=NodePort即可 通过Service.yml文件创建Service 通过yml文件创建service，我们首先要创建好Pod，然后指定Pod的Label，如下Pod的yml文件，nginx-pod.yml 123456789101112131415apiVersion: v1kind: Podmetadata: name: nginx-pod # 注意：这里我们指明了Pod的Label labels: app: nginxspec: containers: - name: nginx-container image: nginx ports: - name: nginx-port containerPort: 80 创建service的yml文件，如下service_nginx.yml 1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: # 指定端口映射，这里表示将Pod的端口映射为本地的8080和NodePort的8080 ports: - port: 32333 nodePort: 32333 # 这里就表示我们创建service要引用的Pod的，这里指定的是Pod的Label，所以我们创建service的文件是用Label来确定具体Pod的 targetPort: nginx-port protocol: TCP selector: app: nginx type: NodePort 创建service 1kubectl create -f service_nginx.yml 创建LoadBalance类型Servcice示例创建LoadBalance类型的Servcie需要借助于云服务，所以这里不再演示，比较复杂，具体用的时候查资料或者视频即可 具体创建命令： kubectl expose pods [pod名称] –type=LoadBalance kubectl expose depolyment [depolyment名称] –type=LoadBalance secret使用secret的使用与Swarm很类似，具体就是先在集群中创建secret，然后在service的yml文件中要使用的地方引入此secret名称即可 创建secret： kubectl create secret generic [secret名称] –from-literal=[key]=[value]：通过命令直接指定创建 kubectl create secret generic [secret名称] –from-file=[file路径]：通过编辑好的文件创建 示例如下： 12# 创建一个名称为mysql-pass的 键为password 值为jjw0923的secretkubectl create secret generic mysql-pass --from-literal=password=jjw0923 然后在service的yml文件引入即可，具体使用查资料","categories":[],"tags":[{"name":"Kebernetes","slug":"Kebernetes","permalink":"https://jjw-story.github.io/tags/Kebernetes/"},{"name":"进阶","slug":"进阶","permalink":"https://jjw-story.github.io/tags/进阶/"}],"author":"JJW"},{"title":"Kubernetes","slug":"Kubernetes","date":"2020-06-14T05:14:20.000Z","updated":"2020-07-10T12:50:14.433Z","comments":true,"path":"2020/06/14/Kubernetes/","link":"","permalink":"https://jjw-story.github.io/2020/06/14/Kubernetes/","excerpt":"","text":"Kubernetes介绍Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。它与Swarm是类似的，并且是竞争产品。Swarm是Docker公司开发内置的，K8s是一个专门的社区，在2017年的时候，Docker公司宣布支持K8s，因为K8s的社区等都比较活跃及完善，所以这场竞争中，是K8s胜利了。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… Kubernetes解决的问题： 调度 - 容器应该在哪个机器上运行 生命周期和健康状况 - 容器在无错的条件下运行 服务发现 - 容器在哪，怎样与它通信 监控 - 容器是否运行正常 认证 - 谁能访问容器 容器聚合 - 如何将多个容器合并成一个工程 Kubernetes架构及组件Kubernetes属于主从分布式架构，主要由Master Node和Worker Node组成，以及包括客户端命令行工具kubectl和其它附加项（Add on）。 Master Node作为控制节点，对集群进行调度管理（它类似于Swarm的Manager节点），它是K8s集群的大脑。Master Node由API Server、Scheduler、Cluster State Store和Controller-Manger Server所组成。 API ServerAPI Server是暴露给外接访问的，我们可以通过UI或者CLI通过API Server去跟集群进行交互。 API Server主要用来处理REST的操作，确保它们生效，并执行相关业务逻辑，以及更新etcd（或者其他存储）中的相关对象。API Server是所有REST命令的入口，它的相关结果状态将被保存在etcd（或其他存储）中。 另外，API Server也作为集群的网关。默认情况，客户端通过API Server对集群进行访问，客户端需要通过认证，并使用API Server作为访问Node和Pod（以及service）的堡垒和代理/通道。 SchedulerScheduler是一个调度模块，比如我们通过API Server创建一个应用，这个应用有两个容器，那么这两个容器到底要部署在哪个节点上，这就是通过Scheduler模块来进行一些算法来确认的。 scheduler组件为容器自动选择运行的主机。依据请求资源的可用性，服务请求的质量等约束条件，scheduler监控未绑定的pod，并将其绑定至特定的node节点。Kubernetes也支持用户自己提供的调度器，Scheduler负责根据调度策略自动将Pod部署到合适Node中，调度策略分为预选策略和优选策略，Pod的整个调度过程分为两步： 预选Node：遍历集群中所有的Node，按照具体的预选策略筛选出符合要求的Node列表。如没有Node符合预选策略规则，该Pod就会被挂起，直到集群中出现符合要求的Node。 优选Node：预选Node列表的基础上，按照优选策略为待选的Node进行打分和排序，从中获取最优Node。 Controller-Manager Server控制管理服务器，主要是控制容器的负载均衡，或者对容器的横向扩展，比如增加节点等。 它用于执行大部分的集群层次的功能，它既执行生命周期功能(例如：命名空间创建和生命周期、事件垃圾收集、已终止垃圾收集、级联删除垃圾收集、node垃圾收集)，也执行API业务逻辑（例如：pod的弹性扩容）。控制管理提供自愈能力、扩容、应用生命周期管理、服务发现、路由、服务绑定和提供。Kubernetes默认提供Replication Controller、Node Controller、Namespace Controller、Service Controller、Endpoints Controller、Persistent Controller、DaemonSet Controller等控制器。 etcd（Cluster state store）集群状态存储，Kubernetes默认使用etcd作为集群整体存储，当然也可以使用其它的技术。etcd是一个简单的、分布式的、一致的key-value存储，主要被用来共享配置和服务发现。etcd提供了一个CRUD操作的REST API，以及提供了作为注册的接口，以监控指定的Node。集群的所有状态都存储在etcd实例中，并具有监控的能力，因此当etcd中的信息发生变化时，就能够快速的通知集群中相关的组件。 Worker Node作为真正的工作节点，运行业务应用的容器；Worker Node包含kubelet、kube proxy和Fluented、Container Runtime。 Pod在Kubernets中，Pod作为基本的执行单元，在Kubernetes中，最小的管理元素不是一个个独立的容器，而是Pod，Pod是最小的，管理，创建，计划的最小单元。Pod指的是具有相同的Name Space（这里的Name Space包含了所有的，最重要的是Network Name Space）的一些Container的组合，容器可能有多个，如果是多个，它们之间共享一个Network Name Space所以它可以拥有多个容器和存储数据卷，能够方便在每个容器中打包一个单一的应用，从而解耦了应用构建时和部署时的所关心的事项，已经能够方便在物理机/虚拟机之间进行迁移。 KubeletKubelet是Kubernetes中最主要的控制器，它是Pod和Node API的主要实现者，Kubelet负责驱动容器执行层。在Kubernetes中，应用容器彼此是隔离的，并且与运行其的主机也是隔离的，这是对应用进行独立解耦管理的关键点。 API准入控制可以拒绝或者Pod，或者为Pod添加额外的调度约束，但是Kubelet才是Pod是否能够运行在特定Node上的最终裁决者，而不是scheduler或者DaemonSet。kubelet默认情况使用cAdvisor进行资源监控。负责管理Pod、容器、镜像、数据卷等，实现集群（Manager）对节点的管理，并将容器的运行状态汇报给Kubernetes API Server。 kube proxy它是跟网络有关的，（总结就是例如一个service有多个容器，我们对这个service的所有容器提供一个公共的IP，并且实现负载均衡），基于一种公共访问策略（例如：负载均衡），服务提供了一种访问一群pod的途径。此方式通过创建一个虚拟的IP来实现，客户端能够访问此IP，并能够将服务透明的代理至Pod。每一个Node都会运行一个kube-proxy，kube proxy通过iptables规则引导访问至服务IP，并将重定向至正确的后端应用，通过这种方式kube-proxy提供了一个高可用的负载均衡解决方案。服务发现主要通过DNS实现。 在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。 Container Runtime每一个Node都会运行一个Container Runtime，其负责下载镜像和运行容器。这里容器我们一般选择Docker，当然也可以选择其他产品。 Fluented主要是用于日志的采集、存储、查询等。 Add-onAdd-on是对Kubernetes核心功能的扩展，例如增加网络和网络策略等能力，在Kunbernetes中可以以附加项的方式扩展Kubernetes的功能，目前主要有网络、服务发现和可视化这三大类的附加项。 kubectlkubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。它用于通过命令行与API Server进行交互，而对Kubernetes进行操作，实现在集群中进行各种资源的增删改查等操作。命令的语法如下所示：kubectl [command] [TYPE] [NAME] [flags] Kubernates安装 Kubernates-the-hard-way：最基础的方式，一步一步的安装各种组件，然后安装完成K8s集群，困难度很高 minikube：是一个工具，它能快速在我们本地创建一个只有一个节点的K8s集群 kuberadm：是一个工具，它能快速在我们本地搭建一个有多个节点的K8s集群 kops：快速的在云上创建K8s集群，比如AWS等，操作比较简单 Tectonic：企业版的一个工具，节点数量比较少，免费，大于十个节点是收费的 Play Way Kubernates：在云上快速搭建一个K8s集群，但是它有时间限制，四个小时后会自动销毁 具体的搭建方法可以Google或者Baidu 我们这里使用minikube的方式来创建。 安装完成我们就可以通过一些基础命令开查看K8s集群的状态了，例如： kubectl config view：查看当前config的基本情况，包括Api的IP地址和端口，以及认证信息，context的名字等 kubectl config get-contexts：查看当前的context kubectl cluster-info：查看当前K8s集群的基本情况，节点信息，类似于swarm中的 docker node 命令 使用minikube创建的k8s集群我们可以通过 minikube ssh 命令进入到虚拟机中，在这里面我们可以做具体的操作，比如直接操作docker等 Pod详解及基本操作命令K8s我们不直接对container进行操作，pod是我们的基本操作单元。之前我们说过，一个Pod中的容器是共享一个Namespace的，他们之间是可以直接通信的。 基础操作创建Pod，K8s中我们创建Pod需要通过一个yml文件来创建，这个yml文件有点类似于我们之前学习docker-compose和swarm的stack的文件，以下是一个示例文件： 12345678910111213141516171819202122# api的版本apiVersion: v1# yml锁定义的内容的类型，这里是一个Podkind: Pod# 一些基本的元数据metadata: name: nginx-pod labels: app: nginx# 最重要的部分，它是直接定义容器的spec: # containers表示我们可以定义多个容器，这里只写了一些 containers: - name: nginx-container image: nginx ports: - name: nginx-port containerPort: 80 kubectl create -f [yml文件的路径名称]： 创建Pod kubectl delete -f [yml文件的路径名称]： 删除已经创建的Pod kubectl get pods：查看当前集群中所有的Pod及状态信息等 kubectl get pods -o wide：查看当前集群中所有的Pod及状态信息，包含的信息更全面一些，包含了容器的IP信息和运行的节点等 kubectl delete pods [Pod名称]：删除指定的Pod kubectl describe [Pod名称]：查看Pod的详细信息，包含名称，节点信息，Namespace，所有的Container的详细信息，我们的Pod中的具体容器信息也能显示出来，这里我们就能够查到容器的Iamge、IP、端口、容器ID、状态等信息 kubectl exec -it [Pod名称] -c [Pod中Container名称] /bin/bash：进入Pod中的具体某一个容器，如果我们这里不使用 -c 参数，默认是进入第一个容器中 kubectl port-forward [Pod名称] [本地端口:Pod端口]：端口映射，类似于将容器的端口映射到宿主机上，此命令如果停止我们就无法访问 kubectl get nodes: 查询K8s集群中所有的节点信息 ReplicationControllerReplication Controller简称RC，RC是Kubernetes系统中的核心概念之一，简单来说，RC可以保证在任意时间运行Pod的副本数量，能够保证Pod总是可用的。如果实际Pod数量比指定的多那就结束掉多余的，如果实际数量比指定的少就新启动一些Pod，当Pod失败、被删除或者挂掉后，RC都会去自动创建新的Pod来保证副本数量，所以即使只有一个Pod，我们也应该使用RC来管理我们的Pod。可以说，通过ReplicationController，Kubernetes实现了集群的高可用性。 核心命令： kubectl create -f [rc的yml路径名称]：根据yml文件创建ReplicationController应用 kubectl get rc：查看我们K8s集群所有的rc的简单信息 kubectl get rc -o wide：查看我们K8s集群所有的rc的详细一些信息 kubectl scala rc [rc名称] –replicas=[要扩展后或回收后的Pod总数量]：扩展或者回收rc应用的pod kubectl delete -f [rc的yml路径名称]：根据yml文件删除ReplicationController应用 ReplicationController和Pod一样，都是Kubernetes中的对象，因此创建方式类似。通过yaml或json描述文件来定义一个ReplicationController对象。一个最简单的ReplicationController的定义如下 rc-nginx.yml文件： 12345678910111213141516171819apiVersion: v1kind: ReplicationControllermetadata: name: nginxspec: # 指定副本数，这里指定为3，那么K8s就会创建三个Pod replicas: 3 template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 基于此文件，我们就可以通过命令： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 1.通过rc-nginx.yml来创建我们的应用jjw@jjw-PC:~$ kubectl create -f rc-nginx.ymlreplicationcontroller/nginx created# 2.查看我们K8s集群创建的此rc的简单信息jjw@jjw-PC:~$ kubectl get rcNAME DESIRED CURRENT READY AGEnginx 3 3 0 32s# 3.查看集群中所有Pod的具体信息，这里有三个Pod，因为我们在rc-nginx.yml文件中指定了replicas为3个jjw@jjw-PC:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-5p7s2 1/1 Running 0 61snginx-ckph9 1/1 Running 0 61snginx-rs8qk 1/1 Running 0 61s# 4.我们尝试通过命令删除上述查询出的其中一个Podjjw@jjw-PC:~$ kubectl delete pods nginx-5p7s2pod &quot;nginx-5p7s2&quot; deleted# 5.删除之后我们再次查看所有Pod，发现有四个，其中一个的状态为Terminating，然后新增了一个，一共三个是Runnig的状态，说明我们通过rc创建的Pod，它能帮我们维持Pod的数目，这里与Swarm中的stack是一样的jjw@jjw-PC:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-ckph9 1/1 Running 0 51snginx-rs8qk 1/1 Running 0 51snginx-6w6zs 1/1 Running 0 4m1s# 6.我们通过scala将此rc的Pod数量降为2个jjw@jjw-PC:~$ kubectl scale rc nginx --replicas=2replicationcontroller/nginx scaled# 7.然后查看rc的具体信息，发现该nginx rc只有两个了jjw@jjw-PC:~$ kubectl get rcNAME DESIRED CURRENT READY AGEnginx 2 2 2 7m22s# 8.我们查看每一个Pod的具体信息，它们的IP地址是不同的jjw@jjw-PC:~$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ckph9 1/1 Running 0 7m45s 172.18.0.6 minikube &lt;none&gt; &lt;none&gt;nginx-rs8qk 1/1 Running 0 7m45s 172.18.0.5 minikube &lt;none&gt; &lt;none&gt;# 9.我们将其中一个pod的端口映射到当前机器上jjw@jjw-PC:~$ kubectl port-forward nginx-ckph9 8080:80Forwarding from 127.0.0.1:8080 -&gt; 80Forwarding from [::1]:8080 -&gt; 80# 10.通过在外部访问，发现我们的8080端口能正确访问映射到pod的80端口jjw@jjw-PC:~$ curl 127.0.0.1:8080&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;# 11.删除此ReplicationControllerjjw@jjw-PC:~$ kubectl delete -f rc-nginx.ymlreplicationcontroller &quot;nginx&quot; deleted ReplicaSet在新版本的 Kubernetes 中建议使用 ReplicaSet（简称为RS ）来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector（ReplicationController 仅支持等式） 核心命令（与ReplicationController的核心命令大概相同）： kubectl create -f [rs的yml路径名称]：根据yml文件创建ReplicaSet应用 kubectl get rs：查看我们K8s集群所有的rs的简单信息 kubectl get rs -o wide：查看我们K8s集群所有的rs的详细一些信息 kubectl scale rs [rs名称] –replicas=[要扩展后或回收后的Pod总数量]：扩展或者回收rs应用的pod kubectl delete -f [rs的yml路径名称]：根据yml文件删除ReplicaSet应用 下面是rs文件示例，rs-nginx.yml: 123456789101112131415161718192021222324apiVersion: apps/v1kind: ReplicaSetmetadata: name: nginx # 注意 labels的定义与RC有所不同 labels: tier: frontendspec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: name: nginx labels: tier: frontend spec: containers: - name: nginx image: nginx ports: - containerPort: 80 具体的操作我们发现与RC是没有什么区别的，我们可以通过上面的示例，替换一下命令来测试一下，如下： 12345678910111213141516171819202122232425262728jjw@jjw-PC:~$ kubectl create -f rs-nginx.yml replicaset.apps/nginx createdjjw@jjw-PC:~$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx 3 3 0 10sjjw@jjw-PC:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-7zzbg 1/1 Running 0 28snginx-nzsrh 1/1 Running 0 28snginx-zhzbk 1/1 Running 0 28sjjw@jjw-PC:~$ kubectl scale rs nginx --replicas=4replicaset.apps/nginx scaledjjw@jjw-PC:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-7zzbg 1/1 Running 0 91snginx-kp4fz 1/1 Running 0 15snginx-nzsrh 1/1 Running 0 91snginx-zhzbk 1/1 Running 0 91sjjw@jjw-PC:~$ kubectl delete -f rs-nginx.yml replicaset.apps &quot;nginx&quot; deletedjjw@jjw-PC:~$ kubectl get podsNo resources found in default namespace. DeploymentDeployment为Pod和Replica Set提供声明式更新。（个人理解最大的作用就是更新我们的应用用的，它用来替换我们上面学习的三种创建Pod的方式，它底层使用的还是ReplicaSet，类似与对ReplicaSet又做了一层高级的封装）你只需要在 Deployment 中描述您想要的目标状态是什么，Deployment controller 就会帮您将 Pod 和ReplicaSet 的实际状态改变到您的目标状态。您可以定义一个全新的 Deployment 来创建 ReplicaSet 或者删除已有的 Deployment 并创建一个新的来替换。注意：不能手动管理由 Deployment 创建的 Replica Set，否则就篡越了 Deployment controller 的职责！ 核心命令（与ReplicaSet的核心命令大概相同）： kubectl create -f [deployment的yml路径名称]：根据yml文件创建deployment应用 kubectl get deployment：查看我们K8s集群所有的deployment：的简单信息 kubectl get deployment -o wide：查看我们K8s集群所有的deployment的详细一些信息 kubectl set image deployment [deployment名称] [要升级的image名称]=[升级的image版本]：通过命令行直接指定升级的image来升级我们的deployment pod应用 kubectl rollout history deployment [deployment名称]：查看我们的deployment的历史版本，这里默认只有当前和上一个两个版本 kubectl rollout undo deployment [deployment名称]：回滚deployment应用的版本至上一个版本 kubectl delete -f [deployment的yml路径名称]：根据yml文件删除Deployment应用 下面我们通过一个示例来讲解： 首先定义deployment的yml文件，文件内容如下（deployment-nginx.yml） 123456789101112131415161718192021222324252627# 定义的版本，因为它底层使用的是Replica SetapiVersion: apps/v1# 定义类型为Deploymentkind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx # 定义我们nginx的版本，这里故意使用一个低版本，便于我们演示升级Pod image: nginx:1.12.2 ports: - containerPort: 80 下面演示具体的操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 通过此yml文件创建deploymentjjw@jjw-PC:~$ kubectl create -f deployment_nginx.yml deployment.apps/nginx-deployment created# 查看我们K8s集群创建的deployment的简单信息jjw@jjw-PC:~$ kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 3/3 3 3 50s# 通过命令查看k8s集群中的所有ReplicaSet，我们发现有一个，而且它的名字是我们deployment的名字加上一段字符jjw@jjw-PC:~$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-5cc6c7559b 3 3 3 3m44s# 查看所有的Pod信息，我们发现pod的名称又是rs的名称加上一段字符jjw@jjw-PC:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-5cc6c7559b-c8l5c 1/1 Running 0 5m8snginx-deployment-5cc6c7559b-cqdzg 1/1 Running 0 5m8snginx-deployment-5cc6c7559b-llzgk 1/1 Running 0 5m8s# 查看我们K8s集群创建的deployment的详细信息，这里我们的image版本是1.12.2jjw@jjw-PC:~$ kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 11m nginx nginx:1.12.2 app=nginx# 通过命令行指定image的版本，升级我们的deployment应用jjw@jjw-PC:~$ kubectl set image deployment nginx-deployment nginx=nginx:1.13deployment.apps/nginx-deployment image updated# 再次查看我们K8s集群创建的deployment的详细信息，发现这里我们的image版本已经升级为1.13jjw@jjw-PC:~$ kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 14m nginx nginx:1.13 app=nginx# 然后我们查看ReplicaSet信息，发现之前的rs DESIRED 已经变成了0个，我们又新创建了一个rsjjw@jjw-PC:~$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-5cc6c7559b 0 0 0 15mnginx-deployment-76d7bfdc99 3 3 3 2m12s# 查看pod信息，发现在运行的有三个，但是都是全新的与之前不一样jjw@jjw-PC:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-76d7bfdc99-9rkbg 1/1 Running 0 3m18snginx-deployment-76d7bfdc99-fhm5f 1/1 Running 0 3m38snginx-deployment-76d7bfdc99-zprtj 1/1 Running 0 3m12s# 查看历史版本信息jjw@jjw-PC:~$ kubectl rollout history deployment nginx-deploymentdeployment.apps/nginx-deployment REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;# 使用命令回滚到上一个版本jjw@jjw-PC:~$ kubectl rollout undo deployment nginx-deploymentdeployment.apps/nginx-deployment rolled back# 查看deployment的信息，发现image的版本恢复到了1.12.2jjw@jjw-PC:~$ kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 22m nginx nginx:1.12.2 app=nginx# 再次查看历史版本信息，发现序号是递增的jjw@jjw-PC:~$ kubectl rollout history deployment nginx-deploymentdeployment.apps/nginx-deployment REVISION CHANGE-CAUSE2 &lt;none&gt;3 &lt;none&gt; 上述升级方式并不是滚动升级的，即不是优雅升级，期间会有一段时间服务处于宕机状态，实现滚动升级，我们只需要重新定义depolyment.yml文件，升级后的depolyment.yml文件中添加如下内容： 1234567891011121314151617181920212223242526272829303132333435# 定义的版本，因为它底层使用的是Replica SetapiVersion: apps/v1# 定义类型为Deploymentkind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx # 定义我们nginx的版本，这里故意使用一个低版本，便于我们演示升级Pod image: nginx:1.12.2 ports: - containerPort: 80 #滚动升级策略 minReadySeconds: 5 strategy: # indicate which strategy we want for rolling update type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 具体添加内容解释如下： minReadySeconds: Kubernetes在等待设置的时间后才进行升级 如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了 如果没有设置该值，在某些极端情况下可能会造成服务不正常运行maxSurge: 升级过程中最多可以比原先设置多出的POD数量 例如：maxSurage=1，replicas=5,则表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。maxUnavaible: 升级过程中最多有多少个POD处于无法提供服务的状态 当maxSurge不为0时，该值也不能为0 例如：maxUnavaible=1，则表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。 然后执行命令： kubectl apply -f [新的depolyment的yml文件] 这样就实现了滚动升级 升级可以暂停和继续，使用如下命令： kubectl rollout pause deployment &lt;deployment名称&gt;:暂停升级 kubectl rollout resume deployment &lt;deployment名称&gt;:继续升级","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jjw-story.github.io/categories/Kubernetes/"}],"tags":[{"name":"入门","slug":"入门","permalink":"https://jjw-story.github.io/tags/入门/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jjw-story.github.io/tags/Kubernetes/"}],"author":"JJW"},{"title":"Docker-Swarm","slug":"Docker-Swarm","date":"2020-06-06T08:30:37.000Z","updated":"2020-06-14T06:51:21.608Z","comments":true,"path":"2020/06/06/Docker-Swarm/","link":"","permalink":"https://jjw-story.github.io/2020/06/06/Docker-Swarm/","excerpt":"","text":"容器编排介绍（Swarm Mode）我们之前的学习创建的容器都是在一台机器上的，但是实际生产环境中，我们创建的容器不可能都是在一台机器上的，而是在多台物理机器上。在生产环境中我们要部署一个应用，可能涉及很多个容器，而且这些容器不可能都部署在一台机器上，那么我们就需要解决一系列问题，比如： 怎么去管理这么多容器 怎么方便的横向扩展 如果容器down了，怎么自动恢复 如何去更新容器而不影响业务 如何去监控追踪这些容器 如何去调度容器的创建 如何保护隐私数据 Swarm介绍Swarm就是做容器编排的一个工具，用于容器服务的创建删除等操作和调度，它用来解决上述问题，它是Docker内置的一个工具，但是我们要注意，容器编排的工具并不是Swarm一个，只是Docker内置的是这个，不需要安装任何其他的东西。 Swarm是一个集群的架构，它有两种角色，Manager和Worker。Manager是整个集群的大脑，一般至少要有两个机器，Worker就是实际容器的运行的节点，当然Manager节点也可以运行我们的业务容器，但是一般我们都是部署在Worker节点。 Service：Swarm中一个非常重要的概念，我们Swarm环境下的Service的概念基本和Compose中的Service是一样的，一个Service就代表了一个容器。 Replicas：作为容器横向扩展时的概念，就是我们部署一个service时，通过scale指定部署的容器个数时，我们每一个容器就是作为了一个Replica，可以理解为同一个容器的多个副本，但是这里不存在主分片的概念。 Swarm的应用创建Swarm集群我们这里只讲述生产环境中多个机器的Swarm的创建方式。 创建Swarm节点，方法是在一个安装了Docker环境的机器上通过命令： docker swarm init –advertise=[本机IP地址] 我们先运行此命令的节点将会成为Manager节点，运行此命令后，终端会输出如果要添加Manager节点和添加Worker节点的命令，我们要添加此集群的节点，就复制此对应的命令，然后到另外的机器上执行即可 创建Worker节点，将上述输出的添加Worker命令粘贴，进入另外的机器（可以是多个），执行即可 我们可以在Manager节点通过命令：docker node ls 查看此集群下的所有节点信息 在swarm集群环境service操作 我们docker service的创建有点类似于docker run，但是我们在searm模式下就不用docker run命令了，因为docker run只是用于在本地创建docker容器，创建命令如下： docker service create –name [service名称] [Image名称] 我们可以通过命令查看创建的容器信息，此信息会展示容器的Replicas数量等： docker service ls 查看到service信息后，我们可以查看每个容器具体的部署信息，比如部署在哪个节点，状态等，通过如下命令： docker service ps [service名称] 我们创建完成后就可以通过scale来进行容器数量的横向扩展，使用命令： docker service scale [service名称]=[容器的总数量] 执行完成后，我们通过ls命令，查看到的service信息，Replicas的数量就是我们设置的容器总数量了，并且我们通过ps命令查看到每个容器的部署是平均的分配在了不同的机器上 上述创建的容器我们说会平均分布在不同的机器上，如果此时我们有一个机器down掉了或者我们手动将容器给停止了，我们发现过一会在通过ls命令查看的容器数量还是5，这是因为它又新扩展了一个容器在其他的机器上。这就证明scale它不仅仅是横向扩展的功能，而且它能保证横向扩展的数目是稳定有效的，当其中容器down掉之后，他们 检测到然后立即创建新的容器保证扩展的数量 service的删除，我们service的删除使用如下命令，删除后会将此service下所有的容器都回收掉： docker service rm [service名称] Swarm网络示例及现象介绍首先我们搭建一个三台机器的swarm集群，有一个Manager节点和两个Worker节点，然后我们部署wordpress和mysql应用，我们在部署前创建驱动类型为 overlay 的Network Namespace，然后创建mysql和wordpress的service，并且在创建的时候通过 -p 参数指定wordpress的端口映射，使得我们可以向外部提供服务，在创建的时候指定此两个service使用的Network Namespace（具体方式与run启动指定参数一样），使他们之间可以直接通过service名称就能实现网络互通，完成我们的应用部署。 注意：我们如果要保证本机之间的容器通信，需要使用bridge驱动类型的Network，但是如果是多机之间的通信，需要使用overlay驱动类型的Network 以上出现的现象是： 我们之前挨Manager节点上创建的驱动类型为 overlay 的Network Namespace，在其他的两个Worker机器节点上也能看到并且存在 我们的mysql service容器部署在了manager机器上，wordpress service容器部署在了其中一台Worker节点上，但是我们通过我们映射好的端口，通过任何一个swarm集群机器的ip加此映射好的端口，都能访问到我们的workpress服务 Rounting Mesh的两种体现Docker Swarm的网络技术 Internal Network针对于Swarm service之间的访问及负载均衡，我们同一个Swarm中的容器之间是怎么通信的，他们是用过service名称来进行通信的，我们service名称对应的IP地址并不是容器的具体地址，因为我们某一个service下可能会通过scale创建多个容器，并且容器扩增或删除都会造成service下容器IP地址的变化，所以我们肯定不是直接通过容器的IP地址进行通信的，我们service名称对应的IP地址是一个VIP，就是一个虚拟IP，如果我们的横向容器扩展了之后，VIP是不会变化的，并且它会对我们的访问做负载均衡，这就是VIP的主要作用和Swarm的网络访问实现。 Internal Load Balancing，既我们通过service名称访问的容器它是通过VIP访问的，VIP会自动负载均衡到请求的不同的容器节点上去，它是基于LVS来实现的，LVS是一个很牛的中国大佬写的，已经加入到Linux内核中了，详细google或baidu了解。 Ingress Network针对于外部访问的负载均衡，我们某个service绑定了对外的端口，这个service可能只是部署在了我们Swarm集群中的部分节点，但是我们在Swarm中的任何一个机器节点都可以通过此机器IP加端口都能访问到此容器的服务，它就是使用了Ingress网络。 Docker StackDocker Stack及对于一个应用包含的多个容器进行统一管理的工具，它与Docker Compose的作用及使用类似，不同的是compose是应用于单机环境的，而Stack是应用于Swarm下的多机环境的。 Docker Stack的使用也是使用docker-compose.yml文件来定义的，此文件的格式与之前介绍的compose定义一样，只不过添加了deploy参数，此参数下定义的内容有非常多，包括service部署更新等策略，容器的数量、内存CPU的使用限制等非常多，具体我们可以通过官网去查看。 docker stack 命令参数说明： deploy：部署或更新stack，使用示例：docker stack depoly [指定stack名称] –compose-file=docker-compose.yml ls：列举出当前所有stack及每个stack中包含的services个数，使用示例：docker stack ls ps：查询指定stack的详细信息，包括容器部署的节点、状态、使用的image等信息，使用示例：docker stack ps [指定stack名称] rm：删除指定的stack，使用示例：docker stack rm [指定stack名称] services：列举出当前stack中包含的所有service，以及service的副本数量等信息，使用示例：docker stack services [指定stack名称] docker-compose.yml文件示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071version: &apos;3&apos;services: web: image: wordpress ports: - 8080:80 environment: WORDPRESS_DB_HOST: mysql WORDPRESS_DB_PASSWORD: root networks: - my-network depends_on: - mysql # web的service stack配置 deploy: # 部署模式-多例 mode: replicated # 容器分片数量 replicas: 3 # 重启策略 restart_policy: condition: on-failure delay: 5s max_attempts: 3 # 更新策略 update_config: # 更新时同时进行的容器数量 parallelism: 1 delay: 10s mysql: image: mysql environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: wordpress volumes: - mysql-data:/var/lib/mysql networks: - my-network # mysql的service stack配置 deploy: # 部署模式-单例 mode: global placement: # 容器部署的节点位置，指定部署在manager机器上 constraints: - node.role == manager # 一个可视化的stack工具，访问此端口可以通过可视化的界面查看具体的stack信息 visualizer: image: dockersamples/visualizer:stable ports: - &quot;8080:8080&quot; stop_grace_period: 1m30s volumes: - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; deploy: placement: constraints: [node.role == manager]volumes: mysql-data:# 指定使用的网络驱动networks: my-network: driver: overlay Docker Secret管理什么是Secret，主要是指一下内容： 用户名密码 SSH Key TLS认证 任何不想让别人看到的数据 我们之前在讲Swarm时说过，在Manager节点之间，它们有一个内置的分布式存储， 它们是基于Raft协议来同步数据的，保证他们之间的信息时实时共享同步的。 Secret Management Secret都是存储在Swarm 的 Manager节点，存储在Raft database中的，都是加密的 Secret可以assign给一个service，在这个service中就可以看到这个secret 在container内部Secret看起来像文件，但实际是在内存中的 Secret的具体操作使用Secret的创建首先介绍查看Secret的命令： docker secret ls，表示查看当前环境中的Secret 创建有两种方式，一种是从一个文件中创建，一种是从标准的输入中创建。 我们首先创建一个文件，然后再文件中写入我们的Secret内容，例如密码等内容，然后我们通过命令：docker secret create [secret名称] [文件路径名称]，这样我们就创建完成，一般我们通过文件创建好secret后，就把我们的文件删除掉，这是为了保护我们的隐私数据。创建完成我们就可以通过ls命令来查看创建的Secret。 通过标准输入创建，使用示例如下：echo “secret具体内容” | docker secret create [secret名称] -，这样我们就创建完成了，可以通过ls命令查看了 Secret的删除使用命令：docker secret rm [secret名称] 将Secret暴露给Service容器来使用我们在创建service容器时，指定一下即可，例如： 123456789# 1.创建容器并指定要应用的Searetdocker service create --name [service名称] --secret [要使用的之前在Manager节点创建好的secret名称] [image名称]# 2.进入创建好的容器docker exec -it [容器ID] /bin/bash# 3.进入 /run/secret 目录并查看文件，我们发现有一个文件跟我们指定的Secret名称一样，查看后发现内部的内容就是我们之前定义好的内容cd /run/secretls 具体使用例如我们创建一个MySQL容器，并且创建时指定root用户的密码，具体使用命令如下： 1docker service create --name [mysql service 名称] --secret [要使用的之前在Manager节点创建好的secret名称] -e MYSQL_ROOT_PASSWORD_FILE=/run.secret/[之前在Manager节点创建好的secret名称] [mysql image名称] Searct在stack中的应用应用原理其实与上述暴露给service中一样，只不过我们将一些命令直接写在了docker-compose.yml文件中，例如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 1.首先通过上述方式在Manager节点创建好Secret# 2.docker-compose.yml文件中引入应用version: &apos;3&apos;services: web: image: wordpress ports: - 8080:80 # 定义我们要引入的secret secrets: - my-pw # 环境变量应用我们引入的secret environment: WORDPRESS_DB_HOST: mysql WORDPRESS_DB_PASSWORD_FILE: /run/secrets/my-pw networks: - my-network depends_on: - mysql deploy: mode: replicated replicas: 3 restart_policy: condition: on-failure delay: 5s max_attempts: 3 update_config: parallelism: 1 delay: 10s mysql: image: mysql # 定义我们要引入的secret secrets: - my-pw # 环境变量应用我们引入的secret environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/my-pw MYSQL_DATABASE: wordpress volumes: - mysql-data:/var/lib/mysql networks: - my-network deploy: mode: global placement: constraints: - node.role == managervolumes: mysql-data:networks: my-network: driver: overlay# 我们可以不再先前就创建好secret，可以定义一个文件在创建service时引入文件创建，但是这种做法不安全，所以我们不推荐（一般还是先创建好）# secrets:# my-pw:# file: ./password Service的更新本节讲述的是对Swarm中运行的service进行升级，我们需要实现的是不宕机的升级，参照现在的生产环境服务升级。 Service的更新可以更新很多内容，例如Secret，暴露的端口，image等。 普通方式更新 更新image，当我们的servcie使用scale创建了多个实例的时候，我们使用如下命令更新服务，发现容器更新是逐个进行的，可以表述为优雅上下线，命令如下： 更新服务使用命令： docker service update –image [更新的新的image名称] [service名称] 更新端口，命令如下(同理更新secret)： docker service update –pulish-rm 8080:5000 –pulish-add 8088:5000 [service名称] stack模式下的更新stack模式下的更新我们只需要修改 docker-compose.yml 文件的内容，然后重新发布就自动完成了更新，而且更新的过程更上述一样，都是优雅的。 例如我们要升级服务更新image，只需要修改 docker-compose.yml 中对应的serivce中对应的iamge即可，更新其他内容也一样，只需要修改此文件，然后直接 depoly 即可。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/tags/Docker/"},{"name":"Swarm","slug":"Swarm","permalink":"https://jjw-story.github.io/tags/Swarm/"}],"author":"JJW"},{"title":"Docker-Compose","slug":"Docker-Compose","date":"2020-06-03T14:34:23.000Z","updated":"2020-06-03T14:33:07.000Z","comments":true,"path":"2020/06/03/Docker-Compose/","link":"","permalink":"https://jjw-story.github.io/2020/06/03/Docker-Compose/","excerpt":"","text":"Docker Compose介绍如果我们有一个APP是由多个容器组成，那我们在部署这个APP的时候就会非常的繁琐，我们要去维护多个Docker image，基于这些image我们还要创建多个container，要管理这些container会非常麻烦，比如启动停止删除等，需要分别一个个的操作，Compose就是来解决这个问题的，它所赋予的使命就是“批处理”，如果我们一个应用有十个容器，那我们利用compose，只需要执行一条命令就可以完成这些容器的启动停止等操作。 Docker Compose是一个工具，这个工具可以通过一个yml文件定义多个容器的Docker应用，通过一条命令就可以根据yml文件的定义去创建或者管理这些多个容器。 docker-compose.yml文件compose的yml文件默认名称是Docker-compose.yml，我们可以根据需要更改名称。此文件有三大概念，Services、Networks、Volumes。 docker-compose的文件格式版本经历了三个版本，分别时version-1、2、3，我们现在基本使用3版本，但是1版本现在不要用，基本淘汰了，2版本定义虽然变化不大，但是2版本只能应用在单机，3版本就可以管理多机的场景（多个容器部署在不同的物理机器上） Services一个services代表一个container，这个container可以从dockerhub的image来创建，或者从本地的Dockerfile build出image然后创建 Service的启动类似与docker run命令，所以我们docker run命令可以指定的参数我们都可以通过compose文件指定出来，可以给其指定network和volume，所以可以给service指定network和volume的引用 yml配置语法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 固定声明文件版本version: &apos;3&apos;# 固定声明servicesservices: # worker是表示这个services的名称 worker: # （注意此两项只需要写一项） # 声明要创建的容器的image，来源为DockerHub image:mysql：5.7 # 声明要创建的容器的image，来源为此配置目录下的Dockerfile build: ./worker # 声明端口映射，将容器的端口映射到本地 ports: - 8080:80 # 声明环境变量 environment: WORDPRESS_DB_HOST: mysql # 声明映射的volume volumes： - db-data:/var/lib/mysql/data # 声明我们的容器会与那几个容器做link，实际应用不广泛，我们在link时都是直接指定network的bridge，多个容器链接在同一个bridges上，它们自然是互通的 links: - redis - mysql # 生命容器使用的network networks: - back-tier# 固定生命volumesvolumes: # 与上述services中声明对应 db-data: # 固定声明networksnetworks: # 与services中声明对应，这里详细具体定义 back-tier: # 声明此命名空间使用的驱动 driver: bridge 以下是一个实际应用的示例，这个示例中我们一个APP应用了两个容器，并对它们做关联，具体的yml文件如下： 123456789101112131415161718192021222324252627282930version: &apos;3&apos;services: wordpress: image: wordpress ports: - 8080:80 environment: WORDPRESS_DB_HOST: mysql WORDPRESS_DB_PASSWORD: root networks: - my-bridge mysql: image: mysql environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: wordpress volumes: - mysql-data:/var/lib/mysql networks: - my-bridgevolumes: mysql-data:networks: my-bridge: driver: bridge docker-compose的具体使用首先需要安装docker-compose，在Windows等版本的Docker安装中默认就会安装compose，但是Linux中不会，所以需要我们手动安装一下，安装步骤如下： curl -L “https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)” -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 安装完成，运行： docker-compose –version 就可以看到compose的具体信息 docker-compose 命令具体使用我们可以通过：docker-compose –help 命令来查看帮助 docker-compose up命令 前面我们讲了yml文件，定义好此文件后，我们可以通过此命令来创建容器了，一般我们就在yml文件所在的目录执行：dokker-compose up命令即可，但是我们之前说此yml文件的名称是可以自定义的，如果修改为其他名称就需要：docker-compose -f [文件名称] up 来创建我们的容器了，我们也可以添加 -d 参数来后台启动，即不打印日志 docker-compose stop命令 停止我们通过 docker-compose up 命令一次启动的各个容器 docker-compose start 重启我们之前通过 docker-compose up 命令一次启动的各个容器 docker-compose down 停止并删除之前通过 docker-compose up 命令一次启动的各个容器，并且删除container，以及network，但是它并不会删除image和volume docker-compose ps 查看我们通过yml创建的所有容器 docker-compose iamge 查看我们通过yml创建container，以及定义它的image docker-compose exec docker-compose exec [yml文件service定义的容器名称] bash，等同于docker exec it [容器名称] /bin/bash 水平扩展和伏在均衡（scale）有时候我们线上的服务机器数量较少，但遇到大并发机器处理不过来的时候，我们需要横向扩展机器，这时我们compose就遇到大用处了 通过：docker-compose up -scala [yml文件services中定义的容器名称]=[容器的总数量] -d 就可以迅速的扩展机器 注意：如过本身我们已经有2个容器，我们指定容器的总数量为5,那么它会新添加3个，一共5个，而不是新添加5个 扩展后我们通过：docker-compose ps，就可以查看到扩展后的容器 注意：如果我们在yml文件中指定了端口映射，那么如果一个机器上扩展的多个容器都映射到本机一个端口上，就会造成端口冲突，所以我们需要在yml文件中去除端口映射的代码，让它自己生成。这个时候我们只需要在添加一个负载均衡的容器，让它来监听各个容器内部的端口，一般都是我们在容器服务代码中指定好的端口，然后请求打在这个负载均衡的容器上，就完成了一个完美的横向扩展。（这里我们需要了解到，Spring Cloud Eurka它天生就是这个作用，我们启动服务自己就注册到了Eurka上，可以快速的横向扩展） 当我们使用此命令设置的容器的总数量小于实际在运行的容器总数量时，它会自动回收停止掉一些，完成闲时资源释放 注意：上述命令我们指定了要扩展数量的容器名称，如果我们的yml中定义了多个容器，我们只会扩展指定的容器的数量，不会扩展其中定义的其他容器","categories":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/tags/Docker/"},{"name":"Compose","slug":"Compose","permalink":"https://jjw-story.github.io/tags/Compose/"}],"author":"JJW"},{"title":"Docker-数据持久化","slug":"Docker数据持久化","date":"2020-05-23T13:32:39.000Z","updated":"2020-06-02T14:08:49.000Z","comments":true,"path":"2020/05/23/Docker数据持久化/","link":"","permalink":"https://jjw-story.github.io/2020/05/23/Docker数据持久化/","excerpt":"","text":"Docker数据持久化简介我们创建的容器里面是可以写数据的,image是只读的,但是container是可以写数据的,但是我们在container里面写的数据仅限于存在于这个容器,如果我们将这个容器停掉或者删除掉,那么我们创建的文件或存储的数据就都没了,所以容器的存储是临时的. 但是有的时候我们就需要将一些数据保存起来,必须我们的数据库容器,这个我们就需要利用Docker的数据持久化技术. Docker持久化数据的方案 基于本地文件系统的Volume:可以在执行docker create 或者 docker run时,通过 -v 参数将主机的目录作为容器的数据卷.这部分功能便是基于本地文件系统的voloum管理. 基于plugin的Volume:支持第三方的存储方案,比如NAS,aws Volume的类型: 受管理的Volume(data Volume):它是受管理的data volume,由docker后台自动创建,不需要我们通过 -v 参数指定,它的位置是固定的,但是名字是随机的 绑定挂载的Volume(bind Volume):具体的挂载位置可以由用户来指定,这样的好处是方便用户来管理 Data Volume这节主要讲述第一种的数据持久化方案,通过指定主机的目录作为容器的数据卷,我们主要是使用-v参数来指定volume的名称,然后发现不同的容器是可以复用一个volume 我们通过安装一个MySQL容器并且指定数据持久化的位置(MySQL的Dockerfile中是有指定Volume的,我们只是通过 -v 参数来指定一个Volume名称),然后在此MySQL中创建一个表,然后删除此容器,重新创建一个新的MySQL容器,然后指定Volume是上一次创建的那个Volume,通过进入容器,发现上个容器中创建的表还是存在的,来说明数据持久化的方案是生效的. 主要命令: -v 指定容器的数据存储位置及位置名称: docker run -d -v [volume名称]:[具体宿主机文件位置绝对路径] –name [容器名称] [image名称] 查看所有的Volume: docker volume ls 查看指定Volume的具体信息: docker volume inspect [volume名称] 删除指定的Volume: docker volume rm [volume名称] 下面是演示示例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# 1.docker hub上下载MySQL5.7版本的Image# 2.查看Dockerfile发现由如下代码,表明指定了MySQL数据存储的具体宿主机位置VOLUME /var/lib/mysql# 3.启动此Docker容器, -e 指定了环境变量,设置此MySQL的ROOT用户密码. -v 指定了volume的名称地址jjw@jjw-PC:~$ docker run -d -v mysql:/var/lib/mysql --name mysql01 -e MYSQL_ROOT_PASSWORD=123456 mysql:5.728e8bb72065bb5417936b098952dc741969420e80de22d177c3cb50cfe538101# 4.查看我们创建的mysql Volume是否创建成功jjw@jjw-PC:~$ docker volume lsDRIVER VOLUME NAMElocal mysql# 5.查看此volume的具体信息docker volume inspect [volume名称]jjw@jjw-PC:~$ docker volume inspect mysql[ &#123; &quot;CreatedAt&quot;: &quot;2020-06-02T21:34:14+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/mysql/_data&quot;, &quot;Name&quot;: &quot;mysql&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;]# 6.我们进入此MySQL容器中docker exec -it mysql01 /bin/bashjjw@jjw-PC:~$ docker exec -it mysql01 /bin/bashroot@28e8bb72065b:/# # 7.查看当前数据库中的表并创建一个新的表叫dockerroot@c1afa5e97468:/# mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 11Server version: 5.7.30 MySQL Community Server (GPL)show databasesmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.00 sec)create database dockermysql&gt; create database docker;Query OK, 1 row affected (0.00 sec)show databasemysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || docker || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec)# 8.退出容器并删除此容器jjw@jjw-PC:~$ docker rm -f mysql01mysql01# 9.查看volume,发现容器创建的volume还是在的jjw@jjw-PC:~$ docker volume lsDRIVER VOLUME NAMElocal mysql# 10.我们重新去创建一个MySQL的容器,指定Volume还是上一次的那个Volumedocker run -d -v mysql:/var/lib/mysql --name mysql02 -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql# 11.创建完成我们进入此新的容器中,并查看数据库总所有的表,发现我们之前mysql01创建的表还在,说明我们的数据持久化是成功的jjw@jjw-PC:~$ docker run -d -v mysql:/var/lib/mysql --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7881ba147a8b1804edab835d268fd70bb6974a1e643f595d9ff98fff684868e3djjw@jjw-PC:~$ docker exec -it mysql02 /bin/bashroot@881ba147a8b1:/# mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.7.30 MySQL Community Server (GPL)show databases;mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || docker || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec) bind volume上一种方式我们创建的Volume需要在Dockerfile中使用 VOLUME 参数指定具体的位置,我们虽然可以通过-v参数修改volume的名称,但是不能修改具体位置.而这一节的这种方式我们是通过绑定的方式,不需要在Dockerfile中去定义,我们只需要在启动docker容器时指定容器的存储目录和本地的目录做一个绑定关系,这样我们当容器的目录中数据发生变化时,会同步到本地的目录中,因为其实它们使用的是同一个目录文件. 以下我们通过示例来演示,示例内容主要是利用一个Dockerfile,然后在Dockerfile中使用 WORKDIR 指定工作目录,我们不使用 VOLUME 参数指定数据存储目录,然后我们启动容器时使用-v参数绑定一个本机目录,然后再容器中创建文件及修改文件,我们发现在本地的目录中也会创建这样的文件并跟随修改,并且我们在本机修改次文件内容,进入容器发现容器中的文件也跟随修改,因为它们是同一个文件 12345678910111213# 1.修改Dockerfile,添加WORKDIR参数WORKDIR /usr/share/test# 2.根据此Dockerfile构建image# 3.启动容器并指定绑定的目录docker run -d -v [本机目录绝对路径]:[容器中需要映射的目录] --name [容器名称] [image名称]# 4.进入容器内部在被bind的目录下创建一个文件# 5.退出容器进入本机绑定的目录发现由我们刚才在容器中创建的目录# 6.在本机修改此文件,然后进入容器内部,发现容器内的文件也被修改(因为他们是同一个文件)","categories":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/tags/Docker/"},{"name":"数据持久化","slug":"数据持久化","permalink":"https://jjw-story.github.io/tags/数据持久化/"}],"author":"JJW"},{"title":"Docker-Network","slug":"Docker-Network","date":"2020-05-13T13:32:39.000Z","updated":"2020-05-23T11:49:27.000Z","comments":true,"path":"2020/05/13/Docker-Network/","link":"","permalink":"https://jjw-story.github.io/2020/05/13/Docker-Network/","excerpt":"","text":"Dcocker网络-现象容器与宿主机的网络隔离首先我们创建一个容器，然后进入此容器查询容器的网关，然后退出口查看宿主机的网关，查看可以发现它们是完全不一样的，由此可见容器的Network Namespace与宿主机是隔离开的，演示如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 1.启动容器jjw@jjw-PC:~$ docker run -d --name demo01 jjw-story/flask-hello-world645a63e0a07637efcd5fe85813d30eb5d92b4533b7c7a1458d2c4db5d6142cda# 2.进入容器查看网关jjw@jjw-PC:~$ docker exec -it demo01 /bin/bashroot@645a63e0a076:/app# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever5: eth0@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever# 3.查看宿主机网关jw@jjw-PC:~$ ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp2s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 link/ether d8:d0:90:00:dd:94 brd ff:ff:ff:ff:ff:ff3: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 48:5f:99:ce:dc:3b brd ff:ff:ff:ff:ff:ff inet 192.168.1.9/24 brd 192.168.1.255 scope global dynamic noprefixroute wlp3s0 valid_lft 603108sec preferred_lft 603108sec inet6 2409:8a00:78ce:bb0:eaf5:3d9b:3a9d:dfd0/64 scope global dynamic noprefixroute valid_lft 259159sec preferred_lft 172759sec inet6 fe80::d29c:f72a:7c51:83cc/64 scope link noprefixroute valid_lft forever preferred_lft forever4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ca:69:08:5a brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:caff:fe69:85a/64 scope link valid_lft forever preferred_lft forever6: veth531bf01@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether ae:ba:05:48:45:07 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::acba:5ff:fe48:4507/64 scope link valid_lft forever preferred_lft forever 容器与容器之间网络是互通的我们创建第二个容器，然后进入第二个容器查看网关，发现与第一个容器是不一样的，然后我们在第二个容器中ping第一个容器的ip，发现是可以ping通的 12345678910111213141516171819202122232425262728# 1.创建第二个容器jjw@jjw-PC:~$ docker run -d --name demo02 jjw-story/flask-hello-worldf6606faba1d160e78e4fb1accadadbb4cf61df5feddeabc1b3ef0d25f229fa3d# 2.进入容器查看第二个容器的网管jjw@jjw-PC:~$ docker exec -it demo02 /bin/bashroot@f6606faba1d1:/app# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever# 3.ping第一个容器的ip地址root@f6606faba1d1:/app# ping 172.17.0.2PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.169 ms64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.111 ms64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.106 ms64 bytes from 172.17.0.2: icmp_seq=4 ttl=64 time=0.103 ms^C--- 172.17.0.2 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 73msrtt min/avg/max/mdev = 0.103/0.122/0.169/0.028 ms 下面一节我们讲述一下原理 Namespace-网络命名空间我们通过在Linux上创建一个可用的Network Namespace来说明它的原理 查看当前机器的Network Namespace： sudo ip netns list 删除指定的Network Namespace： sudo ip netns delete [namespace名称] 创建Network Namespace： sudo ip netns add [namespace名称] 查看Network Namespace的网关及ip地址：sudo ip netns exec [namespace名称] ip a 查询各个网口当前的状态： ip link，注意 DOWN表示未启动，UP表示正在与运行，UNKNOWN表示未知 修改指定网口的状态： sudo ip netns exec [namespace名称] ip link set dev [网口名称] up l0：每次我们查询网关的时候都会发现有这样的一个网关，它代表的是localback0，本地回环口 我们的网口需要UP起来，它必须是要与另外一个网口成为一对的，也就是说它必须是要两端直连起来的，也就是单个端口是没有办法UP起来的，这个是硬性限制，网络的基础知识，需要注意 创建自己的网口并且是UP状态示例 添加自定义的Namespace，然后查询此Namespace中网口的ip和状态 1234567jjw@jjw-PC:~$ sudo ip netns add test1jjw@jjw-PC:~$ sudo ip netns listtest1jjw@jjw-PC:~$ sudo ip netns exec test1 ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 我们发现此网口没有IP地址，并且状态是DOWN 修改此网口的状态为UP，并查看修改结果 1234jjw@jjw-PC:~$ sudo ip netns exec test1 ip link set dev lo upjjw@jjw-PC:~$ sudo ip netns exec test1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 我们发现这里设置为UP之后，它并不是直接成为UP状态，而是成为了UNKNOWN状态，这是因为我们此端口并没有与另外一个端口成为一对并且直连，所以它并不会UP起来 我们创建一对相互配对的网口（为了创建出一个UP的端口） 123456789101112131415161718jw@jjw-PC:~$ sudo ip link add veth-test1 type veth peer name veth-test2jjw@jjw-PC:~$ ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp2s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT group default qlen 1000 link/ether d8:d0:90:00:dd:94 brd ff:ff:ff:ff:ff:ff3: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DORMANT group default qlen 1000 link/ether 48:5f:99:ce:dc:3b brd ff:ff:ff:ff:ff:ff4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:ca:69:08:5a brd ff:ff:ff:ff:ff:ff6: veth531bf01@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ae:ba:05:48:45:07 brd ff:ff:ff:ff:ff:ff link-netnsid 08: veth973fc89@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether f6:6d:60:62:80:cf brd ff:ff:ff:ff:ff:ff link-netnsid 19: veth-test2@veth-test1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 62:55:74:b2:2f:11 brd ff:ff:ff:ff:ff:ff10: veth-test1@veth-test2: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 22:c8:a4:b3:f4:b2 brd ff:ff:ff:ff:ff:ff 创建后我们发现我们已经创建成功，并且他们的名称都是 veth-test2@veth-test1 和 veth-test1@veth-test2，它们有mac地址，但是没有ip地址，状态都为DOWN 将我们创建的网口添加到我们之前创建的Namespace中 123456jjw@jjw-PC:~$ sudo ip link set veth-test1 netns test1jjw@jjw-PC:~$ sudo ip netns exec test1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0010: veth-test1@if9: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 22:c8:a4:b3:f4:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 我们发现我们新创建的网口已经被添加到我们之前创建的Namespace中了 同理我们创建test2的Namespace，然后将veth-test2这个网口添加到test2这个Namespace中 12345678910jjw@jjw-PC:~$ sudo ip netns add test2jjw@jjw-PC:~$ sudo ip netns listtest2test1 (id: 2)jjw@jjw-PC:~$ sudo ip link set veth-test2 netns test2jjw@jjw-PC:~$ sudo ip netns exec test2 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:009: veth-test2@if10: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 62:55:74:b2:2f:11 brd ff:ff:ff:ff:ff:ff link-netns test1 查看本机的ip link，我们发现已经没有了刚才的veth-test2@veth-test1 和 veth-test1@veth-test2，说明我们已经将此两个端口放到了我们创建的两个Namespace中 给我们刚刚创建的两个Namespace的端口分配IP地址 12jjw@jjw-PC:~$ sudo ip netns exec test1 ip addr add 192.168.1.1/24 dev veth-test1jjw@jjw-PC:~$ sudo ip netns exec test2 ip addr add 192.168.1.2/24 dev veth-test2 这时如果我们去查看这两个Namespace的ip和状态，发现我们的网口还是没有ip地址，并且状态还是DOWN，这时我们需要设置将这两个网口启动起来 12jjw@jjw-PC:~$ sudo ip netns exec test1 ip link set dev veth-test1 upjjw@jjw-PC:~$ sudo ip netns exec test2 ip link set dev veth-test2 up 这时我们再去查看这两个Namespace中的新创建网口状态和IP发现就都是正常的了 1234567891011121314151617181920212223242526272829jjw@jjw-PC:~$ sudo ip netns exec test1 ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0012: veth-test1@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 16:af:b8:ee:76:02 brd ff:ff:ff:ff:ff:ff link-netns test2 inet 192.168.1.1/24 scope global veth-test1 valid_lft forever preferred_lft forever inet6 fe80::14af:b8ff:feee:7602/64 scope link valid_lft forever preferred_lft foreverjjw@jjw-PC:~$ sudo ip netns exec test1 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0012: veth-test1@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 16:af:b8:ee:76:02 brd ff:ff:ff:ff:ff:ff link-netns test2jjw@jjw-PC:~$ sudo ip netns exec test2 ip a1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0011: veth-test2@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 36:f9:03:cb:ef:e1 brd ff:ff:ff:ff:ff:ff link-netns test1 inet 192.168.1.2/24 scope global veth-test2 valid_lft forever preferred_lft forever inet6 fe80::34f9:3ff:fecb:efe1/64 scope link valid_lft forever preferred_lft foreverjjw@jjw-PC:~$ sudo ip netns exec test2 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0011: veth-test2@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 36:f9:03:cb:ef:e1 brd ff:ff:ff:ff:ff:ff link-netns test1 现在我们就终于创建好了我们自己的正常的Namespace和互联互通的网口，我们就可以在其中一个Namespace中去ping另外一个Namespace的这个网口，发现就可以通了 互相ping IP地址 123456789jjw@jjw-PC:~$ sudo ip netns exec test1 ping 192.168.1.2PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data.64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.031 ms64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=0.058 ms64 bytes from 192.168.1.2: icmp_seq=3 ttl=64 time=0.058 ms^C--- 192.168.1.2 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 49msrtt min/avg/max/mdev = 0.031/0.049/0.058/0.012 ms 综上就是我们Namespace的使用方法及具体作用，如第一节中我们所示，Docker创建的容器他们之间ip能互通，基本使用的就是此原理，与我们的示例原理是一致的 Docker BridgeDocker的默认网络，上述我们发现在创建容器后发现容器之间ip是互通的，并且在容器内部是可以ping通外部网络的，并且在我们每次启动一个容器，就会在宿主机器上多出一个网口，如下： 1234567891011121314151617181920212223242526272829303132# 1.查询原本宿主机网口jw@jjw-PC:~$ ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 ...2: enp2s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 ...3: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 ...4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default ...# 2.启动一个容器jjw@jjw-PC:~$ docker start demo01demo01# 3.查看现在宿主机网口1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 ...2: enp2s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 ...3: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 ...4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:5b:44:0a:01 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:5bff:fe44:a01/64 scope link valid_lft forever preferred_lft forever8: veth54021aa@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 72:40:50:33:72:99 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::7040:50ff:fe33:7299/64 scope link valid_lft forever preferred_lft forever 我们发现在上述启动一个容器后，宿主机新添加了一个网口veth54021aa@if7，并且它是与docker0网口相关联的，我们说这个新的网口就是第一节示例中的Network Namespace的一对peer，它们链接了容器的ip和docker0，docker0网口就是我们说的 Docker Bridge 我们没创建一个容器，它都会创建这样的一个peer，然后与docker0相关联，这就是容器之间ip是互通的原理，也是容器内访问外网的原因 Docker Bridge是Docker网络最重要的部分 容器之间的link–link的使用本章注意下面命令： docker network ls: 查看docker上存在的所有Network Namespace docker network create -d bridge [自定义bridge名称]： 创建自定义bridge docker network connect [自定义bridge名称] [容器名称]： 将容器链接到自定义bridge docker network inspect [bridge ID]：查看bridge的具体信息，ID通过上述 ls 命令查看 前面我们说过，容器之间的网络是互通的，但是很多时候我们容器之间需要链接，但是我们并不知道另一方容器的ip地址，比如一个服务容器需要链接一个数据库容器，这时候我们就可以利用docker的link，我们可以不通过ip地址而是通过容器名称来进行链接 下面我们通过link使用示例来说明，我们创建两个容器，然后用第二个容器link到第一个容器，然后ping第一个容器的名称 123456789101112131415161718192021222324# 1.创建第一个容器jjw@jjw-PC:~$ docker run -d --name test1 jjw-story/flask-hello-world968f4e9fafe3fb2662e457515010c3cfedd57dc91d89839d7d091de753ef7e4a# 2.创建第二个容器并链接到第一个容器（使用 --link 参数）jjw@jjw-PC:~$ docker run -d --name test2 --link test1 jjw-story/flask-hello-worldec56c76cc2e38daca937672582d9451758d98566f510d2d7212ec4a9ea8eea4f# 3.进入第二个容器通过ping第一个容器的名称jjw@jjw-PC:~$ docker exec -it test2 /bin/bashroot@ec56c76cc2e3:/app# ping test1PING test1 (172.17.0.2) 56(84) bytes of data.64 bytes from test1 (172.17.0.2): icmp_seq=1 ttl=64 time=0.234 ms64 bytes from test1 (172.17.0.2): icmp_seq=2 ttl=64 time=0.105 ms64 bytes from test1 (172.17.0.2): icmp_seq=3 ttl=64 time=0.104 ms^C--- test1 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 52msrtt min/avg/max/mdev = 0.104/0.147/0.234/0.062 ms# 4.进入第一个容器ping第二个容器的名称，我们发现是ping不通的，因为我们创建test1容器并没有link test2这个容器jjw@jjw-PC:~$ docker exec -it test1 /bin/bashroot@968f4e9fafe3:/app# ping test2ping: test2: Temporary failure in name resolution 以上示例就是link的使用方法，生产中我们链接数据库时就可以通过数据库容器名称:3601来直接访问数据库服务。 创建自己的bridge我们发现link是单向的，必须在创建容器时就指定好，不然是ping不通的，所以正式用的时候我们很少使用这种方式，而是使用第二种方式，创建自定义的bridge，将容器链接到此bridge上，这两链接的多个容器就都是互通的，可以通过容器名称互访 一般我们创建容器会链接到Docker的默认的bridge，我们也可以在创建容器时指定链接的bridge，或者在容器运行时添加链接到新的bridge，每个容器都可以链接多个bridge 以下我们通过示例创建一个自定义bridge，然后将服务挂载实现服务间名称访问网络互通 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 1.创建自定义的bridgejjw@jjw-PC:~$ docker network create -d bridge my-bridge714b9b2f827ee434b7ecbc08ad792d34cf9e298e2cbc42339d697317087bf157jjw@jjw-PC:~$ docker network lsNETWORK ID NAME DRIVER SCOPE413d667b89f6 bridge bridge localcfcb7ba38694 host host local714b9b2f827e my-bridge bridge local7a8a158e3c19 none null local# 2.创建第三个容器，指定链接到此自定义的bridgejjw@jjw-PC:~$ docker run -d --name test3 --network my-bridge jjw-story/flask-hello-world811ddcb7a6fbff14f3c18e7f09d93df9df23c90c4b1f162a4bd0e3d182619aa0# 3.将之前已经存在的容器test2链接到此自定义的bridge上jjw@jjw-PC:~$ docker network connect my-bridge test2# 4.查看此自定义bridge信息，发现此自定义bridge上挂载了两个容器jjw@jjw-PC:~$ docker network inspect 714b9b2f827e[ &#123; &quot;Name&quot;: &quot;my-bridge&quot;, &quot;Id&quot;: &quot;714b9b2f827ee434b7ecbc08ad792d34cf9e298e2cbc42339d697317087bf157&quot;, ... &quot;Containers&quot;: &#123; &quot;811ddcb7a6fbff14f3c18e7f09d93df9df23c90c4b1f162a4bd0e3d182619aa0&quot;: &#123; &quot;Name&quot;: &quot;test3&quot;, &quot;EndpointID&quot;: &quot;3dabecb55602d042563ac700343ac06a6f5221fa713caa521ff3eb75460b1aa7&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.18.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;ec56c76cc2e38daca937672582d9451758d98566f510d2d7212ec4a9ea8eea4f&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;1e9dcad5b2f800d440a53f39fe809e671b5bfbe72f46820bbe9ad51daa12fe32&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.18.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125; ... &#125;]# 5.分别进入两个容器，互相ping test2 和 test3,我们发现是可以通的jjw@jjw-PC:~$ docker exec -it test2 /bin/bashroot@ec56c76cc2e3:/app# ping test3PING test3 (172.18.0.2) 56(84) bytes of data.64 bytes from test3.my-bridge (172.18.0.2): icmp_seq=1 ttl=64 time=0.056 ms64 bytes from test3.my-bridge (172.18.0.2): icmp_seq=2 ttl=64 time=0.112 ms^C--- test3 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 3msrtt min/avg/max/mdev = 0.056/0.084/0.112/0.028 msroot@ec56c76cc2e3:/app# exitexitjjw@jjw-PC:~$ docker exec -it test3 /bin/bashroot@811ddcb7a6fb:/app# ping test2PING test2 (172.18.0.3) 56(84) bytes of data.64 bytes from test2.my-bridge (172.18.0.3): icmp_seq=1 ttl=64 time=0.040 ms64 bytes from test2.my-bridge (172.18.0.3): icmp_seq=2 ttl=64 time=0.132 ms^C--- test2 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 3msrtt min/avg/max/mdev = 0.040/0.086/0.132/0.046 ms# 6. ping test1 我们发现是不通的，因为我们test1没有加入自定义的bradgeroot@811ddcb7a6fb:/app# ping test1ping: test1: Temporary failure in name resolution 由此我们可以发现，我们没创建一个容器，它都会加入默认的bridge，这是容器之间虽然通过ip是互通的，但是不能通过容器名称互通访问网络但是我们通过创建自己的bridge，然后将两个或多个容器加入此bridge，容器之间就可以通过名称网络互通（注意默认bridge和自定义bridge的区别） Docker容器的端口映射注意：本章内容是一个重点 通常我们创建的Docker容器，需要对外提供服务时，我们需要将容器的ip地址和端口暴露出去，但是我们容器的ip和端口都只能在宿主机上访问，不能被外界访问，只有宿主机关联的公网ip才可以被外网访问，这个时候我们就需要将容器的端口映射到宿主机的ip端口，这时我们通过访问宿主机的ip加我们映射好的端口，就可以访问到我们Docker容器内部的服务 具体命令如下： docker run -d -p [容器服务端口]:[宿主机需要被映射的端口] –name [容器的名称] [image名称] 使用示例如下：docker run -d -p 80:80 –name nignxweb nginx 这样，我们访问宿主机的ip加80端口，就可以映射到容器内部的80端口，从而访问服务 Docker host和none Network Namespace上面我们通过命令docker network ls查看docker的网络命名空间，发现有三个，bridge是默认链接的，host和none没有介绍，下面我们介绍这两种命名空间 none命名空间这个命名空间很少使用，因为此命名空间是非常孤立的，它不跟外界任何网络有联通，所以它的应用场景很局限，我们可以创建一个容器然后挂在到此命名空间上观察一下，它一般就是我们的容器非常机密，里面保存了各种密码必须只能进入容器内部才能查看这些信息，这种场景比较适合使用此none网络命名空间 host命名空间此命名空间我们可以创建一个容器然后关联到此命名空间，然后进入容器内部使用ip a查看网口信息，发现与宿主机上查看的信息一模一样，也就是说它是直接关联宿主机的网络命名空间，host使用也非常少，因为我们容器这样关联的话，Network Namespace就和宿主机的没有隔离，容易出现端口冲突等问题","categories":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/tags/Docker/"},{"name":"Network","slug":"Network","permalink":"https://jjw-story.github.io/tags/Network/"}],"author":"JJW"},{"title":"Docker镜像的构建和发布及容器操作","slug":"Docker镜像的构建和发布","date":"2020-04-28T13:05:38.000Z","updated":"2020-05-08T15:42:03.000Z","comments":true,"path":"2020/04/28/Docker镜像的构建和发布/","link":"","permalink":"https://jjw-story.github.io/2020/04/28/Docker镜像的构建和发布/","excerpt":"","text":"Docker镜像的构建Docker镜像的构建有两种方式，第一种是通过Container来构建，第二种是通过Dockerfile来构建，我们一般推荐通过第二种方式来构建，因为第一种我们将此镜像发不出去，我们是黑盒的，我们不知道它内部是不是有什么不安全的东西在里面，所以推荐使用第二种方式来创建 通过Container构建Docker镜像在通常的使用中，我们会在已经创建的容器中添加各种文件或操作，这样我们就希望在添加了这些文件或操作后的容器能形成一个新的容器，以便我们移植使用，这个时候我们就可以基于当前已经有的容器来构建一个Docker镜像，以便我们移植。具体步骤如下： 首先通过命令：docker container ls -a 查询出我们添加了操作的要构建镜像的Container 通过命令创建新的镜像：docker commit [旧的Container名称] [tag/新的镜像名称] 通过命令查看我们创建的新的镜像：docker image ls 通过命令查看容器分层情况：docker history [Container ID]，通过此命令查看我们旧的容器和新的容器，发现它们之间共享了很多底层容器，由此可见，我们新的容器是基于旧的容器分层之上创建的 通过Dockerfile来构建镜像这种方式我们是通过编写Dockerfile来构建镜像，使用命令：Docker build -t [tag/镜像名称] 通过docker image ls，就可以查看到我们新创建的镜像 推荐使用这种方式来构建镜像，因为这种方式我们要给别人这个镜像的时候，我们只需要把Dockerfile共享给其他人就行，而且其他人能看到这个镜像中道理有什么东西，也比较放心 Dockerfile语法FromFrom关键字是一般Dockerfile最开头的语句，它指定了我们要build的image它的 Base image 是什么，也就是它在我们哪个Base Image之上来构建我们的Image，示例如下： From scratch # scratch表示我们不需要任何的Base Image，直接从头来制作的 From centos 或者 From ubuntu：14.04 # 表示我们的Image是基于centos或者ubuntu之上来构建的 LABEL它定义的是我们的Image的 Mate data，一般Dockerfile中此信息不能少，它是帮助其他人来了解我们的Image的重要信息来源，示例如下： LABEL maintainer=”jjw-story.china@gmail.com“ # Image的作者 LABEL version=”1.0” # Image的版本 LABEL description=”描述信息“ # Image的描述信息 RUNRUN就是我们要执行的命令，一般在构建Image时，我们都需要安装一些软件，它的定义是：执行命令并创建新的 Image Layer，这个时候就需要使用RUN来实现，示例如下： RUN yum update &amp;&amp; yum install -y vim \\ yum install python 这里有一点需要注意，我们每运行一次RUN，对于我们的Image都会生成新的一层，每生成新的一层，我们在构建的时候就会新生成一层新的Container，然后基于此基础上构建新的Iamge，所以，为了美观，复杂的RUN使用反斜线换行，避免无用分层，合并多条命令成一行，合并多条命令为一行，可以使用 &amp;&amp; 符号 WORKDIRWORKDIR是设定当前工作目录的，这个有点类似在linux中通过cd来改变当前的工作目录，示例如下： WORKDIR /root WORKDIR /testWORKDIR /demoRUN pwd # 这里会输出的是 /test/demo 我们变更工作目录，尽量使用 WORKDIR，不要使用 RUN cd ，尽量是哟过绝对目录，不要使用相对目录，绝对目录更加清晰 如果我们没有此目录，Docker对自动创建 AND 和 COPYADD 和 COPY非常像，都是将本地的文件添加到Image中，使用示例： ADD test.tar.gz # 会自动解压 WORKDIR /rootADD demo.tar.gz test/ # 这里会将文件添加到 /root/test/demo 目录下 WORKDIR /rootCOPY demo.tar.gz test/ # 这里会将文件添加到 /root/test/demo 目录下 ADD 和 COPY不同的点在于，使用 ADD 会将压缩文件添加并解压掉 大部分情况，COPY由于ADD，添加远程文件/目录，使用 curl 或者 wget 来实现 ENVENV作为设置常量或环境变量之用，示例如下： ENV MYSQL_VERSION 5.6 # 设置常量RUN apt-get install -y mysql-server=”${MYSQL_VERSION}” # 引用常量 对于我们Dockerfile中上下文中有多处使用此常量，推荐使用ENV，因为我们修改的时候只需要改一次env即可 CMD 和 ENTRYPOINTCMD：设置容器启动后默认执行的命令和参数 ENTRYPOINT：设置容器启动时运行的命令 Shell格式和Exec格式首先我们了解两种格式，Shell格式和Exec格式 Shell格式是说把我们要运行的命令当成Shell的格式去运行，第一个是命令，后面的都是命令的参数等，使用示例如下： 123RUN apt-get install -y vimCMD ehco &quot;hello docker&quot;ENTRYPOINT echo &quot;hello docker&quot; Exec格式需要通过特定的格式去指明需要运行的命令，以及命令所跟的参数，使用示例如下： 123RUN [ &quot;apt-get&quot;, &quot;install&quot;, &quot;-y&quot;, &quot;vim&quot; ]CMD [ &quot;ehco&quot;, &quot;hello docker&quot; ]ENTRYPOINT [ &quot;echo&quot;, &quot;hello docker&quot;] Exec格式需要指定可执行命令的具体文件 两种方式的区别在于，例如如下示例： 1234567# 第一种Dockerfile定义，其他省略ENV name &quot;docker&quot;ENTRYPOINT [ &quot;/bin/echo&quot;, &quot;hello $&#123;name&#125;&quot;]# 第二种Dockerfile定义，其他省略ENV name &quot;docker&quot;ENTRYPOINT &quot;echo&quot;, &quot;hello $&#123;name&#125;&quot; 当我们分别通过两种格式去build Image，并且创建Container时，第一种方式打印出来的是 hello ${name}, 第二种方式打印的是 hello docker，由此可见，第一种Exec的方式它并没有解析ENV的配置，这是因为第二种Shell格式，他会默认铜鼓shell来执行命令echo，而Exec方式，它只是单纯的去执行/bin/echo这个命令，它并没有在shell环境中，所以它解析不了，如何让Exec格式也能解析呢，可以通过如下方法： 12ENV name &quot;docker&quot;ENTRYPOINT [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello $&#123;name&#125;&quot;] 这样就可以保证后面的echo命令实在shell环境中执行的，就能正常的解析占位符和变量 这两种方式都可以在RUN CMD ENTRYPOINT中使用 CMDCMD是容器启动时默认执行的命令，如果docker run指定的其他命令，CMD命令会被忽略，如果定义了多个CMD,只有最后一个会执行，如下示例： docker run -it jjw-story/hello-docker /bin.bash 如此：我们可以进入容器中，但是并不会打印我们在hello-docker中定义的CMD命令执行的东西，因为，我们在run命令后指定了其他命令， /bin/bash ENTRYPOINT让容器以应用程序或者服务的形式与运行，比如说我们启动一个数据的服务，让他作为一个后台进程一直在运行。它不会被忽略，一定会被执行，这是与CMD不同的地方，这里可以写一个shell脚本作为entrypoint启动的脚本 ENTRYPOINT是使用比较广泛的，比CMD应用的多 EXPOSE我们的容器中很多时候部署的是需要常驻内存的服务，这时我们就需要对外提供端口，此选项就是指定容器对外暴露的端口的，使用方式如下，暴露 5000 的端口 EXPOSE 5000 镜像的发布很多时候我们自己创建的镜像需要push到我们自己的仓库里以便与别人共享，这时就可以通过早docker hub上创建我们自己的仓库，来把我们创建好的镜像上传上去 首先需要在Docker Hub上创建我们自己的帐号，登陆上去之后，就可以看到我们自己的仓库，这里面都是我们自己上传的镜像 在本地找到我们的Docker Image，如果我们要上传此Image，那么我们在build我们自己的Image时，tag一定是我们在Docker Hub上创建的帐号的名称，否则我们在push时，会报没有权限的错误 本地上传首先我们使用命令在本地登陆Docker Hub，命令： docker login，然后输入我们的用户名和密码 使用命令上传：docker image push [用户名/Imag名称:tag]，注意这里tag一般是latest push完成后，就可以在Docker Hub上仓库中看到我们上传的Image 下次我们想下载此Image就可以通过命令：docker pull [用户名/Imag名称] 来下载此Image 上述方式下载的Image比较黑盒，我们不知道这里面究竟有没有其他东西，我们可以通过Docker Hub关联GitHub，将Dockerfile上传到GitHub上，然后关联起来，每次我们上传Dockerfile时，DockerHub会自动的检测到，然后build出新的Image，这样我们在下载时，就可以看到具体的Dockerfile，这里使用时我们自己百度就可以的。 上述方式DockerHub是公有的，所有人都能看到，有些公司私密的我们不需要别人看到，这时我们就需要自己搭建一个私密或者个人的Docker Hub，具体如下： 在本地使用命令搭建私有的Docker Hub，命令：docker run -d -p [端口号：端口号] –restart always –name regestry registry:2，使用示例如下：docker run -d -p [5000: 5000] –restart always –name regestry registry:2，这样我们就在本地搭建好了一个私有的Docker Hub，我们通过 docker ps 就可以查看此启动的容器 push我们自己的Image，这个时候我们Image的用户名就不能是DOckerHub的用户名了，应该是我们私有的DockerHub的IP和端口，例如：192.168.26.112:5000/hello-world，push命令同上，docker push 192.168.26.112:5000/hello-world，这样就完成了push image到本地私有库。第一次push使用可能会出现错误，因为它会认为我们是不安全的，这里我们通过百度一下来找解决方案，就是创建一个文件加入配置即可 push完成后，下次我们想下载此Image就可以通过命令：docker pull [IP：端口/Imag名称] 来下载此Image 镜像构建问题排查方法有的时候我们构建一些镜像会出现错误，这时我们就需要通过一些方式来进行具体查看问题原因，例如如下Dockerfile构建示例： Dockerfile： 1234567FROM python:2.7LABEL maintainer=&quot;JJW-STORY&quot;RUN pip install flaskCOPY app.py /appWORKDIR /appEXPOSE 5000CMD [&quot;python&quot;, &quot;app.py&quot;] 首先我们通过命令构建：docker build -t jjw-story/flask-hello-world . 发现发生如下错误： 1234Step 4/7 : COPY app.py /app ---&gt; 89f35be7aac8Step 5/7 : WORKDIR /appCannot mkdir: /app is not a directory 前面说过，docker镜像构建是基于分层的，我们构建时，每一层都会创建一个临时的container，然后基于此container来创建新的image，比如这里出现了此问题，我们就可以通过查看创建的临时container来查看内部具体错误原因 通过命令查看所有image，找到我们出现错误的上一层创建的临时image，然后基于此image启动容器 12345678910111213# 1.查找image，对应上述第四步创建的分层镜像jjw@jjw-PC:~/AppBoot/flask-hello-world$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 89f35be7aac8 2 minutes ago 906MBjjw-story/helloworld-java latest 3c378e4f1c1d 8 days ago 682Bjjw-story/hello-docker-c latest 73404f3002a9 11 days ago 756kB# 2.基于此临时镜像启动容器并使用base进入此容器~/AppBoot/flask-hello-world$ docker run -it 89f35be7aac8 /bin/bash# 3.进入容器内部后查看根目录中不存在 /app 文件夹，所以我们定位到了问题root@e9599f7da1aa:/# lsapp bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 重点命令： docker run -it 89f35be7aac8 /bin/bash 修改Dcokerfile中的错误，将 COPY app.py /app 修改为 COPY app.py /app/ 此问题即可解决 重新构建构建镜像，发现已经可以了，并可以启动容器 容器的操作exec进入正在运行的docker容器，我们有的时候需要进入正在运行的docker容器中，来查看容器服务运行的状态，日志等，就需要此操作，使用命令： docker exec -it [运行的container ID] /bin/bash 使用示例： 1234567891011jjw@jjw-PC:~$ docker exec -it 80e308651aee /bin/bashroot@80e308651aee:/app# lsapp.pyroot@80e308651aee:/# ps -ef | grep pythonroot 1 0 0 14:08 ? 00:00:00 python app.pyroot@80e308651aee:/# pythonPython 2.7.18 (default, Apr 20 2020, 19:27:10) [GCC 8.3.0] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; stop停止容器命令： docker container stop [运行的container ID/容器名称] 或者直接 docker stop [运行的container ID/容器名称] 这里可以使用容器名称来停止容器，因为docker中容器名称是唯一的 1234jjw@jjw-PC:~$ docker container stop 80e308651aee80e308651aeejjw@jjw-PC:~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES run（启动容器指定容器名称）启动容器并指定容器名称使用 –name 参数来实现，具体命令如下： docker run -d –name=[容器名称] image名称，使用示例： 1234567891011jjw@jjw-PC:~$ docker run -d --name=demo jjw-story/flask-hello-world2e9940a72f76168232e1f9bfbc560c69007a240a4ff85da9fff58e78dde5185bjjw@jjw-PC:~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2e9940a72f76 jjw-story/flask-hello-world &quot;python app.py&quot; 11 seconds ago Up 8 seconds 5000/tcp demojw@jjw-PC:~$ docker stop demo demojjw@jjw-PC:~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES start（启动已有容器或者说启动被停止的容器）已有的容器我们可以直接通过名称来启动它，默认就是后台启动，使用命令如下： docker start [容器名称]，使用示例： 12345jjw@jjw-PC:~$ docker start demo demojjw@jjw-PC:~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2e9940a72f76 jjw-story/flask-hello-world &quot;python app.py&quot; 5 minutes ago Up 3 seconds 5000/tcp demo inspect（查看容器的详细信息）可以通过此命令来查看我们启动的或者未启动的容器信息，使用命令如下： docker inspece [容器ID]，使用示例： 123456789101112131415161718192021222324252627282930jjw@jjw-PC:~$ docker inspect 2e9940a72f76[ &#123; &quot;Id&quot;: &quot;2e9940a72f76168232e1f9bfbc560c69007a240a4ff85da9fff58e78dde5185b&quot;, &quot;Created&quot;: &quot;2020-05-07T14:35:16.815307467Z&quot;, &quot;Path&quot;: &quot;python&quot;, &quot;Args&quot;: [ &quot;app.py&quot; ],... &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;ce37397299d77a4de06125c0443f242cdad381acb35f4abece69c00e68b63643&quot;, &quot;EndpointID&quot;: &quot;5c9e31916c947b9c8ef29452ab80eeb075a7c3f6ca048d669b93848f431902b1&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] 可以看到信息非常详细，中间省略，具体可以在使用时查看 logs（查看容器运行的log）使用命令：docker logs [容器ID]，使用示例： 1234567jw@jjw-PC:~$ docker logs 2e9940a72f76 * Serving Flask app &quot;app&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) 容器的资源控制我们可以对容器进行资源限制，虚拟化的技术就是我们一个在一个物理机器上创建多个虚拟机，然后对每个虚拟机进行不同的资源划分，同样对于容器技术来讲，我们的container也是运行在Linux的机器上的，机器的资源也是有限的，如果我们不对容器进行资源限制，那我们创建的这个容器就会尽可能的去占用更多的资源，直到占用满整个机器的内存，所以我们就需要限制容器所占用的资源，比如内存、CPU个数等 具体限制的类型有很多中，我们可以通过： docker run –help 命令来具体查看，这里我们只介绍重要的几种 内存和交换分区的限制内存和交换分区的限制一般是并行出现的，使用 –memory 和 –memory-swap 参数来指定，需要注意，如果我们只限制 –memory 不限制 –memory-swap，那么默认交换分区的容量和我们限制的内存容量大小是一样的，使用方式如下： docker run –memory=[自定义大小] –memory-swap=[自定义大小] -d [image名称] 123jjw@jjw-PC:~$ docker run --memory=200M -d jjw-story/flask-hello-worldWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.30f8d4297cc49d1bf7fee1f1d53bad77f3ebfcfc44eafbf0c789515cb814380e 当我们的容器占用内存超过我们的限制，容器就会宕机自动退出 限定CPU资源CPU资源的限定我们一般使用的是 –cpu-shares 参数，此参数限定的不是说占用CPU的个数，而是一个相对的权重，意思就是：日如我们有两个容器，一个容器我们设置此参数值为10,一个容器我们设置此参数值为5,如果物理CPU已经被占用满了，那么他们其实占用的是一个比例，而且是 2:1 的比例 使用方法： docker run –cpu-shares [image名称] 1234567jjw@jjw-PC:~$ docker run --cpu-shares=10 jjw-story/flask-hello-world * Serving Flask app &quot;app&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)","categories":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/tags/Docker/"},{"name":"镜像的构建和发布及容器操作","slug":"镜像的构建和发布及容器操作","permalink":"https://jjw-story.github.io/tags/镜像的构建和发布及容器操作/"}],"author":"JJW"},{"title":"Docker入门","slug":"Docker入门","date":"2020-04-20T13:05:38.000Z","updated":"2020-05-07T01:51:50.956Z","comments":true,"path":"2020/04/20/Docker入门/","link":"","permalink":"https://jjw-story.github.io/2020/04/20/Docker入门/","excerpt":"","text":"Docker入门容器技术介绍原始方式的应用程序部署有以下不足： 部署非常慢（需要找机房，安装操作系统及各种环境等） 成本非常高（有些应用程序占用的资源非常少，但是我们还是要部署一个机器，成本很高） 资源浪费（应用程序使用资源很少，导致我们的机器很多资源都在空置，造成浪费） 难于迁移和扩展（我们迁移应用的时候需要重新找机器安装各种环境，扩展同理，也需要我们准备各种环境，有的时候可以通过扩展机器配置来完成，但这样也很麻烦） 虚拟化技术出现以后： 一个物理机可以部署多个APP，每个APP可以运行在单独的一个VM中，虚拟化的优点： 资源池：一个物理机的资源分配到了不同的虚拟机中，可以节约资源 很容易扩展：在扩展的时候我们可以通过添加物理机或加虚拟机的方式来实现 虚拟化的局限性： 每个虚拟机都是一个完整的操作系统，需要给其分配资源，当虚拟机数量增多时，操作系统本身消耗的资源势必增多 容器技术解决了什么问题： 对软件和其依赖的标准化自动化打包和发布 能实现应用之间的相互隔离，类似于上述虚拟化技术的隔离，但是它的隔离没有虚拟化技术隔离的那么好 容器可以共享同一个OS（实现多个应用程序运行在一个OS上） 可以运行在很多主流的操作系统上 容器是APP层面的隔离，虚拟化是物理资源层面的隔离，我们也可以将虚拟化技术与容器技术结合使用，既对物理机器划分为不同的VM，每个VM上运行多个容器 Docker介绍Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app），更重要的是容器性能开销极低。 Docker的优点Docker 是一个用于开发，交付和运行应用程序的开放平台。Docker 使您能够将应用程序与基础架构分开，从而可以快速交付软件。借助 Docker，您可以与管理应用程序相同的方式来管理基础架构。通过利用 Docker 的方法来快速交付，测试和部署代码，您可以大大减少编写代码和在生产环境中运行代码之间的延迟。 1、快速，一致地交付您的应用程序 Docker 允许开发人员使用您提供的应用程序或服务的本地容器在标准化环境中工作，从而简化了开发的生命周期。 容器非常适合持续集成和持续交付（CI / CD）工作流程，请考虑以下示例方案： 您的开发人员在本地编写代码，并使用 Docker 容器与同事共享他们的工作。他们使用 Docker 将其应用程序推送到测试环境中，并执行自动或手动测试。当开发人员发现错误时，他们可以在开发环境中对其进行修复，然后将其重新部署到测试环境中，以进行测试和验证。测试完成后，将修补程序推送给生产环境，就像将更新的镜像推送到生产环境一样简单。 2、响应式部署和扩展 Docker 是基于容器的平台，允许高度可移植的工作负载。Docker 容器可以在开发人员的本机上，数据中心的物理或虚拟机上，云服务上或混合环境中运行。 Docker 的可移植性和轻量级的特性，还可以使您轻松地完成动态管理的工作负担，并根据业务需求指示，实时扩展或拆除应用程序和服务。 3、在同一硬件上运行更多工作负载 Docker 轻巧快速。它为基于虚拟机管理程序的虚拟机提供了可行、经济、高效的替代方案，因此您可以利用更多的计算能力来实现业务目标。Docker 非常适合于高密度环境以及中小型部署，而您可以用更少的资源做更多的事情。 Docker架构和底层技术实现docker的前生LXCLXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。相当于C++中的NameSpace。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。与传统虚拟化技术相比，它的优势在于： 与宿主机使用同一个内核，性能损耗小； 不需要指令级模拟； 不需要即时(Just-in-time)编译； 容器可以在CPU核心的本地运行指令，不需要任何专门的解释机制； 避免了准虚拟化和系统调用替换中的复杂性； 轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。 总结：Linux Container是一种轻量级的虚拟化的手段。Linux Container提供了在单一可控主机节点上支持多个相互隔离的server container同时执行的机制。Linux Container有点像chroot，提供了一个拥有自己进程和网络空间的虚拟环境，但又有别于虚拟机，因为lxc是一种操作系统层次上的资源的虚拟化。 docker并不是LXC替代品，docker底层使用了LXC来实现，LXC将linux进程沙盒化，使得进程之间相互隔离，并且能够控制各进程的资源分配。 Docker架构Docker架构可以分为三个部分 Docker Client：这个就是我们平时操作Docker的主要入口，我们输入各种命令都是通过Client来操作的，客户端可以与Docker在同一个服务 Docker Host：是我们启动了docker的机器，也就是Docker所在的服务器，这上面主要有两个重要的概念，Containers（镜像）和images（容器） Image镜像：docker镜像就是一个只读模板，比如，一个镜像可以包含一个完整的centos，里面仅安装apache或用户的其他应用，镜像可以用来创建docker容器，另外docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。 Image是文件和meta data的集合，Linux分为内核空间和用户空间，内核空间就是Linux Kernel，也称为bootfs，用户空间其实就是基于此内核空间我们创建了各种Linux的发行版本，比如Ubuntu，CentOS，Debian等，这些其实就是一个Image，被称为Base Image，在此基础上，我们可以创建更高一级的Image，Base Image是不包含Liunx内核的，所以我们这些不同的Image可以共享Linux内核，也就是bootfs。我们可以在此Base Image上安装各种不同的软件，然后就形成了新的Image，个人理解Image就是基于内核安装各种不同的软件，然后形成不同的镜像，成为Image。所以Image是可以分层的，我们可以在每一层添加和删除文件，形成新的Image。 Image的获取有两种方式，1.我们的Docker Image，可以通过手动创建Dockerfile，然后根据它的语法来写入指令，实现自己的Dockerfile。2.Pull from Registry，其实就是类似于GitHub，我们通过Pull命令来此远程仓库拉取公开的各种Dockerfile，一般是通过Docker Hub来拉取，它是一个官方的仓库，我们可以去此仓库找到我们需要的Dockerfile，下载到本地，来创建我们的专用的Docker容器。 container容器：docker利用容器来运行应用，容器是从镜像创建的运行实例，它可以被启动，开始、停止、删除、每个容器都是互相隔离的，保证安全的平台，可以把容器看做是要给简易版的linux环境（包括root用户权限、镜像空间、用户空间和网络空间等）和运行再其中的应用程序。 container是通过image创建的。container是在image之上建立的一个container layer（可读写），image是一个只读的东西，container要去运行程序或者安装软件，所以它是可写的。container和image类似与我们Java中类和实例的关系，container就是实例，image是负责app的存储和分发的，Container负责运行App。 Registry：是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。它可以理解为我们的GitHUb，仓库是集中存储镜像文件的，registry是仓库主从服务器，实际上参考注册服务器上存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag），仓库分为两种，公有参考，和私有仓库，最大的公开仓库是docker Hub，存放了数量庞大的镜像供用户下周，国内的docker pool，这里仓库的概念与Git类似，registry可以理解为github这样的托管服务 底层技术支持 Namespace docker是通过namespace实现资源隔离，它可以实现六项资源的隔离， UTS：主机与域名 IPS：信号量和消息队列和共享内容 PID：进程编号 NETWORK：网络设备、网络栈、端口等 MOUNT：挂载点，既文件系统 USER：用户和用户组 Control Group CGroup它是用来做资源限制的，主要有四大功能： 资源限制：可以对任务使用的资源总额进行限制 优先级分配：通过分配的cpu时间片数量以及磁盘IO带宽大小，实际上相当于控制了任务运行优先级 资源统计：可以统计系统的资源使用量，如cpu时长，内存用量等 任务控制：cgroup可以对任务执行挂起、恢复等操作 Union file systems Container和image的分层，Docker的存储驱动的实现是基于Union File System，简称UnionFS，他是一种为Linux 、FreeBSD 和NetBSD 操作系统设计的，把其他文件系统联合到一个联合挂载点的文件系统服务。它用到了一个重要的资源管理技术,叫写时复制。写时复制（copy-on-write），也叫隐式共享，是一种对可修改资源实现高效复制的资源管理技术。对于一个重复资源，若不修改，则无需立刻创建一个新的资源，该资源可以被共享使用。当发生修改的时候，才会创建新资源。这会大大减少对于未修改资源复制的消耗。Docker正式基于此去创建images和containers 入门使用及基本命令Image操作 查询当前所有的image：docker image ls 或者 docker images 删除image：docker image rm ‘Image ID’ 或者 docker rmi ‘Image ID’， ‘Image ID‘是通过ls命令查出的，有时候我们一个Dockerfile被build成了多个image，这时我们不能直接删除，可以加 -f 参数来强制删除，示例： 12345678910111213141516171819202122# 1.查看所有的imagejw@jjw-PC:~$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEjjw-story/hello-docker-c latest 73404f3002a9 47 minutes ago 756kBjjw-story/hello-docker-copy latest f982998b02d3 49 minutes ago 682Bjjw-story/hello-docker latest ad1e583a2447 51 minutes ago 682B# 第一种删除方案# 2.删除失败jjw@jjw-PC:~$ docker image rm f982998b02d3Error response from daemon: conflict: unable to delete f982998b02d3 (must be forced) - image is being used by stopped container 88443ef588b1# 3.强制删除jjw@jjw-PC:~$ docker image rm -f f982998b02d3Untagged: jjw-story/hello-docker-copy:latestDeleted: sha256:f982998b02d382876511b156632c77d795a8155103e4a7e2c4d67703a16be89c# 4.查看结果jjw@jjw-PC:~$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEjjw-story/hello-docker-c latest 73404f3002a9 About an hour ago 756kBjjw-story/hello-docker latest ad1e583a2447 About an hour ago 682B 如上示例，COMMAND 列展示的其实是我们在 Dockerfile CMD项 指定的命令 Container操作 创建container即实例化Image：docker run ‘Image tag’，image tag就是我们使用Dockerfile构建image时指定的tag 交互式运行container，及后台运行不自动关闭，以便我们可以再次container中进行一些操作：docker run -it ‘Image name’，如下示例： 12jjw@jjw-PC:~$ docker run -it jjw-story/hello-docker-chello docker 后台运行container：docker run -d ‘Image name’ 查看当前所有正在运行的container：docker container ls，如下示例： 查看所有的container，包括历史运行的：docker container ls -a 或者 docker ps -a，如下示例： 1234jjw@jjw-PC:~$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES979454ee2253 jjw-story/hello-docker-c &quot;/hello&quot; 4 minutes ago Exited (0) 4 minutes ago sweet_davinciad4051fc47ed jjw-story/hello-docker &quot;java /HelloWorld&quot; 4 minutes ago Created clever_goodall 上述查询结果中，COMMAND 展示的就是我们在编写Dockerfile时制定的 CMD 所对应的命令内容 删除container： docker container rm ‘Container ID’ 或者直接 docker rm ‘Container ID’ 效果是一样的，默认删除的就是container，Container ID是通用 container ls 命令查询出来的，这里的ID我们也可以只写ID的前几位，只要能区分出唯一即可，例如： 12345678910jw@jjw-PC:~$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd4d452d8cf2c jjw-story/hello-docker-c &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago competent_varahamihira6f40251fb684 jjw-story/hello-docker-c &quot;/hello&quot; 4 minutes ago Exited (0) 4 minutes ago jovial_keldyshjw@jjw-PC:~$ docker container rm 6f6fjjw@jjw-PC:~$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd4d452d8cf2c jjw-story/hello-docker-c &quot;/hello&quot; 5 minutes ago Exited (0) 5 minutes ago competent_varahamihira 查看当前所有的container，并只显示id：docker container ls -aq，此命令会显示标题。去除标题：docker container ls -a | awk {‘print$1’} 基于上述命令删除所有的container：docker rm $(docker container ls -aq) 查看所有已经退出的容器：docker container ls -f “status=exited”, 只列出ID：docker container ls -f “status=exited” -q 删除所有已退出的容器：docker rm $(docker container ls -f “status=exited” -q)","categories":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jjw-story.github.io/tags/Docker/"},{"name":"入门","slug":"入门","permalink":"https://jjw-story.github.io/tags/入门/"}],"author":"JJW"},{"title":"分布式消息队列","slug":"分布式消息队列","date":"2020-04-05T04:11:21.000Z","updated":"2020-04-26T03:04:11.347Z","comments":true,"path":"2020/04/05/分布式消息队列/","link":"","permalink":"https://jjw-story.github.io/2020/04/05/分布式消息队列/","excerpt":"","text":"MQ应用详解分布式消息队列的应用思考点 生产端的消息可靠性投递 就是有些业务场景我们需要消息是百分之百投递成功的，或者与我们的数据库一定是一个原子性的操作 消费端幂等 在生产端为了保证消息投递可靠的时候，可能会出现重复发送消息的情况，我们的消费端一定要做好消费的幂等，杜绝出现消息重复消费的情况 MQ的高可用 我们要保证MQ的节点在挂掉一个或多个，MQ还是可用的状态 MQ的低延迟 在流量非常大的时候，我们如何保证消息的低延迟 MQ的消息的可靠性 就是如果我们的消息落到了MQ中，如何保证消息肯定不会丢失，比如某个磁盘出现问题，还能使消息不丢失（一般都是使用分片、副本的概念解决） MQ消息的堆积能力 当消息量非常大，消费者消费速度跟不上的时候，我们的MQ能否堆积一个很大的消息量 扩展性等 主流的分布式消息队列目前业界主流的消息中间件有： ActiveMQ、RabbitMQ、RocketMQ、Kafka 如何进行技术选型 各个MQ的性能、优缺点、响应的业务场景、 集群架构模式，分布式、可扩展性、高可用、可维护性 综合成本问题，集群规模，人员成本（既看公司的技术栈，公司整体比较熟悉哪种MQ的使用等等的综合考虑） 未来的方向、规划、思考 JMS及其专业术语JMS（Java Message Service）规范，也就是Java消息服务，它定义了Java中访问消息中间件的接口的规范。在这里注意，JMS只是接口，并没有给予实现，实现JMS接口的消息中间件称为 “JMS Provider”，目前知名的开源 MOM （Message Oriented Middleware，也就是消息中间件）系统包括Apache的ActiveMQ、RocketMQ、Kafka，以及RabbitMQ，可以说他们都 “基本遵循” 或 “参考” JMS规范，都有自己的特点和优势 专业术语： JMS（Java Message Service）：实现JMS 接口的消息中间件； Provider（MessageProvider）：消息的生产者； Consumer（MessageConsumer）：消息的消费者； PTP（Point to Point）：即点对点的消息模型，这也是非常经典的模型； Pub / Sub（Publish/Subscribe）：，即发布/订阅的消息模型； Queue：队列目标，也就是我们常说的消息队列，一般都是会真正的进行物理存储； Topic：主题目标； ConnectionFactory：连接工厂，JMS 用它创建连接； Connection：JMS 客户端到JMS Provider 的连接； Destination：消息的目的地； Session：会话，一个发送或接收消息的线程（这里Session可以类比Mybatis的Session）； ActiveMQActiveMQ介绍ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，尽管JMS规范出台已经是很久的事情了，但是JMS在早些年的 “J2EE应用” 时期扮演着特殊的地位，可以说那个年代ActiveMQ在业界应用最广泛，当然如果现在想要有更强大的性能和海量数据处理能力，ActiveMQ还需要不断的升级版本，不断的提升性能和架构设计的重构。 就算现在我们 80% 以上的业务我们使用ActiveMQ已经足够满足需求，其丰富的API、多种集群构建模式使得他成为业界老牌消息中间件，在中小型企业中应用广泛！ 当然如果你想针对大规模、高并发应用服务做消息中间件技术选型，譬如淘宝、京东这种大型的电商网站，尤其是双11这种特殊时间，ActiveMQ可能就显得力不从心了 ActiveMQ消息投递模式 点对点：生产者向队列投递一条消息，只有一个消费者能够监听得到这条消息（PTP) 发布订阅：生产者向队列投递一条消息，所有监听该队列的消费者都能够监听得到这条消息（P/S) ActiveMQ各项指标衡量一个MOM，我们主要从三方面考虑即可，即服务性能、存储堆积能力、可扩展性。 服务性能：ActiveMQ的性能一般，在早期传统行业为王的时代还是比较流行的，但现如今面对高并发、大数据的业务场景，往往力不从心！ 数据存储：默认采用kahadb存储（索引文件形式存储），也可以使用高性能的google leveldb（内存数据库存储）， 或者可以使用MySql、Oracle进程消息存储（关系型数据库存储）。 集群架构：ActiveMQ 可以与zookeeper进行构建 主备集群模型，并且多套的主备模型直接可以采用Network的方式构建分布式集群。 ActiveMQ集群架构模式ActiveMQ最经典的两种集群架构模式，Master-Slave 、Network 集群模式 Master-Slave: 顾名思义，就是主从方式，当然这里要理解为主备的方式，也就是双机热备机制；Master Slave 背后的想法是，消息被复制到slave broker，因此即使master broker遇到了像硬件故障之类的错误，你也可以立即切换到slave broker而不丢失任何消息。 Master Slave是目前ActiveMQ推荐的高可靠性和容错的解决方案。 zookeeper的作用就是为了当绿色的主节点宕机时，进行及时切换到备份的灰色节点上去，使其进行主从角色的互换，用于实现高可用性的方案。 Master-Slave集群模型的缺点也显而易见，就是不能做到分布式的topic、queue，当消息量巨大时，我们的MQ集群压力过大，没办法满足分布式的需求 Network：这里可以理解为网络通信方式，也可以说叫Network of brokers。这种方式真正解决了分布式消息存储和故障转移、broker切换的问题。可以理解消息会进行均衡；从ActiveMQ1.1版本起，ActiveMQ支持networks of brokers。它支持分布式的queues和topics。一个broker会相同对待所有的订阅（subscription）：不管他们是来自本地的客户连接，还是来自远程broker，它都会递送有关的消息拷贝到每个订阅。远程broker得到这个消息拷贝后，会依次把它递送到其内部的本地连接上。（说白了就是部署多套MQ集群，以每个集群为单位进行通信，每个集群有自己的主从节点，有自己的zookeeper节点） Network集群模型的关键点： 这种方案需要两套或多套（Master-Slave）的集群模型才可以搞定，部署非常麻烦，需要两套或多套集群直接相互交叉配置，相互间能够感知到彼此的存在 Network虽然解决了分布式消息队列这个难题，但是还有很多潜在的问题，最典型的就是资源浪费问题，并且也可能达不到所预期的效果；通常采用Master-Slave模型是传统型互联网公司的首选，作为互联网公司往往会选择开箱即用的消息中间件，从运维、部署、使用各个方面都要优于ActiveMQ，当然ActiveMQ毕竟是 “老牌传统强Q”，Apache的顶级项目之一，目前正在进行新版本的重构（对于5.X版本）与落地。 RibbitMQRibbitMQ四种集群模式 主备模式warren（兔子窝）：经典的主备模式，正常情况由主节点提供服务，从节点只是备份数据，当主节点挂掉，从节点会代替主节点提供服务。与Active不同的是，主从实现不是通过zookeeper来实现的，它使用Haproxy实现的，Haproxy跟Nginx有点类似 远程模式早起版本提供的一种多活的模式，主要是服务异地的容灾，与上述ActiveMQ的NetWork模式非常类似，主要是在不同的地方部署不同的集群，可以提高容灾能力及处理性能，现在的版本已经不推荐使用 远距离通信和复制，可以实现多活的一种模式，简称Shovel模式，就是我们可以把消息进行不同的数据中心的复制工作，可以让跨地域的两个MQ集群互联，当其中的某一个集群处理消息处理不过来的时候，可以把消息转发到另外一个集群进行处理。集群之间的通信使用MQ的amqp协议来做通信的。此模式的配置非常麻烦，现在已经很少使用，只做了解就行。 镜像模式业界使用最为广泛的模型，非常经典的Mirror镜像模式，保证数据100%的不丢失，镜像模式其实就是数据的备份。可靠性非常高，因为数据发过来之后，它需要将数据同步到MQ镜像集群中所有的节点，所有节点都会对数据做备份存储，它的模型跟ES的很像，但是它的副本是在所有的节点上。但是缺点也很明显，就是每个节点都存储了所有的数据，如果我们要扩容的话只能增加所有节点的磁盘大小，而不能通过增加机器来实现 多活模型这种模式也是实现异地数据复制的主流模式，因为Shovel模式配置比较复杂，所以一般实现异地集群都是使用这种多活模型。这种模型需要依赖RabbitMQ的federation插件，可以实现持续可靠的AMQP数据通信，配置与应用很简单。 部署架构采用多中心模式，在两套或多套数据中心中各部署一套Rabbit集群，各中心的RabbitMQ服务除了为业务提供正常消息服务外，中心之间需要实现部分队列消息共享。集群可以是不同RibbitMQ版本的集群 RocketMQ概述RocketMQ是一款分布式、队列模型的消息中间件，由阿里巴巴自主研发的一款适用于高并发、高可靠性、海量数据场景的消息中间件。早期开源2.x版本名为MetaQ；15年迭代3.x版本，更名为RocketMQ，16年开始贡献到Apache，经过1年多的孵化，最终成为Apache顶级的开源项目，更新非常频繁，社区活跃度也非常高；RocketMQ参考借鉴了优秀的开源消息中间件Apache Kafka，其消息的路由、存储、集群划分都借鉴了Kafka优秀的设计思路，并结合自身的 “双十一” 场景进行了合理的扩展和API丰富。 优秀的能力与支持 支持集群模型、负载均衡、水平扩展能力 亿级别的消息堆积能力 采用零拷贝的原理、顺序写盘、随机读（索引文件） 丰富的API使用 代码优秀，底层通信框架采用Netty NIO框架 NameServer 代替 Zookeeper 强调集群无单点，可扩展，任意一点高可用，水平可扩展 消息失败重试机制、消息可查询 开源社区活跃度、是否足够成熟（经过双十一考验） 专业术语 Producer：消息生产者，负责产生消息，一般由业务系统负责产生消息。 Consumer：消息消费者，负责消费消息，一般是后台系统负责异步消费。 Push Consumer：Consumer的一种，需要向Consumer对象注册监听。 Pull Consumer：Consumer的一种，需要主动请求Broker拉取消息。 Producer Group：生产者集合，一般用于发送一类消息。 Consumer Group：消费者集合，一般用于接受一类消息进行消费。 Broker ： MQ消息服务（中转角色，用于消息存储与生产消费转发） 集群架构模型RocketMQ为我们提供了丰富的集群架构模型，包括单点模式、主从模式、双主模式、以及生产上使用最多的双主双从模式（或者说多主多从模式） Producer集群就是生产者集群（他们在同一个生产者组 Producer Group） Consumer集群就是消费者集群（他们在同一个消费者组 Consumer Group） NameServer集群作为超轻量级的配置中心，只做集群元数据存储和心跳工作，不必保障节点间数据强一致性，也就是说NameServer集群是一个多机热备的概念。 对于Broker而言，通常Master与Slave为一组服务，他们互为主从节点，通过NameServer与外部的Client端暴露统一的集群入口。Broker就是消息存储的核心MQ服务了。 RocketMQ作为国内顶级的消息中间件，其性能主要依赖于天然的分布式Topic/Queue，并且其内存与磁盘都会存储消息数据，借鉴了Kafka的 “空中接力” 概念，所谓 “空中接力” 就是指数据不一定要落地，RocketMQ提供了同步/异步双写、同步/异步复制的特性。在真正的生产环境中应该选择符合自己业务的配置。下面针对于RocketMQ的高性能及其瓶颈在这里加以说明： RocketMQ目前其主要瓶颈最终会落在IOPS上面，当高峰期来临的时候，磁盘读写能力是主要的性能瓶颈，为什么瓶颈在IOPS? 根本原因还是因为云环境导致的问题，云环境的SSD物理存储显然和自建机房SSD会有不小的差距，这一点我们无论是从数据库的磁盘性能、还是搜索服务（ElasticSearch）的磁盘性能，都能给出准确的瓶颈点，单机IOPS达到1万左右就是云存储SSD的性能瓶颈，这个也解释了 “木桶短板原理” 的效应，在真正的生产中，CPU的工作主要在等待IO操作，高并发下 CPU资源接近极限，但是IOPS还是达不到我们想要的效果。 与KAFKA对比既然RocketMQ有Kafka所有的优点，那么它两的区别在哪呢？ 消息投递实时性 Kafka使用短轮询方式，实时性取决于轮询间隔时间 RocketMQ使用长轮询，同Push方式实时性一致，消息的投递延时通常在几个毫秒。 严格的消息顺序 Kafka支持消息顺序，但是一台Broker宕机后，就会产生消息乱序 RocketMQ支持严格的消息顺序，在顺序消息场景下，一台Broker宕机后，发送消息会失败，但是不会乱序 namesrv VS zk kafka和rocketMq在协调节点选择上的差异，kafka通过zookeeper来进行协调，而rocketMq通过自身的namesrv进行协调。 kafka在具备选举功能，在Kafka里面，Master/Slave的选举，有2步：第1步，先通过ZK在所有机器中，选举出一个KafkaController；第2步，再由这个Controller，决定每个partition的Master是谁，Slave是谁。因为有了选举功能，所以kafka某个partition的master挂了，该partition对应的某个slave会升级为主对外提供服务。 rocketMQ不具备选举，Master/Slave的角色也是固定的。当一个Master挂了之后，你可以写到其他Master上，但不能让一个Slave切换成Master。那么rocketMq是如何实现高可用的呢，其实很简单，rocketMq的所有broker节点的角色都是一样，上面分配的topic和对应的queue的数量也是一样的，Mq只能保证当一个broker挂了，把原本写到这个broker的请求迁移到其他broker上面，而并不是这个broker对应的slave升级为主。 吞吐量 kafka在消息存储过程中会根据topic和partition的数量创建物理文件，也就是说我们创建一个topic并指定了3个partition，那么就会有3个物理文件目录，也就说说partition的数量和对应的物理文件是一一对应的。 rocketMq在消息存储方式就一个物流问题，也就说传说中的commitLog，rocketMq的queue的数量其实是在consumeQueue里面体现的，在真正存储消息的commitLog其实就只有一个物理文件。 kafka的多文件并发写入 VS rocketMq的单文件写入，性能差异kafka完胜可想而知。 kafka的大量文件存储会导致一个问题，也就说在partition特别多的时候，磁盘的访问会发生很大的瓶颈，毕竟单个文件看着是append操作，但是多个文件之间必然会导致磁盘的寻道。 在性能上Kafka是完胜的 KAFKA介绍Kafka是LinkedIn开源的分布式消息系统，目前归属于Apache顶级项目 主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始就是用于日志收集 0.8版本开始支持复制，不支持事务，对消息的重复、丢失、错误没有严格要求，适合产生大量数据的互联网服务的数据收集业务 特点 有分布式的特性，就是支持分区的概念，一个主题下可以有多个分区 有跨平台的特性，支持不同语言的客户端 堆积能力特别强，且并不影响消息的接收和发送 实时性非常强 高性能的原因（重点）顺序写就是顺序写盘，可以提高磁盘的利用率，就是一个一个的写，而不是随机写，这样会大大提高写的性能。 每个topic有不同的分区，而每个分区下包含若干个只能追加写的提交日志：新消息被追加到文件的最末端。最直接的证明就是Kafka源码中只调用了FileChannel.write(ByteBuffer)，而没有调用过带offset参数的write方法，说明它不会执行随机写操作。 Page Cache首先，Kafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache，同时标记Page属性为Dirty。当读操作发生时，先从PageCache中查找，如果发生缺页才进行磁盘调度，最终返回需要的数据。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。同时如果有其他进程申请内存，回收PageCache的代价又很小，所以现代的OS都支持PageCache。 使用PageCache功能同时可以避免在JVM内部缓存数据，JVM为我们提供了强大的GC能力，同时也引入了一些问题不适用与Kafka的设计。 如果在Heap内管理缓存，JVM的GC线程会频繁扫描Heap空间，带来不必要的开销。如果Heap过大，执行一次Full GC对系统的可用性来说将是极大的挑战。 所有在在JVM内的对象都不免带有一个Object Overhead(千万不可小视)，内存的有效空间利用率会因此降低。 所有的In-Process Cache在OS中都有一份同样的PageCache。所以通过只在PageCache中做缓存至少可以提高一倍的缓存空间。 如果Kafka重启，所有的In-Process Cache都会失效，而OS管理的PageCache依然可以继续使用。 零拷贝首先介绍一下传统的网络I/O操作流程，大体上分为以下4步： OS从硬盘把数据读到内核区的PageCache。 用户进程把数据从内核区Copy到用户区。 然后用户进程再把数据写入到Socket，数据流入内核区的Socket Buffer上。 OS再把数据从Buffer中Copy到网卡的Buffer上，这样完成一次发送。 整个过程一共经历了四次拷贝，同一份数据在内核Buffer与用户Buffer之间重复拷贝，效率低下。其中2、3两步没有必要，完全可以直接在内核区完成数据拷贝。 零拷贝技术就是省略了第2、3步，不难看出，Kafka的设计初衷是尽一切努力在内存中完成数据交换，无论是对外作为一整个消息系统，或是内部同底层操作系统的交互。如果Producer和Consumer之间生产和消费进度上配合得当，完全可以实现数据交换零I/O。这也就是我为什么说Kafka使用“硬盘”并没有带来过多性能损失的原因。 主要特点 同时为发布和订阅提供高吞吐量。据了解，Kafka每秒可以生产约25万消息（50 MB），每秒处理55万消息（110 MB）。 可进行持久化操作。将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。通过将数据持久化到硬盘以及replication防止数据丢失。 分布式系统，易于向外扩展。所有的producer、broker和consumer都会有多个，均为分布式的。无需停机即可扩展机器。 消息被处理的状态是在consumer端维护，而不是由server端维护。当失败时能自动平衡。 支持online和offline的场景。 Kafka的架构Kafka的整体架构非常简单，是显式分布式架构，producer、broker（kafka）和consumer都可以有多个。Producer，consumer实现Kafka注册的接口，数据从producer发送到broker，broker承担一个中间缓存和分发的作用。broker分发注册到系统中的consumer。broker的作用类似于缓存，即活跃的数据和离线处理系统之间的缓存。客户端和服务器端的通信，是基于简单，高性能，且与编程语言无关的TCP协议。 基本概念 Topic：特指Kafka处理的消息源（feeds of messages）的不同分类。 Partition：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。 Message：消息，是通信的基本单位，每个producer可以向一个topic（主题）发布一些消息。 Producers：消息和数据生产者，向Kafka的一个topic发布消息的过程叫做producers。 Consumers：消息和数据消费者，订阅topics并处理其发布的消息的过程叫做consumers。 Broker：缓存代理，Kafka集群中的一台或多台服务器统称为broker。 发送消息的流程 Producer根据指定的partition方法（round-robin、hash等），将消息发布到指定topic的partition里面 kafka集群接收到Producer发过来的消息后，将其持久化到硬盘，并保留消息指定时长（可配置），而不关注消息是否被消费。 Consumer从kafka集群pull数据，并控制获取消息的offset kafka的优秀设计从kafka的吞吐量、负载均衡、消息拉取、扩展性来说一说kafka的优秀设计。 高吞吐高吞吐是kafka需要实现的核心目标之一，为此kafka做了以下一些设计： 内存访问：直接使用 linux 文件系统的cache，来高效缓存数据，对数据进行读取和写入。 数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能。 zero-copy：减少IO操作步骤，采用linux Zero-Copy提高发送性能。传统的数据发送需要发送4次上下文切换，采用sendfile系统调用之后，数据直接在内核态交换，系统上下文切换减少为2次。根据测试结果，可以提高60%的数据发送性能。Zero-Copy详细的技术细节可以参考：https://www.ibm.com/developerworks/linux/library/j-zerocopy/ 对消息的处理：支持数据批量发送、支持数据压缩机制 主题分区：Topic划分为多个partition，提高生产/消费端处理消息的parallelism（并行度），数据在磁盘上存取代价为O(1)。kafka以topic来进行消息管理，每个topic包含多个part（ition），每个part对应一个逻辑log，有多个segment组成。每个segment中存储多条消息，消息id由其逻辑位置决定，即从消息id可直接定位到消息的存储位置，避免id到位置的额外映射。每个part在内存中对应一个index，记录每个segment中的第一条消息偏移。发布者发到某个topic的消息会被均匀的分布到多个part上（随机或根据用户指定的回调函数进行分布），broker收到发布消息往对应part的最后一个segment上添加该消息，当某个segment上的消息条数达到配置值或消息发布时间超过阈值时，segment上的消息会被flush到磁盘，只有flush到磁盘上的消息订阅者才能订阅到，segment达到一定的大小后将不会再往该segment写数据，broker会创建新的segment。 负载均衡 producer根据用户指定的算法，将消息发送到指定的partition 存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上 多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over 通过zookeeper管理broker与consumer的动态加入与离开 消息的拉取 简化kafka设计（由于kafka broker会持久化数据，broker没有内存压力，因此，consumer非常适合采取pull的方式消费数据） consumer根据消费能力自主控制消息拉取速度 consumer根据自身情况自主选择消费模式，例如批量，重复消费，从尾端开始消费等 可扩展性当需要增加broker结点时，新增的broker会向zookeeper注册，而producer及consumer会根据注册在zookeeper上的watcher感知这些变化，并及时作出调整。 KAFKA应用场景 消息队列：比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统一般吞吐量相对较低，但是需要更小的端到端延时，并常常依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMQ或RabbitMQ。 行为跟踪：Kafka的另一个应用场景是跟踪用户浏览页面、搜索及其他行为，以发布-订阅的模式实时记录到对应的topic里。那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。 元信息监控：作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。 日志收集：日志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。 流处理：这个场景可能比较多，也很好理解。保存收集流数据，以提供之后对接的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行阶段性处理，汇总，扩充或者以其他的方式转换到新的topic下再继续后面的处理。例如一个文章推荐的处理流程，可能是先从RSS数据源中抓取文章的内容，然后将其丢入一个叫做“文章”的topic中；后续操作可能是需要对这个内容进行清理，比如回复正常数据或者删除重复数据，最后再将内容匹配的结果返还给用户。这就在一个独立的topic之外，产生了一系列的实时数据处理的流程。Strom和Samza是非常著名的实现这种类型数据转换的框架。 事件源：事件源是一种应用程序设计的方式，该方式的状态转移被记录为按时间顺序排序的记录序列。Kafka可以存储大量的日志数据，这使得它成为一个对这种方式的应用来说绝佳的后台。比如动态汇总（News feed） 持久性日志（commit log）：Kafka可以为一种外部的持久性日志的分布式系统提供服务。这种日志可以在节点间备份数据，并为故障节点数据回复提供一种重新同步的机制。Kafka中日志压缩功能为这种用法提供了条件。在这种用法中，Kafka类似于Apache BookKeeper项目。","categories":[{"name":"MQ","slug":"MQ","permalink":"https://jjw-story.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://jjw-story.github.io/tags/MQ/"}],"author":"JJW"},{"title":"Shell","slug":"shell","date":"2020-03-22T09:58:31.000Z","updated":"2020-04-10T02:35:29.211Z","comments":true,"path":"2020/03/22/shell/","link":"","permalink":"https://jjw-story.github.io/2020/03/22/shell/","excerpt":"","text":"Shellshell介绍Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言Shell是命令解释器，用来解释用户对操作系统的操作，也就是将我们用户执行的命令，翻译给内核，内核根据命令执行的结果，将结果反馈给用户Shell 脚本（shell script），是一种为 shell 编写的脚本程序，业界所说的 shell 通常都是指 shell 脚本Shell有很多种，我们一般使用的都是 bash Linux的启动过程BLOS - MBR - BootLoader(grub) - kernel - systemd - 系统初始化 - shell BLOS：基本的输入输出系统，这个功能是在主板上的，通过BLOS来选择引导的介质，一般引导的介质有两种，一是早起使用到的光盘，二是硬盘。现在更多用的是网络的方式去引导 MBR：硬盘的主引导记录部分，硬盘是不是可以引导， 是通过这部分来确定的 BootLoader(grub)： 这里就是Linux的部分了，Linux的部分首先不是内核的工作引导，而是通过grub这样的一个软件来引导，grub我们在Linux中称为BootLoader，主要用来启动和引导内核的一个工具。我们可以简单理解为BootLoader是用来选择哪一个内核及选择指定内核版本的，选定后我们就要启动内核了 kernel：内核 systemd：Linux的一号进程，如果是CentOS7以下的版本，头号进程是 init 进程。在systemd中，系统初始化的过程一部分是通过配置文件完成的，一部分是通过shell完成的，init中，系统初始化过程都是通过shell完成的 Shell脚本的格式UNIX的哲学：一条命令只做一件事 为组合命令和多次执行，使用脚本文件来保存需要执行的命令 然后赋予该文件的执行权限（chmod u+rx filename） Shell脚本的作用就是将一系列的命令操作整合在一个文件里，然后再下次执行的时候可以直接执行此文件，而不需要我们一步一步的执行所有命令 一般我们都是使用bash的方式来执行我们的脚本文件，我们的shell脚本文件一般使用 .sh 作为文件的后缀 标准Shell脚本需要包含的元素 Sha-bang：就是每一个shell脚本文件的内容开头 以 #! 开头，是一个声明作用，如果我们使用 【bash 文件名.sh】 的方式来执行，那么此内容变不被识别，如果使用 【./文件名.sh】 的方式来执行，那么此文件头就表示告诉Linux系统，此文件需要使用 bash 脚本的，既告诉Linux，此文件是一个bash的脚本。下面是一般Sha-bang的内容： 1#!/bin/bash 命令：这里说的就是我们之前学习的那么多的Linux命令 “#” 开头的注释。在我们的shell脚本中，# 开头的行表示内容是注释 赋予文件可执行的权限。例如 chmod u+x aa.sh 执行命令的方式： bash ./filename.sh ./filename.sh source ./filename.sh .filename.sh 这里只需要注意：命令我们可以在一行中使用 ; 号隔开来写多个命令，但是一般我们是通过换行的方式来写多个命令","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://jjw-story.github.io/tags/Shell/"}],"author":"JJW"},{"title":"逻辑卷管理","slug":"逻辑卷管理","date":"2020-03-15T07:37:55.000Z","updated":"2020-03-15T09:28:57.415Z","comments":true,"path":"2020/03/15/逻辑卷管理/","link":"","permalink":"https://jjw-story.github.io/2020/03/15/逻辑卷管理/","excerpt":"","text":"逻辑卷管理什么是逻辑卷许多Linux使用者安装操作系统时都会遇到这样的困境：如何精确评估和分配各个硬盘分区的容量，如果当初评估不准确，一旦系统分区不够用时可能不得不备份、删除相关数据，甚至被迫重新规划分区并重装操作系统，以满足应用系统的需要 LVM逻辑卷相当于在传统的硬盘的底层上面在叠一层，把上面的这一层也当做一个硬盘来对待，只是这个硬盘是一个虚拟的硬盘，RAID其实就是一个逻辑卷的应用，Linux中默认使用的就是根目录，逻辑卷去管理磁盘的 增加逻辑卷增加逻辑卷首先我们首先需要添加物理硬盘，然后对添加的物理硬盘做好分区，注意：这里可以添加多块硬盘，然后我们建立上层系统，将几块硬盘做一个整合，整合成一个物理卷，然后再上层查看磁盘状态的时候，我们发现这几个硬盘它是一个整体，只有一块硬盘，这样我们就可以通过上层的系统将这块硬盘划分成不同的逻辑卷（比如，给根目录划分挂载，给boot目录划分挂载，给usr划分挂载，将这些目录进行隔离开，这些隔离开的空间就称为逻辑卷） 下面是具体步骤： 12345678910111213141516171819202122232425262728293031323334# 1.将多块分区组成一个物理卷pvcreate /dev/sdb1 /dev/sdc1 /dev/sdd1# 注意这里可以使用简写，通配符pvcreate /dev/sd[b,c,d]1# 2.查看我们创建的物理卷，会发现我们添加的三块硬盘已经创建成了三个物理卷，但是他们的 VG 选项都是空的，VG选项表示他们属于哪个卷组，这里我们还没有对他们设置卷组pvs# 3.对物理卷设置卷组vgcreate vg1 /dev/sdb1 /dev/sdc1 /dev/sdd1-- 这里 vg1 表示卷组的名称# 4.创建成功后我们继续通过 pvs 命令查看，发现这三个分区的 VG 选项就有了值，是我们设置的vg1-- 注意一个分区只能属于一个卷组# 5.我们可以通过 vgs 命令来插件当前机器的卷组，来查看我们创建的卷组信息vgs-- 注意查询出来的 #LV 选项表述的就是此物理卷创建了几个逻辑卷，一般我们安装LINUX系统时，会默认创建一个逻辑卷，CENTOS，它里面会有两个逻辑卷，一般默认都是 / 和 boot 逻辑卷，我们先创建的物理卷这里会显示为0，表示还没有创建逻辑卷# 6.现在我们就可以创建逻辑卷了# 使用命令：lvcreate -L [逻辑卷大小] -n [逻辑卷名称] [物理卷名称]lvcreate -L 100M -n lv1 vg1# 7.现在通过lvs查看逻辑卷信息，我们发现里面已经有我们创建好的逻辑卷信息，然后通过vgs查看物理卷，发现vg1这个物理卷已经有了逻辑卷，既 #LV 选项值为1lvsvgs# 8.逻辑卷的使用，使用方法还是现将逻辑卷进行格式化，然后创建目录进行挂载# 8.1 创建文件夹mkdir /mnt/test# 8.2 格式化逻辑卷mkfs.xfs /dev/vg1/lv1# 8.3 mount挂载挂载成功后，此逻辑卷就可以正常使用 扩充逻辑卷很多时候当我们物理卷够用的情况下但是逻辑卷大小分配太小，我们可以直接从物理卷上扩充逻辑卷，有的情况是物理卷也太小，需要先扩充物理卷然后扩充逻辑卷，下面是扩充展示： 1234567891011121314151617181920212223242526# 1.扩充物理卷组首先添加磁盘，然后对新添加的磁盘进行分区，例如这里划分分区为 /dev/sde1# 2.将新的分区划分给需要扩充的物理卷组，下面命令 vg1 为物理卷组名称，后面是分区名称vgextend vg1 /dev/sde1# 3.使用pvs命令查看是否划分成功pvs# 4.使用vgs查看物理卷信息vgs-- 这里我们发现 vg1 物理卷的大小已经发生了变化，增大了我们新加入的磁盘分区大小# 5.扩充我们要扩充的逻辑卷, 下面命令 +10G 表示添加10G大小的空间给逻辑卷，后面是逻辑卷名称-- 注意：我们物理卷够用的时候，可以直接从这里开始扩充逻辑卷就好，不需要扩充物理卷lvextend -L +10G /dev/vg1/lv1# 6.使用lvs查看逻辑卷大小是否改变，这里发现已经被扩大lvs这里先不要着急，我们虽然看到逻辑卷已经被扩大，但是我们还没有告诉文件系统，文件系统还认为我们的大小没有变，这里需要告诉文件系统# 7.告诉文件系统xfs_growfs /dev/vg1/lv1# 8.使用 df -h 查看文件夹是否扩容成功df -h","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"逻辑卷管理","slug":"逻辑卷管理","permalink":"https://jjw-story.github.io/tags/逻辑卷管理/"}],"author":"JJW"},{"title":"高效员工起航训练营","slug":"高效员工起航训练营","date":"2020-02-16T11:00:00.000Z","updated":"2020-02-18T03:37:47.358Z","comments":true,"path":"2020/02/16/高效员工起航训练营/","link":"","permalink":"https://jjw-story.github.io/2020/02/16/高效员工起航训练营/","excerpt":"","text":"高效员工起航训练营如何使执行效果达到预期（以终为始） 明确任务执行完成的目标，以任务实现达到的效果为目标，而不是以执行任务为目标 任务执行效果未达到预期，应以实际证据来支撑未达成目标的原因，而不是临时应付造原因 有效执行是第一有效的方案（积极主动） 接收到任务时，应该立即投入到执行中，不要拖拉，总在快要到交付时间的时候才开始，这样可以完成的记住任务所有的需求或要求，避免拖延执行而忘记某些需求点，导致目标实现效果不佳 即使在因为某些原因而不能立即执行，也应该将当时构思的方案及需求点记录下来，避免执行时丢失某些点 该吃吃该喝喝，该睡觉就好好睡，该玩就好好玩，拖延会造成在做任何事情都会顶着雷，并且随时会爆炸，导致每一种生活都受影响 如何在执行时获得有利帮助 及时汇报工作进展，当遇到个人无法解决的问题时，应及时向领导寻求帮助，使领导协调相应的资源及帮助，不要自己蒙头干，最后还完不成任务（重点：站会的目的，此处解释的淋漓尽致） 遇到问题一定要如实汇报，即使已解决的问题，也要即使汇报，汇报时要详细说明自己遇到的问题情况，并且如何解决的，可以汇报一些自己的收获，这是一种让领导认可你的方式 协调工作可以即使寻求领导帮助，自己协调完成也应该汇报给领导，得到领导的认可和指导 如何做到灵活执行 执行不是按照自己的意识就头脑发热立即开始执行，接到任务一定要分析任务，分析任务完成需要达到的目标，想清楚什么方式能达到最好的执行效果（重点：仔细分析，考虑到影响的结果及后期会产生的问题，全面考虑一下） 如何有效完成上司安排的日常任务（要事第一） 其实就是大石头，运用好的时间管理工具，任务要分清主次，紧急重要、紧急不重要、重要不紧急、不重要不紧急。 千万不要顾此失彼，重要的事情细心的办，分清主次，并且学会寻求帮助及遇到冲突及时反馈 如何明确有效倾听的内涵（知彼解己-同理心交流） 不要随意打断别人的话，保持眼神接触 在需要表达自己想法时，用请原谅来开头 及时察觉对方的情绪 重要内容记录笔记 社交行为风格与沟通技巧 与分析型人沟通:沟通时提供可靠的数据和信息，并且有条理的分析，尽量能有书面的提案，因为沟通者也需要时间思考和决定 与亲和型人沟通:有话好好说，有事好商量（发现我们组的人性格特性十分明显） 与干劲型人沟通:开门见山，态度真诚，少周旋，提供足够信息判断选择（与分析型不太好区分） 与表达型人沟通:这种类型人沟通时喜欢制造热烈与参与的气氛，乐于分享自己的梦想与激情，沟通喜欢激励、主动，比较关注他人对他的意见和看法。这这种类型人最好沟通，就是听他说，听他说，等他说的差不多了，然后你在绕回来直接问他最终的抉择 这一节有这些就够了（我非常确认我的领导们是什么样类型的） 如何与上司沟通？ 要有效的与领导沟通，需要我们仔细倾听，认真观察，理解上司的沟通风格，把握三个问题： 你的上司是倾听者还是阅读者？倾听者喜欢先听后度，阅读者喜欢先看到书面报告，再讨论 上司喜欢看详实材料和数据还是只看看概述？摸清楚之后，使用调整使用对应的策略 上司希望收到新消息的频率如何？了解上司是不是喜欢在某些特点点希望收到新消息，以及会不会不同的项目又不同的对待，摸清楚领导期望收到消息的频率，然后根据要求及时汇报 如何提高沟通效率 讨论期限要确切，避免尽快，或下周什么时候之类模糊的用语，给予确切的时间（重点注意） 坦陈自己能做到什么，做不到什么 要明确自己的目标 不明白就问，同时问清下次何时有机会沟通，因为之后可能还会想到其他问题（重点注意） 工作遇到问题，主动请教 遇到问题要正确面对，不要怕问，不懂的要及时请教别人，避免小问题拖成大问题 请教别人时需要注意： 确定请教对象，专业/专家，省时高效 选择合适的时间：不在别人忙碌时打扰别人 组织语言，确认重点：快速让别人清楚你要咨询什么 尽量一次问清楚：避免模棱相克和反复请教 请教问题时一定要礼貌客气：让别人感觉到你对他的尊重 这节是好东西，实际存在的问题（重点：一定要注意） 高效的与领导沟通 沟通前做好准备，沟通时做好确认（要准备好工作本身的相关资料，并且了解前因后果，这个是重点，遇到好几次了，一定要了解前因后果） 不能慌乱，实事求是。可以说目前是。。。情况，具体我会去核实一下 准备好方案，学会给领导做选择题（这个得慢慢来） 内容重点突出，直击要点，逻辑清晰（千万不要在自己都捋不清的情况下直接去找领导，这个也是重点） 先讲结论，再展开说明（这是一个技巧，要学习把握，重点） 注意沟通方式和态度，意见不一致不要发生激烈顶撞，先尊重和认可领导，在展开讨论 如何增加人脉建立的渠道 珍惜各种聚会及活动的机会，能增加个人曝光率 树立帮助观念这集不喜欢。。。 如何做好人脉的维护工作 有效打破规范（要注意礼仪礼貌，多听少说，尊重对方的想法和愿望，摸清楚对方的兴趣所在，意见不同的地方，要先肯定，再反问，后共识。要真诚赞美对方，发现对方的优点并赞美） 自己人脉圈内的人员要经常来往 如何与上级或同事保持良好的人际关系 如何与上级相处： 最大效率原则 最佳方式原则（开诚布公，表现自己的诚意） 最佳时机原则（选择合适的时间和地点） 及时性原则（及时与上级保持沟通，保证工作的时效性） 将心比心原则（站在领导角度替领导考虑） 尊敬原则（谦虚但不怯场） 道德原则（不说坏话） 特事特办原则（必要时可以越级） 与同事相处 三心两意（积极、知人、自信、主动、诚恳） 沟通合作只冲突处理 应对冲突的三个步骤 确定目标（是要坚持己见，还是满足对方，还是双赢） 选择策略 采取行动这里着重说采取策略： 坚持己见（运用的重点是要：1.坚定和明确立场。2.语气不可模棱两可。3.清晰果断表名自己的期望、观点。4.必要时说明不遵从的可能后果） 拖延回避（不表态、不退让、不采取行动）当局势不利于自己，对方比自己更着急，这样处理可以争取时间创造有利局面，并争取筹码 妥协退让（这里注意不要让对方感觉自己是经过长时间思考、放弃自己珍惜的事务后才做的决定。表明放弃自己的立场是为了想与对方维系长远关系、对方很重要等等） 寻求共赢（敞开心扉，齐心协力找最佳解决方式），双赢思想，这个不做赘述 如何确定问题根源理想工作八步法应该比这个更加详细","categories":[{"name":"高效员工起航训练营","slug":"高效员工起航训练营","permalink":"https://jjw-story.github.io/categories/高效员工起航训练营/"}],"tags":[{"name":"高效员工起航训练营","slug":"高效员工起航训练营","permalink":"https://jjw-story.github.io/tags/高效员工起航训练营/"}],"author":"JJW"},{"title":"Elasticsearch-核心技术四","slug":"Elasticsearch-核心技术四","date":"2020-01-20T12:00:00.000Z","updated":"2021-04-12T03:29:36.464Z","comments":true,"path":"2020/01/20/Elasticsearch-核心技术四/","link":"","permalink":"https://jjw-story.github.io/2020/01/20/Elasticsearch-核心技术四/","excerpt":"","text":"写入速度优化在ES的默认设置下，是综合考虑数据可靠性与搜索实时性、写入速度等因素的。当离开默认设置、追求极致的写入速度时，很多是以牺牲可靠性和搜索实时性为代价的。有时候务上对数据可靠性和搜索实时性要求并不高，对写入速度要求很高，此时可以调整一些策略，最大化写入速度。 接下来的优化基于集群正常运行的前提下，如果是集群首次批量导入数据，则可以将副本数设置为 0， 导入完毕再将副本数调整回去，这样副分片只需要复制，节省了构建索引过程。 综合来说，提升写入速度从以下几方面入手： 加大 translog flush 间隔，目的是降低 iops、writeblock。 加大 index refresh 间隔，除了降低I/O，更重要的是降低了 segment merge 频率。 调整 bulk 请求。 优化磁盘间的任务均匀情况，将 shard 尽量均匀分布到物理主机的各个磁盘。 优化节点间的任务分布，将任务尽量均匀地发到各节点。 优化 Lucene 层建立索引的过程，目的是降低 CPU 占用率及I/O，例如，禁用 _all 字段。 translog flush 间隔调整从ES 2.X 开始，在默认设置下，translog 的持久化策略为: 每个请求都 flush。这是影响写入速度的最大因素。但是只有这样，写操作才有可能是可靠的。如果系统接受一定概率的数据丢失（例如，数据写入主分片成功，尚未复制到副分片时，主机断电。由于数据既没有刷到 Lucene, translog 也没有刷盘，恢复时 translog 没有这个数据，数据丢失），则调整 translog 持久化策略为周期性和一定大小的时候 flush。我们可以加大到 5s 到 30s。 索引刷新间隔 refresh_interval默认情况下索引的 refresh_interval 为 1 秒，这意味着数据写 1 秒后就可以被搜到，每次索引的 refresh 会产生一个新的 Lucene 段，这会导致频繁的 segment merge 行为，如果不需要这么高的搜索实时性，应该降低索引 refresh 周期，例如：index.refresh_interval: 120s。 段合并优化segment merge 操作对系统I/O和内存占用都比较高，ES 2.0 开始，merge 行为不再由 ES 控制，而是由 Lucene 控制。 我们可以配置段合并的最大线程数，以及每层分段的数量，取值越小则最终 segment 越少，因此需要 merge 的操作更多，可以考虑适当增加此值。 indexing bufferindexing buffer 在为 doc 建立索引时使用，当缓冲满时会刷入磁盘，生成一个新的 segment，这是除 refresh_interval 刷新索引外，另一个生成新 segment 的机会。每 shard 有自己的 indexing buffer，下面的这个 buffer 大小的配置需要除以这个节点上所有 shard 的数量： indices.memory.index_buffer_size 在执行大量的索引操作时，indices.memory.index_buffer_size 的默认值设置可能不够，这和用堆内存、单节点上的 shard 数量相关，可以考虑适当增大该值。 使用 bulk 请求批量写比单个索引请求只写单个文档的效率高得多，但是要注 bulk 请求的整体字节数不要太大，太大的请求可能会给集群带来内存压力，因此每个请求最好避免超过几十兆字节，即使较大的请求看上去执行得更好。 建立索引的过程属于计算密集型任务，应该使用固定大小的线 池配置，来不及处理的任务放入队列。线程池最大线程数 应配置为 CPU 核心数＋ ，这也是 bulk 线程地的默认设置，可以避免过多的上下文切换 队列大小可以适当增加，但一定要严格控制大小，过大的队列导致较高的 GC 压力，并可能导致 FGC 频繁发生。 磁盘间的任务均衡如果部署方案是为 path data 配置多个路径来使用多块磁盘，ES 在分配 shard 时，落到各磁盘上的 shard 可能并不均匀，这种不均匀能会导致某些磁盘繁忙，利用率在较长时间内持续达到 100%，这种不均匀达到一定程度会对写入性能产生负面影响。 ES 理多路径时，会预估 shard 会使用的空间，从磁盘可用空间中减去这部分。这种机制只存在于一次索引创建的过程中，下一次的索引创建，磁盘可用空间并不是上次做完减法的结果，这也可以理解，毕竟预估是不准的，一直减下去空间很快就减没了。但是最终效果是，这种机制并没有从根本上解决问题，即使没有完美的解决方案，这种机制的效果也不够好。为此，我们为 ES 增加了两种策略： 简单轮询：在系统初始阶段，简单轮询的效果是最均匀的。 基于可用空间的动态加权轮询：以可用空间作为权重，在磁盘之间加权轮询。 索引过程调整和优化自动生成 doc ID通过 ES 写入流程可以看出，写入 doc 时如果外部指定了 id ，则 ES 会先尝试读取原来 doc的版本号，以判断是否需要更新。这会涉及一次读取磁盘的操作，通过自动生成 doc ID 可以避免这个环节。 调整字段 Mappings 减少字段数量，对于不需要建立索引的宇段，不写入 ES。 将不需要建立索引的字段 index 属性设置为 not_analyzed 或 no。对字段不分词，或者不索引，可以减少很多运算操作，降低 CPU 占用。尤其是 binary 类型，默认情况下占用 CPU 非常高，而这种类型进行分词通常没有什么意义。 减少字段内容长度，如果原始数据的大段内容无须全部建立索引，则可以尽量减少不必要的内容。 使用不同的分析器（analyzer），不同的分析器在索引过程中运算复杂度也有较大的差异。 调整_source字段_source 字段用于存储 doc 原始数据，对于部分不需要存储的字段，可以通过 includes excludes 过滤，或者将_source禁用，一般用于索引和数据分离。 这样可以降低 I/O 的压力，不过实际场景中大多不会禁用 _source字段，而即使过滤掉某些字段，对于写入速度的提升作用也不大，满负荷写入情况下，基本是 CPU 先跑满了，瓶颈在于CPU。 禁用 _all 字段ES 6.0 _all 字段默认为不启用 ，而此前的版本中 _all 字段默认是开启的。_all字段中包含所有字段分词后的关键词，作用是可以在搜索的时候不指定特定字段，从所有字段中检索。ES 6.0 默认禁用 all 字段主要有以下几点原因： 由于需要从其他的全部字段复制所有宇段值，导致 all 字段占用非常大的空间。 all 字段有自己的分析器，在进行某些查询时（例如 ，同义词 〉，结果不符合预期. 因为没有匹配同一个分析器。 由于数据重复引起的额外建立索引的开销。 想要调试时，其内容不容易检查。 有些用户甚至不知道存在这个字段，导致了查询混乱。 有更好的替代方法。 对 Analyzed 的字段禁用 NormsNorms 用于在搜索时 doc 的评分，如果不需要评分 ，则可以将其禁用： 1\"title\": &#123; \"type \": \"string\", \"norms\": &#123;\"enabled\": false&#125;&#125; index_options 设置index_options 于控制在建立倒排索引过程中，哪些内容会被添加到倒排索引，例如，doc数量、词频、positions offsets等信息，优化这些设置可以一定程度降低索引过程中的运算任务，节省 CPU 占用率。 搜索速度优化为文件系统 cache 预留足够的内存在一般情况下，应用程序的读写都会被操作系统 cache（除了 direc 方式），cache 保存在系统物理内存中（线上应该禁用 swap），命中 cache 可以降低对磁盘的直接访问频率。搜索很依赖对系统 cache 的命中，如果某个请求需要从磁盘读取数据，则一定会产生相对较高的延迟。应该至少为系统 cache 预留一半的可用物理内存，更大的内存有更高的 cache 命中率。 系统cache主要是给 doc values 来用的。 使用更快的硬件写入性能对 CPU 的性能更敏感，而搜索性能在一般情况下更多的是在于I/O能力，使用SSD会比旋转类存储介质好得多。尽量避免使用 NFS 等远程文件系统，如果 NFS 比本地存储慢 3 倍，则在搜索场景下响应速度可能会慢 10 倍左右。这可能是因为搜索请求有更多的随机访问。 文档模型为了让搜索时的成本更低，文档应该合理建模。特别是应该避免 join 操作，嵌套（nested）会使查询慢几倍，父子（parent-child）关系可能使查询慢数百倍，因此，如果可以通过非规范化（denormlizing） 文档来回答相同的问题，则可以显著地提高搜索速度。 预索引数据还可以针对某些查询的模式来优化数据的索引方式。例如：如果所有文档都有一个 price 字段，并且大多数查询在一个固定的范围上运行 range 聚合，那么可以通过将范围 pre-indexing 到索引中并使用 terms 聚合来加快聚合速度。 字段映射有些字段的内容是数值，但并不意味着其总是应该被映射为数值类型，例如，一些标识符，将它们映射为 keyword 可能会比 integer 或 long 更好。 避免使用脚本一般来说，应该避免使用脚本。如果一定要用，应该优先考虑 painless 和 expressions。 优化日期搜索在使用日期范围检索时 使用 now 的查询通常不能缓存，因为匹配到的范围一直在变化。但是，从用户体验的角度来看，切换到一个完整的日期通常是可以接受的，这样可以更好地利用查询缓存。例如，我们当前时间的查询替换成精确到分钟的查询，这样在一分钟之内的用户查询就都会查询缓存可以加快查询速度，替换的时间间隔越长，查询缓存越有帮助。 为只读索引执行 force-merge为不再更新的只读索引执行 force-merge，将 Lucene 索引合并为单个分段，可以提升查询速度。当一个 Lucene 索引存在多个分段时，每个分段会单独执行搜索再将结果合井，将只读索引强制合并为一个 Lucene 分段不仅可以优化搜索过程，对索引恢复速度也有好处。 预热文件系统cache如果ES主机重启，则文件系统缓存将为空，此时搜索会比较慢。可以使用 index.store.preload 设置，通过指定文件扩展名，显式地告诉操作系统应该将哪些文件加载到内存中，例如，配置到 elasticsearch.yml 文件中: 1index.store.preload: [&quot;nvd&quot;, &quot;dvd&quot;] 或者在索引创建时设置： 123456PUT /my_index&#123; &quot;settings&quot;: &#123; index.store.preload: [&quot;nvd&quot;, &quot;dvd&quot;] &#125;&#125; 如果文件系统缓存不够大，则无法保存所有数据，那么为太多文件预加载数据到文件系统缓存，会使搜索速度变慢，应谨慎使用。 调节搜索请求中的 batched_reduce_size聚合操作在协调节点需要等所有的分片都取回结果后才执行，使用 batched_reduce_size 参数可以不等待全部分片返回结果，而是在指定数量的分片返回结果之后就可以先处理一部分（reduce）。这样可以避免协调节点在等待全部结果的过程中占用大量内存，避免极端情况下可能导致的 OOM 。该字段的默认值为 512M，从ES 5.4 开始支持。 限制搜索请求的分片数一个搜索请求涉及的分片数量越多，协调节点的 CPU 和内存压力就越大。默认情况下，ES会拒绝超过 1000 个分片的搜索请求。我们应该更好地组织数据，让搜索请求的分片数更少。如果想调节这个值，则可以通过 action.search.shard_count 配置项进行修改。 利用自适应副本选择（ARS）提升 ES 响应速度为了充分利用计算资源和负载均衡，协调节点将搜索请求轮询转发到分片的每个副本，轮询策略是负载均衡过程中最简单的策略，任何一个负载均衡器都具备这种基础的策略，缺点是不考虑后端实际系统压力和健康水平。 例如，一个分片的三个副本分布在三个节点上，其中 Node2 可能因为长时间 GC、磁盘I/O过高、网络带宽跑满等原因处于忙碌状态，如果搜索请求被转发到副本，则会看到相对于其他分片来说，副本2有更高的延迟： 123分片副本1: lOOm分片副本2：1350ms分片副本3: 150m 由于副本2的高延迟，使得整个搜索请求产生长尾效应。ES 希望这个过程足够智能，能够将请求路由到其他数据副本，直到该节点恢复到足以处理更多搜索请求的程度 ES 中，此过程称为自适应副本选择。 ES 的 ARS 实现基于这样一个公式：对每个搜索请求，将分片的每个副本进行排序，以确定哪个最可能是转发请求的“最佳”副本。与轮询方式向分片的每个副本发送请求不同， ES择“最佳”副本并将请求路由到那里。 ARS 公式计算参考因素有如下： 节点未完成搜索的请求数。 系统中数据节点的数量。 响应时间的 EWMA （从协调节点上可以看到），单位为毫秒。 搜索线程池队列中等待任务数量的 EWMA; 数据节点上的搜索服务时间的 EWMA 单位为毫秒。 通过这些信息我们大致可以评估出分片副本所在节点的压力和健康程度，这就可以让我们选出一个能够更快返回搜索请求的节点。 磁盘使用量优化存储内容元数据字段每个文档都有与其相关的元数据，比如 index, _type，_id。当创建映射类型时，可以定制其中一些元数据字段，下面列举一些优化可以用到的: _source:原始的 JSON 文档数据。 _all:索引所有其他字段值的一种通用字段，这个字段中包含了所有其它字段的值。允许在搜索的时候不指定特定的字段名，意味着“从全部字段中搜索”。_all字段是一个全文字段，有自己的分析器。从 ES 6. 开始该字段被禁用。之前的版本默认启用，但字段的 store 属性为 false ，因此它不能被查询后取回显示。 索引映射参数索引创建时可以设置很多映射参数，部分映射参数的详细说明如下： index: 控制字段值是否被索引。它可以设置为 true false 默认为 true 未被索引的字段不会被查询到，但是可以聚合。除非禁用 doc values。 doc values:默认情况下，大多数字段都被索引，这使得它们可以搜索。倒排索引根据term 找到文档列表，然后获取文档原始内容。但是排序和聚合，以及从脚本中访问某个字段值，需要不同的数据访问模式，它们不仅需要根据 term 找到文档，还要获取文档中字段的值，这些值需要单独存储。 doc values 就是用来存储这些字段值的。它种存储在磁盘上的列式存储，在文档索引时构建，这使得上述数据访问模式成为可能。它们以面向列的方式存储与 _source 相同的值，这使得排序和聚合效率更高。几乎所有字段类型都支 doc values，但被分析（analyzed）的字符串字段除外（即 text类型宇符串）。 doc values 默认启用。 store:默认情况下，字段值会被索引使它们能搜索，但它们不会被存储（stored）。意味着可以通过这个字段查询，但不能取回它的原始值。 doc values 和存储字段（”stored” :ture）都属于正排内容，两者的设计初衷不同。stored fields 被设计为优化存储，doc values 被设计为快速访问字段值。搜索可能会访问很多doc value 中的字段，所以必须能够快速访问，我们将 doc values 用于聚合、排序，以及脚本中。现在，ES 中的许多特性都会自动使用 doc values。 优化措施禁用对你来说不需要的特性 默认情况下，ES 为大多数的字段建立索引，并添加到 doc_values ，以便使之可以被搜索和聚合。但是有时候不需要通过某些字段过滤，例如，有一个名为 foo 的数值类型字段，需要运行直方图，但不需要在这个字段上过滤，那么可以不索引这个字段。在mappings 结构的创建中，对foo字段设置 “index”: false。 text 类型的字段会在索引中存储归一因子，以便对文档进行评分，如果只需要在文本宇段上进行匹配，而不关心生成的得分，则可以配置 ES 不将 norms 写入索引。在mappings 结构的创建中，对foo字段设置 “norms”: false。 text 类型的字段默认情况下也在索引中存储频率和位置。频率用于计算得分，位置用于执行短语（phrase）查询。如果不需要运行短语查询，则可以告诉 ES 不索引位置。在mappings 结构的创建中，对foo字段设置 “index_options”: “freqs”。text 类型的字段上 index_options 的默认值为 positions。index_options 参数用于控制添加到倒排索引中的信息。 禁用 doc values所有支持 doc value 字段都默认启用了 doc value，如果确定不需要对字段进行排序或聚合，或者从脚本访问字段值，则可以禁用 doc value 节省磁盘空间。在mappings 结构的创建中，对foo字段设置 “doc_values”: false。 不要使用默认的动态字符串映射默认的动态宇符串映射会把字符串类型的字段同时索引为 text 和 keyword。如果只需要中之一，则显然是一种浪费。通常，id 字段只需作为 keyword 类型进行索引，而 body 宇段只需作为 text 类型进行索引。要禁用默认的动态字符串映射，则可以显式地指定字段类型，或者在动态模板中指定将字符串映射为 text 和 keyword。 观察分片大小较大的分片可以更有效地存储数据。为了增加分片大小，可以在创建索引的时候设置较少的主分片数，或者使用 shrink API 来修改现有索引的主分片数量。但是较大的分片也有缺点，例如，较长的索引恢复时间。 禁用 _source_source 字段存储文档的原始内容,如果不需要访问它，则可以将其禁用。但是需要访_source 的 API 将无法使用，至少包括下列情况： update、update_by_query、reindex; 高亮搜索。 重建索引（包括更新 mapping 分词器，或者集群跨大版本升级可能会用到）。 调试聚合查询功能，需要对比原始数据。 数值类型长度够用就好为数值类型选择的字段类型也可能会对磁盘使用空间产生较大影响，整型可以选择 byte、short、integer 或 long，浮点型可以选择 scaled_float、float、double、half_float, 每个数据类型的字节长度是不同的，为业务选择够用的最小数据类型，可以节省磁盘空间。 综合应用集群层规划集群规模在部署一个新集群时，应该根据多方面的情况评估需要多大的集群规模来支撑业务。这些信息包括： 数据总量，每天的增量。 查询类型和搜索并发，QPS。 SLA 级别。 另一方面，需要控制最大集群规模和数据总，参考下列两个限制条件： 节点总数不应该太多，一般来说，最大集群规模最好控制在 100 节点左右。我们经测试过上千个节点集群，在这种规模下，节点间的连接数和通信量倍增，主节点理压力比较大。 单个分片不要超过 50G，最大集群分片总数控制在几十万的级别。太多分片同样增加了主节点的管理负担，而且集群重启恢复时间会很长。 建议为集群配置较好的硬件，而不是普通的 PC，搜索对 CPU、内存、磁盘的性能要求都很高，要达到比较低的延迟就需要较好的硬件资源。另外，如果使用不同配置的服务器混合部署，则搜索速度可能会取决于最慢的那个节点，产生长尾效应。 单节点还是多节点部署ES不建议为JVM配置超过 32GB 的内存，超过 32GB 时， Java内存指针压缩失效，浪费一些内存，降低了CPU性能，GC压力也较大。因此推荐设置为 31GB。-Xmx3lg -Xms3lg。 确保堆内存最 Xms 与最大值 Xmx 大小相同，防止程序在运行时动态改变堆内存大小，这是很耗系统资源的过程。 当物理主机内存在 64GB 以上，并且拥有多个数据盘，不做 raid 的情况下，部署ES节点时有多种选择： 部署单个节点，JVM内存配置不超 32GB，配置全部数据盘。这种部署模式的缺点是多余的物理内存只能被 cache 使用，而且只要存在一个坏盘，节点重启会无法启动。 部署单个节点，JVM内存配置超过 32GB ，配置全部数据盘。接受指针压缩失效和更长时间的 GC 等负面影响 有多少个数据盘就部署多少个节点，每个节点配置单个数据路径。优点是可以统一配置，缺点是节点数较多，集群管理负担大，只适用于集群规模较小的场景。 使用内存大小除以64GB来确定要部署的节点数，每个节点配置一部分数据盘，优点是利用率最高，缺点是部署复杂。 移除节点当由于坏盘、维护等故障需要下线一个节点时，我们需要先将该节点数据迁移，这可通过分配过滤器实现。 1&quot;transient&quot;：&quot;cluster.routing.allocation.exclude.name&quot;: &quot;node-1&quot; 执行命令后，分片开始迁移，我们可以通过 cat/shard API 来查看该节点的分片是否迁移完生扎当节点维护完毕，重新上线之后，需要取消排除设置，以便后续的分片可以分配到 node-1 节点上。 1&quot;transient&quot;：&quot;cluster.routing.allocation.exclude.name&quot;: &quot;&quot; 独立部署主节点将主节点和数据节点分离部署最大的好处是 Master 切换过程可以迅速完成，有机会跳过gateway 和分片重新分配的过程。例如：具备 Master 资格的节点独立部署，然后关闭当前活跃的主节点，新主当选后由于内存中持有最新的集群状态，因此可以跳过 gateway 的恢过程，井且由于主节点没有存储数据，所以旧的 Master 离线不会产生未分配状态的分片。新主当边后集群状态可以迅速变为 Green。 节点层控制线程池的队列大小不要为 bulk search 分配过大的队列，队列并非越大越好，队列缓存的数据越多，GC力越大。默认的队列大小基本够用了，即使在压力测试的场景中，默认队列大小也足以支持。 为系统 cache 保留一半物理内存搜索操作很依赖对系统 cache 命中，标准建议是把 50% 的可用内存作为 ES 的堆内存， 为 Lucene 保留剩下的 50%，用作系统 cache。 系统层关闭 swap在服务器系统上，无论物理内存多么小，哪怕只有 1GB，都应该关闭交换分区。当服务程序在交换分区缓慢运行时，往往会产生更多不可预期的错误，因此当申请内存的操作如果真到物理内存不足时，宁可让它直接失败。 配置 Linux OOM Killer现在讨论 OOM 并非 JVM OOM，而是 Linux 操作系统的 OOM，Linnx 进程申请的内存并不会立刻为进程分配真实大小的内存，因为进程申请的内存不一定全部使用，内核在利用这些空内存时采取过度分配的策略，接入物理内存为 1GB 两个进程都可以申请1GB的内存，超过了系统实际内存大小。当应用程序实际消耗完内存的时候，怎么办？系统需要杀掉进程来保障系统正常运，这就触发了OOM Killer，通过一些策略给每个进程打分，根据分值高低决定“杀掉”哪些进程。默认情况下，占用内存最多的进程被杀掉。 如果ES 与其它服务混合部署，当系统产生 OOM，ES有可能会无辜被杀掉，为了避免这种情况，我们在用户态调节一些进程参数来让某些进程不容易被 OOM Kill掉，例如，我不希望 ES 进程被杀，可以设置进程的 oom_score_adj 参数为 -17（越小越不容易被杀）。 禁用透明大页透明大页是 Linux 的一个内核特性，它通过更有效地使用处理器的内存映射硬件来提高性能，默认情况下是启用的。禁用透明大页能略微提升程序性能，但是也可能对程序产生负面影响，甚至是严重的内存泄漏。为了避免这些问题，我们应该禁用它（许多项目都建议禁用透明大页，例如， MongoDB Oracle）。 索引层使用全局模板ES 5.x 开始，索引级别的配置需要写到模板中，而不是 elasticsearch.yml 配置文件，但是我们需要一些索引级别的全局设置信息，例如，translog 的刷盘方式等，因此我们可以将这些设置编写到一个模板中，并让这个模板匹配全部索引，这个模板我们称为全局模板。 索引轮转如果有一个索引每天都有新增内容，那么不要让这个索引持续增大，建议使用日期等规则按一定频率生成索引。同时将索引设置写入模板，让模板匹配这一系列的索引，还可以为索引生成一个别名关联部分索引。我们一般按天生成索引。 避免热索引分片不均默认情况 ES 的分片均衡策略是尽量保持各个节点分片数量大致相同 但是当集群扩容新加入集群的节点没有分片，此时新创建的索引分片会集中在新节点上，这导致新节点拥有太多热点数据，该节点可能面临巨大的写入压力。因此，对于每个索引，我需要控制个节点上存储的该索引的分片总数，使索引分片在节点上分布得更均匀些。 副本数选择由于搜索使用较好的硬件配置，硬件故障的概率相对较低 在大部分场景下，将副本数number_of_replicas 设置为1即可。这样每个分片存在两个副本。如果对搜索请求的吞吐量要求较高，则可以适当增加副本数量，让搜索操作可以利用更多的节点。如果在项目初始阶段不知道多少副本数够用，则可以先设置为1，后期再动态调整，对副本数的调整只会涉及数据复制和网络传输，不会重建索引，因此代价较小。 Force Merge对冷索引执行 Force Merge 会有许多好处，我们在之前的章节中曾多次提到： 单一的分段比众多分段占用的磁盘空间更小一些。 可以大幅减少进程需要打开的文件 fd。 可以加快搜索过程，因为搜索需要检索全部分段。 单个分段加载到内存时也比多个分段更节省内存占用。 可以加快索引恢复速度。可以选择在系统的 闲时间段对不再更新的只读索引执行 Force Merge。 Shrink Index需要密切注意集群分片总数，分片数越多集群压力越大。在创建索引时，为索引分配了较的分片，但可能实际数据并没有多大，例如，按日期轮询生成索引，可能有些日子里数据量并不大，对这种索引可以执行 Shrink 操作来降低分片数量。 Shrink 的例子可以参考Shrink 一章。 close 索引如果有些索引暂时不使用，则不会再有新增数据，也不会有对它的查询操作，但是可能以后会用而不能删除，那么可以把这些索引关闭，在需要时再打开。关闭的索引除存储空间外不占用其他资源。 延迟分配分片当一个节点由于某些原因离开集群时，默认情况下 ES 会重新确定主分片，并立即重新分配缺失的副分片，但是，一般来说节点离线是常态，可能因为网络问题、主机断电、进程退出等因素是我们经常面对节点离线的情况，而重新分配副分片的操作代价是很大的，该节点上存储的数据需要在集群上重新分配，复制这些数据需要大量的带宽和时间，因此我们调整节点离线后分片重新分配的延迟时间：”index.unassigned.node_left.delayed_timeout”: “5d”。 小心地使用 fielddata聚合时，ES 通过 doc values 获取宇段值，但 text 类型不支持 doc vales。当在 text 类型字段上聚合时，就会依赖 fielddata 数据结构，但 fielddata 默认关闭，因为它消耗很多堆空间，并且在 text 类型字段上聚合通常没有什么意义。 doc values 在索引文档时就会创建，而 field data 是在聚合、排序，或者脚本中根据需要动态创建的。其读取每个分段中的整个倒排索引，反转 term 和 doc 的关系，将结果存储到 JVM堆空间，这是非常昂贵的过程，会让用户感到明显的延迟。 读写避免搜索操作返回巨大的结果集我们在搜索流程中讨论过，由于协调节点的合并压力，所有的搜索系统都会限制返回的结果集大小，如果确实需要很大的结果集，则应该使用 Scroll API。 避免索引巨大的文档http.max_context_length 的默认值为 1OOMB, ES 会拒绝索引超过此大小的文档，可以增加这个值，但 Lucene 然有大约 2G 的限制。 即使不考虑这些限制，大型文档通常也不实用。大型文档给网络、内存和磁盘造成了更大压力。即使搜索操作设置为不返回_source, ES 总要获取 id ，对于大型文档来说，获取这个字段的代价是很大的，这是由于操作系统的 ache 机制决定的。索引一个文档需要一些内存，所需内存大小是原始文档大小的几倍 。邻近（ Proximity ）搜索（例如，短语查询）和高亮也会变得更加昂贵，因为它们的成本直接取决于原始文档大小。 因此可能要重新考虑信息的单位。例如，想要为一本书建立索引使之可以被搜索，这并不意味着把整本书的内容作为单个文档进行索引。最好使用章节或段落作为文档，然后在文档中加一个属性标识它们属于哪本书。这样不仅避免了大文档的问题，还使搜索的体验更好。 避免将请求发送到同一个协调节点无论索引文档还是执行搜索请求，客户端都应该避免将请求发送到固定的某个或少数几个节点，因为少数几个协调节点作为整个集群对外的读写节点的情况下，它们很有可能承受不了那么多的客户端请求，尤其是搜索请求，协调节点的合并及排序会占用比较高的内存和 CPU，聚合会占用更多内存。因此会导致给客户端的返回慢，甚至导致节点 OOM。 正确的做法是将请求轮询发送到集群所有节点，如果使用 REST API，则可以在构建客户端的客户端对象时传入全部节点列表。如果在前端或脚本中访问 ES 集群，则可以部 LVS户端使用虚 IP 或者部署 Ngin 使用反向代理。 客户踹使用 REST API 而非 Java API由于 Java API 引起版本兼容性问题，以及微弱到可以忽略的性能提升，JavaAPI 将在未来的版本中废弃，客户端最好选择阻 REST API 作为客户端，而不是 Java API。 为读写请求设置比较长的超时时间读写操作都有可能是比较长的操作，例如，写一个比较大的 bulk 数据，或者执行较大范围的聚合。此时客户端为请求设置的超时时间应该尽量长，因为即使客户端断开连接，ES 仍然会在后台将请求处理完，如果超时设置比较短，则在密集的请求时会对ES造成非常大的压力。 控制相关度通过 Painless 脚本控制搜索评分。 ES 有多种方式控制对搜索结果的评分，如果常规方式无法得到想要的评分结果，则可以通过脚本的方式完全自己实现评分算法，以得到预期的评分结果。ES 支持多种脚本语言，经历各版本演变后，从5.0 版本开始实现了自己专用的语Painless。Groovy脚本己弃用。 Painless是内置支持的，脚本内容通过阻REST接口传递给 ES, ES将其保存在集群状态中。在 5.x 版本中可以放到 config/scripts 下，6.x 版本中只能通 REST 接口。 通过脚本控制评分的原理是编写一个自定义脚本，该脚本返回评分值，该分值与原分值进行加法等运算，从而完全控制了评分算法。 使用脚本我们需要注意：如果一个match查询查出来了成千上万个文档，在此阶段使用脚本将会对所有的文档进行计算，这会导致极其糟糕的性能问题。所以我们可以使用二次评分机制，在二次评分中使用脚本进行打分，二次评分使用了一个简单的技巧，对返回文档中的topN进行二次评分，既只改变部分返回文档的排序结果。 Lucene 原理Lucene 倒排索引结构 Term：单词。 Posting List：倒排列表。倒排列表记录了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。Posting list就是一个int的数组，存储了所有符合某个term的文档id。实际上，除此之外还包含：文档的数量、词条在每个文档中出现的次数、出现的位置、每个文档的长度、所有文档的平均长度等，在计算相关度时使用。 Term Dictionary：数据（单词）字典，数据字典记录单词term。假设我们有很多个 term，比如：Carla,Sara,Elin,Ada,Patty,Kate,Selena。如果按照这样的顺序排列，找出某个特定的 term 一定很慢，因为 term 没有排序，需要全部过滤一遍才能找出特定的 term。排序之后就变成了：Ada,Carla,Elin,Kate,Patty,Sara,Selena。这样我们可以用二分查找的方式，比全遍历更快地找出目标的 term。这个就是 term dictionary。有了 term dictionary 之后，可以用 logN 次磁盘查找得到目标。 Term Index：数据（单词）索引。磁盘的随机读操作仍然是非常昂贵的（一次 random access 大概需要 10ms 的时间）。所以尽量少的读磁盘，有必要把一些数据缓存到内存里。但是整个 term dictionary 本身又太大了，无法完整地放到内存里。于是就有了 term index。term index 有点像一本字典的大的章节表。比如： 12A 开头的 term ……………. Xxx 页C 开头的 term ……………. Yyy 页 如果所有的 term 都是英文字符的话，可能这个 term index 就真的是 26 个英文字符表构成的了。但是实际的情况是，term 未必都是英文字符，term 可以是任意的 byte 数组。而且 26 个英文字符也未必是每一个字符都有均等的 term，比如 x 字符开头的 term 可能一个都没有，而 s 开头的 term 又特别多。实际的 term index 是一棵 trie 树（Trie Trees数据结构，简称字典树/单词查找树/键树）： 这棵树不会包含所有的 term，它包含的是 term 的一些前缀。通过 term index 可以快速地定位到 term dictionary 的某个 offset，然后从这个位置再往后顺序查找。现在我们可以回答“为什么 Elasticsearch/Lucene 检索可以比 mysql 快了。Mysql 只有 term dictionary 这一层，是以 b-tree 排序的方式存储在磁盘上的。检索一个 term 需要若干次的 random access 的磁盘操作。而 Lucene 在 term dictionary 的基础上添加了 term index 来加速检索，term index 以树的形式缓存在内存中。从 term index 查到对应的 term dictionary 的 block 位置之后，再去磁盘上找 term，大大减少了磁盘的 random access 次数。 可以形象地理解为，Term Dictionary 就是新华字典的正文部分包含了所有的词汇，Term Index 就是新华字典前面的索引页，用于表明词汇在哪一页。 但是 term index 即不能知道某个Term在Dictionary(.tim)文件上具体的位置，也不能仅通过FST就能确切的知道Term是否真实存在。它只能告诉你，查询的Term可能在这些Blocks上，到底存不存在FST并不能给出确切的答案，因为FST是通过Dictionary的每个Block的前缀构成，所以通过FST只可以直接找到这个Block在.tim文件上具体的File Pointer，并无法直接找到Terms。 Lucene 索引实现Lucene经多年演进优化，现在的一个索引文件结构如图所示，基本可以分为三个部分：词典、倒排表、正向文件、列式存储DocValues。 FSTLucene 的tip文件即为 Term Index 结构，tim文件即为 Term Dictionary 结构。tip中存储的就是多个FST，FST中存储的是&lt;单词前缀，以该前缀开头的所有Term的压缩块在磁盘中的位置&gt;。即为前文提到的从 term index 查到对应的 term dictionary 的 block 位置之后，再去磁盘上找 term，大大减少了磁盘的 random access 次数。它的特点就是： 词查找复杂度为O(len(str))。 共享前缀、节省空间。 内存存放前缀索引、磁盘存放后缀词块。 我们往索引库里插入四个单词abd、abe、acf、acg,看看它的索引文件内容。 tip部分，每列一个FST索引，所以会有多个FST，每个FST存放前缀和后缀块指针，这里前缀就为a、ab、ac。tim里面存放后缀块和词的其他信息如倒排表指针、TFDF等，doc文件里就为每个单词的倒排表。所以它的检索过程分为三个步骤： 内存加载tip文件，通过FST匹配前缀找到后缀词块位置。 根据词块位置，读取磁盘中tim文件中后缀块并找到后缀和相应的倒排表位置信息。 根据倒排表位置去doc文件中加载倒排表。 这里就会有两个问题，第一就是前缀如何计算，第二就是后缀如何写磁盘并通过FST定位，下面将描述下Lucene构建FST过程: 已知FST要求输入有序，所以Lucene会将解析出来的文档单词预先排序，然后构建FST，我们假设输入为abd,abd,acf,acg，那么整个构建过程如下： 插入abd时，没有输出。 插入abe时，计算出前缀ab，但此时不知道后续还不会有其他以ab为前缀的词，所以此时无输出。 插入acf时，因为是有序的，知道不会再有ab前缀的词了，这时就可以写tip和tim了，tim中写入后缀词块d、e和它们的倒排表位置ip_d,ip_e，tip中写入a，b和以ab为前缀的后缀词块位置(真实情况下会写入更多信息如词频等)。 插入acg时，计算出和acf共享前缀ac，这时输入已经结束，所有数据写入磁盘。tim中写入后缀词块f、g和相对应的倒排表位置，tip中写入c和以ac为前缀的后缀词块位置。 以上是一个简化过程，Lucene的FST实现的主要优化策略有： 最小后缀数。Lucene对写入tip的前缀有个最小后缀数要求，默认25，这时为了进一步减少内存使用。如果按照25的后缀数，那么就不存在ab、ac前缀，将只有一个跟节点，abd、abe、acf、acg将都作为后缀存在tim文件中。我们的10g的一个索引库，索引内存消耗只占20M左右。 前缀计算基于byte，而不是char，这样可以减少后缀数，防止后缀数太多，影响性能。如对宇(e9 b8 a2)、守(e9 b8 a3)、安(e9 b8 a4)这三个汉字，FST构建出来，不是只有根节点，三个汉字为后缀，而是从unicode码出发，以e9、b8为前缀，a2、a3、a4为后缀，如下图： 倒排表（PostingList）结构倒排表就是文档号集合，但怎么存，怎么取也有很多讲究，Lucene现使用的倒排表结构叫Frame of reference,它主要有两个特点： 数据压缩。 跳跃表加速合并，因为布尔查询时，and 和or 操作都需要合并倒排表，这时就需要快速定位相同文档号，所以利用跳跃表来进行相同文档号查找。 PostingList 在内存中是以 Skiplist 「跳跃列表」的形式存在的。Lucene 中的 Skiplist 和 Redis 中的 Skiplist 是一样的。只不过 Redis 的 Skiplist 全部在内存中，而 Lucene 的 PostingList 可能只是部分在内存中。Lucene 的策略是只将 Skiplist 中的高层节点放在内存中，当需要访问底层节点时需要额外的一次 IO 读取操作。这样可以显著降低内存压力，因为有些词汇关联的 PostingList 可能特别长，消耗内存会特别多，这属于时间换空间的折中优化。 Lucene 为什么要将 PostingList 设计成跳跃列表呢，这是为了做加速文档的交集运算。当查询的条件是两个 MUST 时，需要对两个词汇的 PostingList 进行交集计算。计算交集时会选择短的列表作为「驱动列表」，驱动列表的指针在往前走时，另外一个列表也要跟着往前跳。就好比一个大人和一个小孩走路，大人走得快，小孩就得跟着跑才能追赶上。同时因为跳跃列表的高层都在内存中，所以跳起来会非常的快，这样的交集运算就会有比较好的性能。 综上所述，倒排索引的 Key 和 Value 都是部分放在内存中，从这点来说 FST 和 Skiplist 的结构具有一定的相似性，它们都是有高度的数据结构，高层的数据留在内存中，底层的数据淘汰到磁盘上，查找方向是先定位高层再定位底层。 正向文件正向文件指的就是原始文档，Lucene对原始文档也提供了存储功能，它存储特点就是分块+压缩，fdt文件就是存放原始文档的文件，它占了索引库90%的磁盘空间，fdx文件为索引文件，通过文档号（自增数字）快速得到文档位置，它们的文件结构如下： fnm中为元信息存放了各列类型、列名、存储方式等信息。 fdt为文档值，里面一个chunk就是一个块，Lucene索引文档时，先缓存文档，缓存大于16KB时，就会把文档压缩存储。一个chunk包含了该chunk起始文档、多少个文档、压缩后的文档内容。 fdx为文档号索引，倒排表存放的时文档号，通过fdx才能快速定位到文档位置即chunk位置，它的索引结构比较简单，就是跳跃表结构，首先它会把1024个chunk归为一个block,每个block记载了起始文档值，block就相当于一级跳表。 所以查找文档，就分为三步： 第一步二分查找block，定位属于哪个block。 第二步就是根据从block里根据每个chunk的起始文档号，找到属于哪个chunk和chunk位置。 第三步就是去加载fdt的chunk，找到文档。这里还有一个细节就是存放chunk起始文档值和chunk位置不是简单的数组，而是采用了平均值压缩法。所以第N个chunk的起始文档值由 DocBase + AvgChunkDocs * n + DocBaseDeltas[n]恢复而来，而第N个chunk再fdt中的位置由 StartPointerBase + AvgChunkSize * n + StartPointerDeltas[n]恢复而来。 从上面分析可以看出，lucene对原始文件的存放是行是存储，并且为了提高空间利用率，是多文档一起压缩，因此取文档时需要读入和解压额外文档，因此取文档过程非常依赖随机IO，以及lucene虽然提供了取特定列，但从存储结构可以看出，并不会减少取文档时间。 列式存储DocValues我们知道倒排索引能够解决从词到文档的快速映射，但当我们需要对检索结果进行分类、排序、数学计算等聚合操作时需要文档号到值的快速映射，而原先不管是倒排索引还是行式存储的文档都无法满足要求。原先4.0版本之前，Lucene实现这种需求是通过FieldCache，它的原理是通过按列逆转倒排表将（field value -&gt;doc）映射变成（doc -&gt; field value）映射，但这种实现方法有着两大显著问题： 构建时间长。 内存占用大，易OutOfMemory，且影响垃圾回收。 因此4.0版本后Lucene推出了DocValues来解决这一问题，它和FieldCache一样，都为列式存储，但它有如下优点： 预先构建，写入文件。 基于映射文件来做，脱离JVM堆内存，系统调度缺页。 DocValues这种实现方法只比内存FieldCache慢大概10~25%，但稳定性却得到了极大提升。Lucene目前有五种类型的DocValues：NUMERIC、BINARY、SORTED、SORTED_SET、SORTED_NUMERIC，针对每种类型Lucene都有特定的压缩方法。 示例：我们看下ElasticSearch如何基于倒排索引和DocValues实现下面一条SQL的： 1select gender,count(*),avg(age) from employee where dept='sales' group by gender 从倒排索引中找出销售部门的倒排表。 根据倒排表去性别的DocValues里取出每个人对应的性别，并分组到Female和Male里。 根据分组情况和年龄DocValues，计算各分组人数和平均年龄 上面就是ElasticSearch进行聚合的整体流程，也可以看出ElasticSearch做聚合的一个瓶颈就是最后一步的聚合只能单机聚合，也因此一些统计会有误差，比如count(*) group by producet limit 5,最终总数不是精确的。因为单点内存聚合，所以每个分区不可能返回所有分组统计信息，只能返回部分，汇总时就会导致最终结果不正确。 ES如何联合索引查询给定查询过滤条件 age=24 的过程就是先从 term index 找到 18 在 term dictionary 的大概位置，然后再从 term dictionary 里精确地找到 18 这个 term，然后得到一个 posting list 或者一个指向 posting list 位置的指针。然后再查询 sex=Female 的过程也是类似的。最后得出 age= 24 AND sex=Female 就是把两个 posting list 做一个“与”的合并。 这个理论上的“与”合并的操作可不容易。对于 mysql 来说，如果你给 age 和 gender 两个字段都建立了索引，查询的时候只会选择其中最 selective 的来用，然后另外一个条件是在遍历行的过程中在内存中计算之后过滤掉。那么要如何才能联合使用两个索引呢？有两种办法： 使用 skip list 数据结构。同时遍历 gender 和 age 的 posting list，互相 skip； 使用 bitset 数据结构，对 gender 和 age 两个 filter 分别求出 bitset，对两个 bitset 做 AN 操作。 Elasticsearch 支持以上两种的联合索引方式，如果查询的 filter 缓存到了内存中（以 bitset 的形式），那么合并就是两个 bitset 的 AND。如果查询的 filter 没有缓存，那么就用 skip list 的方式去遍历两个 on disk 的 posting list。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/tags/Elasticsearch/"},{"name":"核心技术四","slug":"核心技术四","permalink":"https://jjw-story.github.io/tags/核心技术四/"}],"author":"JJW"},{"title":"Elasticsearch-核心技术三","slug":"Elasticsearch-核心技术三","date":"2020-01-08T01:16:55.000Z","updated":"2021-04-11T06:42:00.046Z","comments":true,"path":"2020/01/08/Elasticsearch-核心技术三/","link":"","permalink":"https://jjw-story.github.io/2020/01/08/Elasticsearch-核心技术三/","excerpt":"","text":"索引恢复流程索引恢复是 ES 数据恢复过程。待恢复的数据是客户端写入成功，但未执行刷盘（flush）的 Lucene 分段。例如，当节点异常重启时，写入磁盘的数据先到文件系统的缓忡，未必来得及刷盘，如果不通过某种方式将未刷盘的数据找回来，则会丢失－些数据，这是保持数据完整性的体现。另一方面，由于写入操作在多个分片副本上没有来得及全部执行，副分片需要同步成和主分片完全一致，这是数据副本一致性的体现。 根据数据分片性质，索引恢复过程可分为主分片恢复流程和副分片恢复流程： 主分片从 translog 中自我恢复，尚未执行 flush 到磁盘的 Lucene 分段可以从 translog 重建。 副分片需要从主分片中拉取 Lucene 分段和 translog 进行恢复但是有机会跳过拉取 Lucene 分段的过程。 主分片恢复流程INIT 阶段一个分片的恢复流程中，从开始执行恢复的那一刻起，被标记为 INIT 阶段，恢复流程在新的线程池中开始执行，开始阶段主要是一些验证工作，例如，校验当前分片是否为主分片，分片状态是否异常等。 INDEX 阶段本阶段从 Lucene 读取最后一次提交的分段信息，获取其中的版本号，更新当前索引版本。 VERIFY_INDEX 阶段VERIFY INDEX 中的 INDEX 指 Lucene index，因此本阶段的作用是验证当前分片是否损坏，是否进行本项检查取决于配置项：index.shard.check_on_startup。在索引的数据量较大时，分片检查会消耗更多的时间，默认配置为不执行验证索引，进入最重要的 TRANSLOG 阶段。 TRANSLOG 阶段一个 Lucene 索引由许多分段组成，每次搜索时遍历所有分段。内部维护了一个称为“提交点”的信息，其描述了当前 Lucene 索引都包括哪些分段，这些分段已经被 fsync 系统调用，从操作系统的 cache 刷入磁盘。每次提交操作都会将分段刷入磁盘实现持久化。 本阶段需要重放事务日志中尚未刷入磁盘的信息，因此，根据最后一次提交的信息做快照，来确定事务日志中哪些数据需要重放。重放完毕后将新生成的 Lucene 数据刷入磁盘。 FINALIZE 阶段本阶段执行刷新（ refresh ）操作，将缓冲的数据写入文件，但不刷盘，数据在操作系统的cache中。 DONE 阶段DONE 阶段是恢复工作的最后一个阶段， 进入 DONE 段之前再次执行 refresh 然后更新分片状态。至此，主分片恢复完毕，对恢复结果进行处理。 副分片恢复流程流程概述副分片恢复的核心思想是从主分片拉取 Lucene 分段和 translog 进行恢复，按数据传递的方向，分片节点称为 Source，副分片节点称为 Target。 为什么需要拉取主分片 translog？因为在副分片恢复期间允许新的写操作，从Lucene分段的那一刻开始，所恢复的副分片数据不包括新增的内容，而这些内容存在于主分片的translog中，因此副分片需要从主分片节点拉取 translog 进行重放，以获取新增内容，这就需要主分片节 translog 不被清理。为了防止主分片节点 translog 清理，这方面的实现机制历了多次选代。这里只讲述6.0版本后的处理。 从6.0 版本开始，引入 TranslogDeletionPolicy 的概念，负责维护活跃的 translog 文件。这个类的实现非常简单，它将 translog 做一个快照来保持 translog 不被清理，这样使用者只需创建一个快照，无须担心视图之类。恢复流程实际上确实需要视图，现在可以通过获取一个简单的保留锁来防止清理translog，这消除了视图概念的需求。 在保证 translog 不被清理后，恢复核心处理过程由两个内部阶段（phase）组成： phase1：在主分片所在节点，获取translog保留锁，从获取保留锁开始，会保留translog不受其刷盘清空的影响。然后调用Lucene接口把 shard 做快照，快照含有 shard 中己刷到磁盘的文件引用，把这些 shard 数据复制到副本节点。在 phase1 结束前,会向副分片节点发送告知对方启动 Engine，在 phase2 开始之前，副分片就可以正常处理写请求了。 phase2：对 translog 做快照，这个快照里包含从 phase1 开始，到执行 translog 快照期间的新增索引,将这些 translog 发送到副分片所在节点进行重放。 由于 phase1 需要通过网络复制大量数据，过程非常漫长，在 ES 6.x 中，有两个机会可以跳 phase1: 如果可以基于恢复请求中的 SequenceNumber 进行恢复，则跳过 phase1。 如果主副两分片有相同的 syncid 且 doc 数相同，则跳过 phase1。 synced flush 机制为了解决副分片恢复过程第一阶段时间太漫长而引入了 synced flush，默认情况下5分钟没有写入操作，索引被标记为 inactive，执行 synced flush 生成一个唯一 syncid，写入分片的所有副本中。这个 syncid 是分片级，意味着拥有相同 syncid 的分片具有相同的 Lucene 索引。synced flush 本质上是一次普通的 flush 操作，只是在 Lucene commit 过程中多写了一个syncid。原则上，在没有数据写入的情况下，各分片在同一时间flush成功后，它们理应有相同的 Lucene 索引内容，无论 Lucene 分段是否一致，于是给分片分配一个 id 表示数据一致。 但是显然 synced flush 期间不能有新写入的内容，如果 sync flush 执行期间收到写请求，则ES 选择了写入可用性：让 synced flush 失败，让写操作成功。在没有执行 flush 的情况下己有 syncid不会失效。在某个分片上执行普通 flush 操作会删除己有syncid 因此，synced flush 操作是一个不可靠操作，只适用于冷索引。 副分片节点处理过程副分片恢复的 VERIFY_INDEX、TRANSLOG、FINALIZE 三个阶段由主分片节点发送的RPC 调用触发。核心流程如下图所示： INIT 阶段本阶段在副分片节点执行。与主分片恢复的 INIT 阶段类似，恢复任务开始时被设置为 INIT，构建准备发往主分片 StartRecoveryRequest 请求，请求中包括将本次要恢复的shard信息，如shardid、metadataSnapshot等。metadataSnapshot包含 syncid。 INDEX 阶段INDEX 阶段负责将主分片的Lucene数据复制到副分片节点，向主分片节点发送 action 为 intemal:index/shard/recovery/start_recovery的 RPC 请求，并阻当前线程，等待响应，直到对方处理完成，然后设置为DONE阶段。（概括来说，主分片节点收到请求后把 Lucene translog 发送给副分片）。在这期间主分片节点会发送几次RPC调用，通知副分片节点启 Engine，执行清理等操作。VERIFY_INDEX 和 TRANSLOG 阶段也是由主分片节点的 RPC 调用触发的。 VERIFY_INDEX 阶段副分片的索引验证过程与主分片相同，是否进行验证取决于配置。默认为不执行索引验证。 TRANSLOG 阶段TRANSLOG 阶段负责将主分片的 translog 复制到副分片节点进行重放。先创建新的 Engine，跳过 Engine 自身的 translog 恢复。此时主分片 phase2 尚未开始，接下来的 TRANSLOG 阶段就是等待主分片节点将 translog 发到副分片节点进行重放，也就是phase2 的执行过程。 FINALIZE 阶段主分片节点执行完 phase2，调用 finalizeRecovery 向副分片节点发送 action为internal:index/shard/recovery/finalize的 RPC 请求，副分片节点对此action处理为先更新全局检查点，然后执行与主分片相同的清理操作。 DONE 阶段副分片节点等待 INDEX 阶段执行完成后进入 DONE 阶段，主要处理是调用 indexShard#postRecovery，与主分片的postRecovery处理过程相同，包括对恢复成功或失败的处理，也和主分片的处理过程相同。 主分片节点处理过程核心流程如下图所示： 主分片节点收到副分片节点发送的恢复请求，执行恢 ，然后返回结果，这里也是阻塞处理的过程。 首先获取取一个保留锁，使 translog 不被清理。 判断是否可以从 SequenceNumber 恢复。(除了异常检测和版本号检测，主要在 isTranslogReadyForSequenceNumberBasedRecovery中判断请求的序列号号是否小于主分片节的 local Checkpoint，以及 translog 中的数据是否足以恢复（有可能因为 translog 数据太大或过期删除而无法恢复)。以请求的序列号作为最小值做一个快照，遍历这个值从开始到最新的数据之间的操作，检查序列号验证事务日志中的操作是否完整。 如果可以基于 SequenceNumber 恢复，则跳过 phasel 否则调用 Lucene 接口对分片做快照，执行 phasel。 等待 phasel 执行完毕，主分片节点通知副分片节点启动此分片的 Engine。该方法会阻塞处理，直到分片 Engine 启动完毕。待副分片启动 Engine 完毕，就可以正常接收写请求了。注意，此时 phase2 未开始，此分片的恢复流程尚未结束。 等待当前操作处理完成后，以startingSeqN为起始点，对 translog 做快照，开始执 phase2。（如果基于 SequenceNumber 恢复，则 sartingSeqNo 取值为恢复请求中的序列号，从请求的序列号开始快照 translog。否则取值为0，快照完整的 translog） 最后执行清理工作，该方法向副分片节点发送action为internal:index/shard/recovery/finalize的 RPC 请求告知对方执行清理，同时把全局检查点发送过去，等待对方执行成功，主分片更新全局检查点。 phase1 详解： phase1 检查目标节点上的段文件，井对缺失的部分进行复制。只有具有相同大小和校验和的段才能被重用。但是由于分片副本执行各自的合并策略，所以合并出来的段文件相同的概率很低。在对比分段之前，先检查主副两分片是否都有 syncid，如果 syncid 相同，且 doc 数相同，跳过phase1，否则对比文件差异，发送文件。 phase2 详解： phase2 将 translog 批量发送到副分片节点，发送时将待发送 translog 组合成一批来提高发送效率，默认的批量大小为 512KB，不支持配置。 recovery 速度优化众所周知，索引恢复是集群启动过程中最缓慢的过程，集群完全重启，或者 Master 节点挂掉后，新选出的 Master 也有可能执行这个过程。下面归纳有哪些方法可以提升索引恢复速度： 配置项cluster.routing.allocation.node_concurrent_recoveries 决定了单个节点执行副分片recovery时的最大并发数（进／出），默认为1，适当提高此值可增加 recovery 井发数。 配置项 indices.recovery_max_bytes_per_sec 决定节点间复制数据时的限速，可以适当提此值或取消限速。 配置项 cluster.routing.allocation.node_initial_primaries_recoveries 决定了单个节点执行主分片 recovery 时的最大并发数，默认为4。由于主分片的恢复不涉及在网络上复制数据，仅在本地磁盘读写，所以在节点配置了多个数据磁盘的情况下，可以适当提高此值。 在重启集群之前，先停止写入端，执行 sync flush，让恢复过程有机会跳过 phase1。 适当地多保留些translog，配置项 index.translog.retention.size 默认最大保留 512MB，index.translog.retention.age 默认为不超过 12 小时。调整这两个配置可让恢复过程有机会跳过 phase1。 合并 Lucene 分段，对于冷索引甚至不再更新的索引执行_forcemerge，较少的 Lucene 分段可以提升恢复效率，例如，减少对比，降低文件传输请求数量。 gateway 模块 （元数据存储和恢复模块）gateway 模块负责集群元信息的存储和集群重启时的恢复。 元数据ES 中存储的数据有以下几种： state 元数据信息； index Lucene 生成的索引文件； translog 事务日志。 元数据信息又有以下几种： nodes/O/_state/*.st 集群层面元信息； nodes/O/indices/{index_uuid}/_state/*.st 索引层面元信息； nodes/O/indices/{index_uuid}/0/_state/*.st，分片层面元信息。 分别对应 ES 中的数据结构： MetaData（集群层），主要是 clusterUUID、settings、templates。 IndexMetaData（索引层），主要是 numberOfShards、mappings。 ShardStateMetaData（分片层），主要是 version、indexUUID、primary 等。 上述信息被持久化到磁盘，需要注意的是：持久化的 state 不包括某个分片存在于哪个节点这种内容路由信息，集群完全重启时，依靠 gateway 的 recovery 过程重建 RoutingTable 。当读取某个文档时，根据路由算法确定目的分片后，从RoutingTable中查找分片位于哪个节点，然后将请求转发到目的节点。 元数据的持久化只有具备 Master 资格的节点和数据节点可以持久化集群状态。当收到主节发布的集群状态时，节点判断元信息是否发生变化，如果发生变化将其持久化到磁盘中。 元数据的恢复上述的三种元数据信息被持久化存储到集群的每个节点，当集群完全重启（full restart）时，由于分布式系统的复杂性，各个节点保存的元数据信息可能不同。此时需要选择正确的元数据作为权威元数据。 gateway 的 recovery 负责找到正确的元数据，应用到集群。 当集群完全重启，达到 recovery 条件时，进入元数据恢复流程，一般情况下，recovery件由以下三个配置控制： gateway.expected_nodes：预期的节点数。加入集群的节点数（数据节点或具备 Master格的节点）达到这个数量后即开始 gateway 恢复。默认为0。 gateway.recover_after_time，如果没有达到预期的节点数量，则恢复过程将等待配置的时间，再尝试恢复。默认为 5min。 gateway.recover_after_nodes，只要配置数量的节点（数据节点或具备 Master资格的节点）加入集群就可以开始恢复。 当集群完全启动时，gateway 模块负责集群层和索引层的元数据恢复，分片层元数据恢复在allocation模块实现，但是由 gateway 模块在执行完上述两个层次恢复工作后触发。因此，三个层次的元数据恢复是 gateway 块和 allocation 模块共同完成的。 元数据恢复流程分析 Master 选举成功之后，判断其持有的集群状态中是否存在 STATE_NOT_RECOVEBLOCK，如果不存在，则说明元数据已经恢复，跳过 gateway 过程，否则等待。 Master 从各个节点主动获取元数据信息。 从获取的元数据信息中选择版本号最大的作为最新元数据，包括集群级、索引级。 两者确定之后，调用 allocation 模块的 reroute，对未分配的分片执行分配，主分片分配过程中会异步获取各个 shard 级别元数据，默认超时为 13s。 集群级和索引级元数据信息是根据存储在其中的版本号来选举的，而主分片位于哪个节点却是 allocation 模块动态计算出来的，先前主分片不一定还被选为新的主分片。 选举集群级和索引级别的元数据 判断是否满足进入 recovery 条件，当满足条件时，进入 recovery 主要流程。 首先向有 Master 资格的节点发起请求，获取它们存储的元数据。 等待回复 ，必须收到所有节点的回复，无论回复成功还是失败（节点通信失败异常会被捕获，作为失败处理），此处没有超时。 在收齐的这些恢复中，有效元信息的总数必须达到指定数量。异常情况下，例如，某个节点上元信息读取失败，则回复信息中元数据为空。 接下来就是通过版本号选取集群级和索引级元数据。 触发 allocation当上述两个层次的元信息选举完毕，调用 clusterService.submitStateUpdateTask 提交一个集群任务，该任务在 masterService#updateTask 线程池中执行。主要工作是构建集群状态（ClusterState），其中的内容路由表依赖 allocation 模块协助完成，调用 allocationService.reroute 进入下一阶段：异步执行分片层元数据的恢复，以及分片分配。updateTask 线程结束。 至此，gateway 恢复流程结束，集群级和索引级元数据选举完毕，如果存在未分配的主分片，分片级元数据选举和分片分配正在进行中。 allocation 模块 （分片分配模块）这里注意部署意识的概念 什么是 allocation分片分配就是把一个分片指派到集群中某个节点的过程，分配决策由主节点完成，分配决策包含两方面： 哪些分片应该分配给哪些节点； 哪个分片作为主分片，哪些作为副分片。 对于新建索引和己有索引，分片分配过程也不尽相同。不过不管哪种场景，ES 都通过两个基础组件完成工作： allocators 和 deciders。 allocators 尝试寻找最优的节点来分配分片，deciders则负责判断并决定是否要进行这次分配。 allocators 负责为某个特定的分片分配目的节点。每个 allocator 的主要工作是根据某种逻辑得到一个节点列表，然后调用 deciders 去决策，根据决策结果选择一个目的 node。 分片分配重点 对于新建索引，allocators 负责找出拥有分片数最少的节点，并按分片数量升序排序，因此分片较少的节点会被优先选择。所以对于新建索引，allocators 的目标就是以更均衡的方式把新索引的分片分配到集群的节点中，然后 deciders 依次遍历 allocators 给出的节点，并判断是否把分片分配到该节点。例如：如果分配过滤规则中禁止节点 A 持有索引 idx 中的任一分片，那么过滤器也阻止把索引 idx 分配到节点 A 中，即便A节点是 allocators 集群负载均衡角度选出的最优节点。需要注意的是，allocators 只关心每个节点上的分片数，而不管每个分片的具体大小。这恰好是 deciders 工作的一部分，既阻止把分片分配到超出节点磁盘容量阈值的节点上。 对于己有索引，则要区分主分片还是副分片。对于主分片， allocators 只允许把主分片指定在己经拥有该分片完整数据的节点上。而对于副分片，allocators 是先判断其他节点上是否己有该分片的数据的副本（即便数据不是最新的）。如果有这样的节点，则allocators 优先把分片分配到其中一节点。因为副分片一旦分配，就需要从主分片中进行数据同步，所以当一个节点只拥分片中的部分数据时，也就意味着那些未拥有的数据必须从主节点中复制得到。这样可以明显地提高副分片的数据恢复速度。 分片分配触发时机触发分片分配有以下几种情况： index 增删。 node 增删。 手工 reroute。 replica 数量改变。 集群重启。 流程分析gateway 阶段恢复集群状态中，我们己经知道集群一共有多少个索引，每个索引的主副分片各有多少个，但是不知道它们位于哪个节点，现在需要找到它们都位于哪个节点。集群完全重启的初始状态，所有分片都被标记为未分配状态，此处也被称作分片分配过程。因此分片分配的概念不仅仅是分配一个全新分片。 对于索引某个特定分片的分配过程中，先分配其主分片，后分配其副分片。 gatewayAllocatorgatewayAllocator 分为主分片和副分片分配器。 主分片分配器 遍历所有未分配的分片依次处理，通过 decider 决策分配，期间可能需要 fetchData 获取这个 shard 对应的元数据。如果决策结果为 YES，则将其初始化。 主副分片执行相同的函数，将分片的 unassigned 状态改为initialize 状态。（设置 RoutingNodes 己更新。更新的内容大约就是某个 shard 被分配到了某个节点，这个 shard 是主还是副，副的话会设置 recoverySource PEER,但只是一个类型，并没有告诉节点 recovery 的时候从哪个节点恢复，节点恢复时自己从集群状态中的路由表中查找。） Mast 把新的集群状态广播下去，当数据节点发现某个分片分配给自己，开始执行分片的 recovery。 主分片分配器返回指定的分片是否可以被分配，如果还没有这个分片的信息，则向集群的其他节点去请求该信息；如果己经有了，则根据 decider 进行决策。 首次进入函数时，还没有任何分片的元信息，发起向集群所有数据节点获取某个 shard 元信息的 fetchData 请求。之所以把请求发到所有节点，是因为它不知道哪个节点有这个 shard 的数据。集群启动的时候，遍历所有 shard ，再对每个 shard 向所有数据节点发 fetchData 请求，如果集群有 100 个节点、1000 个分片，则总计需要请求 100 * 1000 = 100000 。虽然是异步的，但仍然存在效率问。当 ES 集群规模比较大、分片数非常多的时候，这个请求的总量就会很大。 向各节点发起 fetchData 请求。（从所有数据节点异步获取某个特定分片的信息，没有超时设置） 数据节点的响应。（数据节点读取本地 shard 元数据返回请求方） 收集返回结果并处理。（进入主分片分配过程，依据这些元信息选择主分片） 主分片选举实现。（ES 之后的主分片选举与之前的版本机制是不一样的。 ES 5 之前的版本依据分片元数据的版本号对比实现，选择分片元信息中版本号高的分片来选举主分片， ES 5 及之后的版本依据allocation id 从 inSyncAllocationlds 中选择1个作为主分片。这种改变的主要原因是依据版本号无法保证版本号最高的分片一定被选为主分片。例如，当前只有1个活跃分片，那它被选为主分片，而拥有最新数据的分片尚未启动。） 副分片分配器副分片决策过程中也需 fetchData， 只不过主分片分配节点已经 fetch 过，可以直接从结果中获取，但是在“fetchData”之前先运行一遍 allocation.deciders().canAllocate 来判断是否至少可以在一个node上分配，如果分配不了就省略后面的逻辑了，例如，其主分片尚未就绪等。 本章注意点： 不需要等所有主分片都分配完才执行副分片的分配。每个分片有自己的分配流程。 不需要等所有分片都分配完才执行 recovery 流程。 主分片不需要等副分片分配成功才进入主分片的 re co very 副分片有自己的 recovery流程。 略过模块 Cluster 模块分析 Transport 模块分析 ThreadPool 模块分析 Shrink 原理分析Shrink API ES 5.0 之后提供的新功能，其可以缩小主分片数量。但其并不对源索引直接进行缩小操作，而是使用与源索引相同的配置创建一个新索引，仅降低分片数。由于添加新文档时使用对分片数量取余获取目的分片的关系，新索引的主分片数必须是源索引主分片数的因数。例如，8个分片可以缩小到 4、2、1 个分片。如果源索引的分片数为素数，则目标索引的分片数只能为1。 Shrink 的工作原理 以相同配置创建目标索引，但是降低主分片数量。 从源索引的 Lucene 分段创建硬链接到目的索引。如果系统不支持硬链接，那么索引的所有分段都将复制到新索引，将会花费大量时间。 对目标索引执行恢复操作，就像一个关闭的索引重新打开时一样。 创建新索引使用旧索引的配置创建新索引，只是减少主分片的数量，所有副本都迁移到同一个节点。显然，创建硬链接时，源文件和目标文件必须在同一台主机。 创建硬链接从源索引到目的索引创建硬链接。如果操作系统不支持硬链接，则复制 Lucene 分段。 为什么一定要硬链接，不使用软链接？ Linux 的文件系统由两部分组成（实际上任何文件系统的基本概念都相似）：inode 和 block。block 用于存储用户数据，inode 用于记录元数据，系统通过 inode 定位唯一的文件。 硬链接：文件有相同的 inode 和 block。 软链接：文件有独立的 inode 和 block, block 内容为目的文件路径名。 那么为什么一定要硬链接过去呢？从本质上来说，我们需要保证 Shrink 之后，源索引和目的索引是完全独立的，读写和删除都不应该互相影响，如果软链接过去，删除濒索引，则目的索引的数据就会被删除，硬链接则不会。 使用硬链接，删除源索引，只是将文件的硬链接数量减 1，删除源索引和目的索引中的任何一个，都不影响另一个正常读写。 由于使用了硬链接，也因为硬链接的特性带来一些限制：不能交叉文件系统或分区进行硬链接的创建，因为不同分区和文件系统有自己的 inode。 不过，既然都是链接，shrink 完成后，修改源索引，目的索引会变吗？答案是不会。虽然链接到了源分段，shrink 期间索引只读，目标索引能看到的只有源索引的当前数据，Shrink完成后，由于 Lucene 中分段的不变性，“write once”机制保证每个文件都不会被更新。源索引新写入的数据随着 refresh 会生成新分段，而新分段没有链接，在目标索引中是看不到的。如果源索引进行 merge，对源分段执行删除时，只是硬链接数量减1，目标索引仍然不受影响 因此，shrink 完毕后最终的效果就是，两个索引的数据看起来是完全独立的。 经过链接过程之后，主分片己经就绪，副分片还是空的，通过 recovery 将主分片数据复制到副分片。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/tags/Elasticsearch/"},{"name":"核心技术三","slug":"核心技术三","permalink":"https://jjw-story.github.io/tags/核心技术三/"}],"author":"JJW"},{"title":"Elasticsearch-核心技术二","slug":"Elasticsearch-核心技术二","date":"2020-01-01T02:52:33.000Z","updated":"2021-04-11T06:34:05.288Z","comments":true,"path":"2020/01/01/Elasticsearch-核心技术二/","link":"","permalink":"https://jjw-story.github.io/2020/01/01/Elasticsearch-核心技术二/","excerpt":"","text":"文档写入流程Index/Bulk 基本流程新建、索引（这里的索引是动词，指写入操作，将文档添加到 Lucene 的过程称为索引文档）和删除请求都是写操作。写操作必须先在主分片执行成功后才能复制到相关的副分片。下面是基本步骤： 客户端向 NODE1 发送写请求。 NODE1 使用文档 ID 来确定文档属于分片0，通过集群状态中的内容路由表信息获知分片0的主分片位于NODE3，因此请求被转发到NODE3上。 NODE3 上的主分片执行写操作，如果写入成功，则它将请求并行转发到 NODE1 和 NODE2 的副分片上，等待返回结果。当所有的副分片都报告成功，NODE3 将向协调节点报告成功，协调节点再向客户端报告成功。 在客户端收到成功响应 ，意味着写操作已经在主分片和所有副分片都执行完成。 写一致性的默认策略是 quorum，即多数的分片（其中分片副本可以是主分片或副分片）在写入操作时处于可用状态。 quorum = int( (primary + number_of_replicas) / 2 ) + 1 Index/Bulk 详细流程以不同角色节点执行的任务整理流程如下图所示: 协调节点流程协调节点负责创建索引、转发请求到主分片节点、等待响应、回复客户端。 参数检查：如同我们平常设计的任何一个对外服务的接口处理一样，收到用户请求后首先检测请求的合法性，把检查操作放在处理流程的第一步，有问题就直接拒绝，对异常请求的处理代价是最小的。 处理 pipeline 请求：数据预处理（ingest）工作通过定义 pipeline和processors 实现。pipeline 是一系列 processors的定义，processors 按照声明的顺序执行。（比如为添加的数据动态的添加一个定义好的固定属性等等类似工作）如果 Index Bulk 请求中指定了 pipeline 参数，先使用相应的 pipline 进行处理。如果本节点不具备预处理资格，则将请求随机转发到其他具备预处理资格的节点。 自动创建索引：如果配置为允许自动创建索引（默认允许〉，则计算请求中涉及的索引，可能有多个，其中有哪些索引是不存在的，然后创建它。如果部分索引创建失败，则涉及创建失败索引的请求被标记为失败。其他索引正常执行写流程。 对请求的预先处理：这里不同于对数据的预处理，对请求的预先处理只是检查参数、自动生成 id 、处理 routing 等。如果 id 不存在，则生成一个 UUID 作为文档 id。 检测集群状态：协调节点在开始处理时会先检测集群状态，若集群异常则取消写入。例如，Master 节点不存在，会阻塞等待 Master 点直至超时。 内容路由，构建基于 shard 的请求：将用户的 bulkRequest 重新组织 shard 的请求列表。例如，原始用户请求可能有10个写操作，如果这些文档的主分片都属于同1个，则写请求被合并为1个。所以这里本质上是合并请求的过程 此处尚未确定主分片节点。 路由算法：路由算法就是根据 routing 和文档 id 计算目标 shard id 的过程。一般情况下，路由计算方式的公式：shard_num = hash(_routing) % num_primary_shards。 转发请求井等待晌应：主要是根据集群状态中的内容路由表确定主分片所在节点，转发请求并等待响应。 主分片节点流程主分片所在节点负责在本地写主分片，写成功后，转发写副本片请求，等待响应，回复协调节点。 检查请求：主分片所在节点收到协调节点发来的请求后。是先做了校验工作，主要检测要写的是否是主分片，Allocationid 是否符合预期，索引是否处于关闭状态等。 是否延迟执行：判断请求是否需要延迟执行，如果需要延迟则放入队列，否则继续下面的流程。 判断主分片是否已经发生迁移：如果己经发生迁移，则转发请求到迁移的节点。 检测写一致性：在开始写之前，检测本次写操作涉及的 shard ，活跃 shard 数量是否足够，不足则不执行写入。默认为1，只要主分片可用就执行写入。 写 Lucene 和事务日志：遍历请求，处理动态更新宇段映射，然后逐条对 doc 进行索引。Engine 封装了 Lucene translog 的调用，对外提供读写接口。在写入 Lucene 之前，先生成 Sequence Number和Version。这些都是在 InternalEngine中实现的。 Sequence Number 每次递增 1, Version 根据 当前 doc 最大版本加 1。索引过程为先写 Lucene 后写 translog。因为 Lucene 写入时对数据有检查，写操作可能会失败。如果先写 translog ，写入 Lucene 失败， 则还需要对 translog 回滚处理。写Lucene这一步开始会对文档uid加锁，然后判断uid对应的version v2和之前update转换时的versoin v1是否一致，不一致则返回第二步重新执行。 如果version一致，如果同id的doc已经存在，则调用lucene的updateDocument接口，如果是新文档则调用lucene的addDoucument. 这里有个问题，如何保证Delete-Then-Add的原子性，ES是通过在Delete之前会加上已refresh锁，禁止被refresh，只有等待Add完成后释放了Refresh Lock, 这样就保证了这个操作的原子性。 flush translog：根据配置的 trans lo flush 策略进行刷盘控制 定时或立即刷盘。 写副分片：现在己经为要写的副本 shard 准备了一个列表，循环处理每个shard，跳过 unassigned 状态的shard，向目标节点发送请求，等待响应。这个过程是异步并行的。在等待Response的过程中，本节点发出了多少个Request就要等待多少个Response。无论这些 Response 是成功的还是失败的，直到超时。收集到全部的 Response 后，执行 finish()，给协调节点返回消息，告知其哪些成功、哪些失败了。 处理副分片写失败情况:主分片所在节点将发送一个shardFailed请求给 Master，然后 Master 会更新集群状态，在新的集群状态中，这个shard将从in_sync_allocations 中删除；在routing_table shard 表中将 state由STARTED 更改为 UNASSIGNED;添加到 routingNodes 的 unassignedShards 列表。 副分片节点流程执行与主分片基本相同的写doc过程，写完毕后回复主分片节点。在副分片的写入过程中，参数检查的实现与主分片略有不同，最终都调用 IndexShardOperationPermits#acquire 断是否需要 delay，继续后面的写流程。 异常处理 如果请求在协调节点的路由阶段失败，则会等待集群状态更新，拿到更新后，进行重试，如果再次失败，则仍旧等集群状态更新，直到超时1分钟为止。超时后仍失败则进行整体请求失败处理。 在主分片写入过程中，写入是阻塞的。只有写入成功，才会发起写副本请求。如果主shard 写失败，则整个请求被认为处理失败 如果有部分副本写失败，则整个请求被认为处理成功。 无论主分片还是副分片，当写一个 doc 失败时，集群不会重试，而是关闭本地 shard然后向 Master 汇报，删除是以 shard 为单位的。 ES系统特性ES 本身也是一个分布式存储系统，如同其他分布式系统一样，我们经常关注的一些特性如下： 数据可靠性：通过分片副本和事务日志机制保障数据安全性。 服务可用性：在可用性和一致性的取舍方面，默认情况下 ES 更倾向于可用性，只要主分片可用即可执行写入操作。 一致性：笔者认为是弱一致性。只要主分片写成功，数据就可能被读取。因此读取操作在主分片和副分片上可能会得到不同结果。 原子性：索引的读写、别名更新是原子操作，不会出现中间状态。但 bulk 不是原子操作，不能用来实现事务。 扩展性：主副分片都可以承担读请求，分担系统负载。 GET 流程ES 的读取分为GET和Search两种操作，这两种读取操作有较大差异，GET/MGET必须指定三元组：_index、_type、_id。也就是说，根据文档id从正排索引中获取内容。而Search不指定_id，根据关键词从倒排索引中获取内容。（GET是根据三元信息一次获取一个文档，MGET是根据三元信息一次获取多个文档） GET 基本流程搜索和读取文档都属于读操作，可以从主分片或副分片中读取数据。读取单个文档的流程： 客户端向 NODE1 发送读请求。 NODEl 使用文档 ID 来确定文档属于分片，通过集群状态中的内容路由表信息获知分片0有三个副本数据，位于所有的三个节点中，此时它可以将请求发送到任意节点，这里它将请求转发到 NODE2。 NODE2 将文档返回给 NODE1， NODEl 将文档返回给客户端。 NODEl 作为协调节点，会将客户端请求轮询发送到集群的所有副本来实现负载均衡。 在读取时，文档可能己经存在于主分片上，但还没有复制到副分片。在这种情况下，读请求命中副分片时可能会报告文档不存在，但是命中主分片可能成功返回文挡。 一旦写请求成功返回给客户端，则意味着文档在主分片和副分片都是可用的。 GET 详细流程GET/MGET 流程涉及两个节点：协调节点和数据节点，流程如下图所示： 协调节点用来处理存在于一个单个（主或副）分片上的读请求。将请求转发到目标节点，如果请求执行失败，则尝试转发到其他节点读取。在收到读请求后，处理过程如下。 内容路由 准备集群状态、节点表等信息。 根据内容路由算法计算目标shardid，也就是文档应该落在哪个分片上。 计算出目标shardid后，结合请求参数中指定的优先级和集群状态确定目标节点，由于分片可能存在多个副本，因此计算出是一个列表。 转发请求作为协调节点，向目标节点转发请求，或者目标是本地节点，直接读取数据。如果发送到网络，则请求被异步发送，等待数据节点的回复，如果数据节点处理成功，返回给客户端；如果数据节点处理失败，则进行重试。内容路由结束时构造了目标节点列表的迭代器，重试发送时，目标节点选择迭代器的下一个。 数据节点读取数据并组织成 Response ，给客户端 channel 返回。 读取及过滤 通过indexShard.get()获取Engine.GetResult()。 Engine.GetResult()类与 innerGet 返回的GetResult是同名的类，但实现不同。IndexShard.get()最终调用 Intema!Engin get 读取数据。 调用 ShardGetService#innerGetLoadFromStoredFields()，根据 type、id、DocumentMapp等信息从刚刚获取的信息中获取数据，对指定的 field、source 进行过滤（source 过滤只支持对字段），把结果存于 GetResult 对象中。 Intema!Engine#get 过程会加读锁。处理 realtime 选项，如果为 true ，则先判断是否有数据可以刷盘，然后调用 Searcher 进行读取。Searcher 是对 IndexSearcher 的封装。在早期的 ES 版本中，如果开启（默认）realtime，则会尝试从 translog 读取，刚写入不久的数据可以从 translog 中读取。5.x 开始不会从 translog 中读取，只从 Lucene 读。 realtime 的实现机制变成依靠 refresh 实现。 注意：GET API 默认是实时的，不受索引刷新（refresh）频率设置的影响。如果文档己经更新，但还没有刷新，则 GET API 将会发出一次刷新调用，使文档可见。 MGET 详细流程 主要流程： 遍历请求，计算出每个 doc 的路由信息，得到由 shardid key 组成的 request map。这个过程没有在 TransportSingleShardAction 中实现，是因为如果在那里实现，shardid 就会重复，这也是合并为基于分片的请求的过程。 循环处理组织好的每个 shard 级请求，调用处理 GET 请求时使用 TransportSingle\u0002ShardAction#AsyncSingleAction 处理单个 doc 的流程。 收集 Response ，全部 Response 返回后执行 finishHim()，给客户端返回结果。 回复的消息中文档顺序与请求的顺序一致。如果部分文档读取失败，则不影响其他结果，检索失败的 doc 会在回复信息中标出。 注意：update 操作需要先 GET 再写，为了保证一致性，update调用 GET 时将 realtime 选项设置为 true，并且不可配置，因此update操作可能会导致 refresh 生成新的 Lucene 分段。 Search 流程索引和搜索ES 中的数据可以分为两类：精确值和全文。 精确值，比如日期和用户 id、IP 地址等。 全文，指文本内容，比如一条日志，或者邮件的内容。 这两种类型的数据在查询时是不同的：对精确值的比较是二进制的，查询要么匹配，要么不匹配。全文内容的查询无法给出 有 还是 没有 的结果，它只能找到结果是“看起来像”你要查询的东西，因此把查询结果按相似度排序，评分越高，相似度越大。 对数据建立索引和执行搜索的原理如下图所示： 建立索引如果是全文数据，则对文本内容进行分析，这项工作在 ES 中由分析器实现。分析器实现如下功能： 字符过滤器：主要是对字符串进行预处理，例如，去掉 HTML ，将＆转换成and等。 分词器（Tokenizer）：将字符串分割为单个词条，例如，根据空格和标点符号分割，输出的词条称为词元（Token）。 Token 过滤器：根据停止词（Stop word）删除词元，例如， and the 等无用词，或者根据同义词表增加词条，例如，jump 和 leap。 语言处理：对上一步得到的 Token 做一些和语言相关的处理，例如，转为小写，以及将单词转换为词根的形式。语言处理组件输出的结果称为词（Term）。 分析完毕后，将分析器输出的词（Term）传递给索引组件，生成倒排和正排索引，再存储到文件系统中。 文档 -&gt; 字符过滤器 -&gt; 分词器 -&gt; 词条过滤器 -&gt; 语言处理 -&gt; 文档写入（正排索引 既 doc values） -&gt; 倒排索引 执行搜索搜索调用 Lucene 完成，如果是全文检索，则： 对检索字段使用建立索引时相同的分析器进行分析，产生 Token 列表； 根据查询语句的语法规则转换成一颗语法树； 查找符合语法树的文档。 对匹配到的文档列表进行相关性评分，评分策略一般使用 TF/IDF;(新版现在都是BM25) 5.根据评分结果进行排序。 分布式搜索过程一个搜索请求必须询问请求的索引中所有分片的某个副本来进行匹配。一次搜索请求只会命中所有分片副本中的一个。当搜索任务执行在分布式系统上时， 整体流程如下图所示： 协调节点流程协调节点有两个流程，Query阶段 和 FETCH阶段。 Query阶段QUERY_THEN_FETCH 搜索类型的查询阶段步骤如下： 客户端发送 search 请求到 NODE3。 Node3 将查询请求转发到索引的每个主分片或副分片中。 每个分片在本地执行查询，并使用本地的 Term/Document Frequency 信息进行打分，添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，协调节点合并这些值到自己的优先队列中，产生一个全局排序后的列表。 协调节点广播查询请求到所有相关分片时，可以是主分片或副分片，协调节点将在之后的请求中轮询所有的分片副本来分摊负载。查询阶段并不会对搜索请求的内容进行解析，无论搜索什么内容，只看本次搜索需要命中哪些 shard，然后针对每个特定 shard 选择一个副本，转发搜索请求。 Query阶段详细流程 解析请求：将请求体解析为 SearchRequest 数据结构。 构造目的 shard 列表：将请求涉及的本集群 shard 列表和远程集群的 shard 列表（远程集群用于跨集群访）合并。 遍历所有 shard 发送请求：请求是基于 shard 遍历的，如果列表中有 shard 位于同一个节点，则向其发送N次请求，并不会把请求合并为一个。 收集返回结果：对收集到的结果进行合并。 FETCH阶段Query 阶段知道了要取哪些数据，但是并没有取具体的数据，这就是 Fetch 阶段要做的。 Fetch阶段由以下步骤构成： 协调节点向相关 NODE 发送 GET 请求。 分片所在节点向协调节点返回数据。 协调节点等待所有文档被取得，然后返回给客户端。 分片所在节点在返回文档数据时，处理有可能出现的 _source 字段和高亮参数。 协调节点首先决定哪些文档确实需要被取回，例如，如果查询指定了 { from:90, size:10 ｝，则只有从第91个开始的10个结果需要被取回。为了避免在协调节点中创建的 number_of_shards * (from + size）优先队列过大，应该尽量控制分页深度。 Fetch阶段详细流程Fetch 阶段的目的是通过文档 ID 获取完整的文档内容。 发送 Fetch 请求。 收集结果。 ExpandSearchPhase：取回阶段完成之后执行 ExpandSearchPhase#run， 主要判断是否启用字段折叠，根据需要实现字段折叠功能，如果没有实现字段折叠，直接返回给客户端。 回复客户端。 数据节点流程晌应 Query 请求主要过程就是执行查询，然后发送 Response。查询时，先看是否允许 cache ，由以下配置决定： index.requests.cache.enable 。 默认为 true，会把查询结果放到 cache 中， 查询 优先从 cache 中取。这 cache 由节点的所有分片共享，基于 LRU 算法实现：空间满的时候删除最近最少使用的数据。cache 并不缓存全部检索结果。 注意：慢查询 Query 日志的统计时间在于本阶段的处理时间。聚合操作在本阶段实现，在 Lucene 检索后完成。 响应 Fetch 请求主要过程是执行 Fetch 然后发送 Response。慢查 Fetch 日志的统计时间在于本阶段的处理时间。 小结 聚合是在 ES 实现的，而非 Lucene。 Query Fetch 请求之间是无状态的，除非是 scroll 方式。 分页搜索不会单独“cache”， cache和分页没有关系。 每次分页的请求都是一次重新搜索的过程，而不是从第一次搜索的结果中获取。看上去不太符合常规的做法，事实上互联网的的搜索引擎都是重新执行了搜索过程：人们基本只看前几页，很少深度分页；重新执行一次搜索很快；如果缓存第一次搜索结果等待翻页命中，则这种缓存的代价较大，意义却不大，因此不如重新执行一次搜索。 搜索需要遍历分片所有的 ucen 分段，因此合并 Lucene 分段对搜索性能有好处。 es 默认采用的分页方式是 from+ size 的形式，在深度分页的情况下，这种使用方式效率是非常低的，比如我们执行如下查询： 12345678GET /student/student/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;:5000, &quot;size&quot;:10&#125; 意味着 es 需要在各个分片上匹配排序并得到5010条数据，协调节点拿到这些数据再进行排序等处理，然后结果集中取最后10条数据返回。我们会发现这样的深度分页将会使得效率非常低，因为我只需要查询10条数据，而es则需要执行from+size条数据然后处理后返回。 其次：es为了性能，限制了我们分页的深度，es目前支持的最大的 max_result_window = 10000；也就是说我们不能分页到10000条数据以上。 在es中如果我们分页要请求大数据集或者一次请求要获取较大的数据集，scroll都是一个非常好的解决方案。 使用scroll滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来scroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的。每次发送scroll请求，我们还需要指定一个scroll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/tags/Elasticsearch/"},{"name":"核心技术二","slug":"核心技术二","permalink":"https://jjw-story.github.io/tags/核心技术二/"}],"author":"JJW"},{"title":"Lamda","slug":"Lamda","date":"2019-12-29T08:57:11.000Z","updated":"2020-02-18T03:22:50.733Z","comments":true,"path":"2019/12/29/Lamda/","link":"","permalink":"https://jjw-story.github.io/2019/12/29/Lamda/","excerpt":"","text":"Lamda表达式基本语法函数式接口的应用函数式接口就是Java类型系统中的接口，是只包含一个接口方法的特殊接口，我们在定义函数式接口时，可以使用注解 @FunctionalInterface 来完成语义化的检测 以下是函数式接口的定义代码示例： 1234567891011121314151617181920212223242526272829303132// 定义函数式接口// 这里使用此注解来帮助我们实现一个正确的函数式接口@FunctionalInterfacepublic interface IUserCredential &#123; // 每一个函数式接口只能包含一个未实现的方法 String verifyUser(String username); // 这里注意，如果包含了两个就会报错，在编译期 // boolean test(); // 有一个特例，因为Java中的类都继承了Object，所以Object类中的方法可以写在这里不实现它 String toString(); // 注意：接口中可以包含实现的静态方法 static boolean verifyMessage(String msg) &#123; if (msg != null) &#123; return true; &#125; return false; &#125; // 1.8中可以在接口中定义默认的方法实现 default String getCredential(String username) &#123; // 模拟方法 if (&quot;admin&quot;.equals(username)) &#123; return &quot;admin + 系统管理员用户&quot;; &#125; else &#123; return &quot;commons + 普通会员用户&quot;; &#125; &#125;&#125; 以下是函数式接口的使用示例： 1234567891011121314151617181920212223// 首先是普通接口的调用实现// 1. 普通实现，直接初始化接口实现类IUserCredential ic = new UserCredentialImpl();System.out.println(ic.verifyUser(&quot;admin&quot;));// 2. 静态方法，直接调用String msg = &quot;hello world&quot;;IMessageFormat.verifyMessage(msg)// 3. 匿名内部类，实现接口的抽象方法IUserCredential ic2 = new IUserCredential() &#123; @Override public String verifyUser(String username) &#123; return &quot;admin&quot;.equals(username)?&quot;管理员&quot;:&quot;会员&quot;; &#125;&#125;;// 4. 使用Lamda的方式实现接口的抽象方法// 这里我们需要注意，Lamda是通过返回的接收对象确定它实现的是哪个接口的方法，所以当有多个函数式接口中都有相同的未实现方法参数列表时，我们是通过返回的接收对象来绑定实现的接口类型，，如此示例中，就是通过 IUserCredential ic3 这个返回的接收接口来类型推导，绑定实现的接口，所以不会有冲突的情况IUserCredential ic3 = (String username) -&gt; &#123; return &quot;admin&quot;.equals(username)?&quot;lbd管理员&quot;: &quot;lbd会员&quot;;&#125;; 以下是JDK8提供的常见的函数式接口 12345678910111213141516171819202122232425262728293031323334353637// 1. PredicatePredicate&lt;String&gt; pre = (String username) -&gt; &#123; return &quot;admin&quot;.equals(username);&#125;;System.out.println(pre.test(&quot;manager&quot;));// 2. ConsumerConsumer&lt;String&gt; con = (String message) -&gt; &#123; System.out.println(&quot;要发送的消息：&quot; + message); System.out.println(&quot;消息发送完成&quot;);&#125;;con.accept(&quot;hello 慕课网的学员们..&quot;);// 3. FunctionFunction&lt;String, Integer&gt; fun = (String gender) -&gt; &#123; return &quot;male&quot;.equals(gender)?1:0;&#125;;System.out.println(fun.apply(&quot;male&quot;));// 4. SupplierSupplier&lt;String&gt; sup = () -&gt; &#123; return UUID.randomUUID().toString();&#125;;System.out.println(sup.get());// 5. UnaryOperatorUnaryOperator&lt;String&gt; uo = (String img)-&gt; &#123; img += &quot;[100x200]&quot;; return img;&#125;;System.out.println(uo.apply(&quot;原图--&quot;));// 6. BinaryOperatorBinaryOperator&lt;Integer&gt; bo = (Integer i1, Integer i2) -&gt; &#123; return i1 &gt; i2? i1: i2;&#125;;System.out.println(bo.apply(12, 13)); java.util.function提供了大量的函数式接口，以上示例使用总结如下： Predicate 接收参数T对象，返回一个boolean类型结果 Consumer 接收参数T对象，没有返回值 Function 接收参数T对象，返回R对象 Supplier 不接受任何参数，直接通过get()获取指定类型的对象 UnaryOperator 接口参数T对象，执行业务处理后，返回更新后的T对象 BinaryOperator 接口接收两个T对象，执行业务处理后，返回一个T对象 lambda表达式的基本语法: 声明：就是和lambda表达式绑定的接口类型 参数：包含在一对圆括号中，和绑定的接口中的抽象方法中的参数个数及顺序一致 操作符：-&gt; 执行代码块：包含在一对大括号中，出现在操作符号的右侧 [接口声明] = (参数) -&gt; {执行代码块}; Lamda表达式的变量捕获首先我们查看原始代码风格中的变量捕获方式及存在的问题: 123456789101112131415161718192021222324252627public class App2 &#123; String s1 = &quot;全局变量&quot;; String s3 = &quot;全局变量s3&quot;; // 1. 匿名内部类型中对于变量的访问 public void testInnerClass() &#123; String s2 = &quot;局部变量&quot;; new Thread(new Runnable() &#123; String s3 = &quot;内部变量s3&quot;; @Override public void run() &#123; // 访问全局变量，这里用this访问到的是外部类中的全局变量s1，因为在内部类中并没有定义s1 System.out.println(this.s1); System.out.println(s1); System.out.println(s2); // 局部变量的访问，~不能对局部变量进行数据的修改[final] // s2 = &quot;hello&quot;; // 这里访问的就是内部类中定义的内部变量，因为就近原则，所以这里很多时候就会被初学者产生歧义 System.out.println(s3); System.out.println(this.s3); &#125; &#125;).start(); &#125; 我们再用lambda表达式变量捕获的方式来实现一遍： 12345678910111213141516171819202122232425public class App2 &#123; String s1 = &quot;全局变量&quot;; String s3 = &quot;全局变量s3&quot;; public void testLambda() &#123; String s2 = &quot;局部变量lambda&quot;; new Thread(() -&gt; &#123; String s3 = &quot;内部变量lambda&quot;; // 访问全局变量 System.out.println(this.s1);// this关键字，表示的就是所属方法所在类型的对象 // 访问局部变量 System.out.println(s2); // 这里不能进行数据修改，默认推导变量的修饰符：final，因为获取到的s2是在栈中的 s2 = &quot;hello&quot;; // 这里我们捕获到的变量就是外部定义的s3全局变量，这里就不会产生歧义 System.out.println(s3); // 这里获取到的s3变量是可以修改的，因为此外部的变量是存放在堆中的 s3 = &quot;labmda 内部变量直接修改&quot;; System.out.println(s3); &#125;).start(); &#125;&#125; 通过以上代码示例，我们就可以看出Lamda表达式对于变量捕获的影响 方法重载对于lmabda表达式的影响具体我们通过代码来查看： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class App4 &#123; interface Param1 &#123; void outInfo(String info); &#125; interface Param2 &#123; void outInfo(String info); &#125; // 定义重载的方法 public void lambdaMethod(Param1 param) &#123; param.outInfo(&quot;hello param1 imooc!&quot;); &#125; public void lambdaMethod(Param2 param) &#123; param.outInfo(&quot;hello param2 imooc&quot;); &#125; // 这里我们通过匿名内部类来实现此重载方法，这样是没有任何问题的 public static void main(String[] args) &#123; App4 app = new App4(); app.lambdaMethod(new Param1() &#123; @Override public void outInfo(String info) &#123; System.out.println(info); &#125; &#125;); app.lambdaMethod(new Param2() &#123; @Override public void outInfo(String info) &#123; System.out.println(&quot;------&quot;); System.out.println(info); &#125; &#125;); // 这里我们通过lamda表达式来重载，发现是失败的， 会报错，因为它无法推导出具体要绑定的接口是哪个 // app.lambdaMethod( (String info) -&gt; &#123; // System.out.println(info); // &#125;); &#125;&#125; 以上示例说明，Lamda是存在类型推导的，lambda表达式存在类型检查-&gt; 自动推导lambda表达式的目标类型，具体推导流程如下 1234567lambdaMethod() -&gt; 方法 -&gt; 重载方法 -&gt; Param1 函数式接口 -&gt; Param2 函数式接口 调用方法-&gt; 传递Lambda表达式-&gt; 自动推导-&gt; -&gt; Param1 | Param2这里到了推导的最后一步，发现有两个接口满足此推导逻辑，导致产生了歧义，这样就出现了问题，导致无法编译 Lamda的方法引用的实现我们还是通过示例的代码来分析： 12345678910111213141516171819202122232425262728293031// 首次定义用于示例的实体类@Data@AllArgsConstructor@NoArgsConstructorclass Person &#123; private String name; // 姓名 private int age; // 年龄 // 这里使用lombox，有默认的构造方式 AllArgsConstructor // 静态方法 public static int compareByAge(Person p1, Person p2) &#123; return p1.getAge() - p2.getAge(); &#125; // 实例方法 public int compareByName(Person p1, Person p2) &#123; return p1.getName().hashCode() - p2.getName().hashCode(); &#125;&#125;// 静态方法调用Person::compareByAge// 实例方法调用Person pu = new Person();pu::compareByName// 构造方法引用：绑定函数式接口Person ip = Person::new; // 无参构造Person person = ip.initPerson(&quot;jerry&quot;, &quot;男&quot;, 22); // 有参构造 Stream常见操作API介绍Stream的获取 批量数据 -&gt; Stream对象 123456789101112131415161718192021222324// 多个数据Stream stream = Stream.of(&quot;admin&quot;, &quot;tom&quot;, &quot;damu&quot;);// 数组String [] strArrays = new String[] &#123;&quot;xueqi&quot;, &quot;biyao&quot;&#125;;Stream stream2 = Arrays.stream(strArrays);// 列表List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(&quot;少林&quot;);list.add(&quot;武当&quot;);Stream stream3 = list.stream();// 集合Set&lt;String&gt; set = new HashSet&lt;&gt;();set.add(&quot;武当长拳&quot;);set.add(&quot;青城剑法&quot;);Stream stream4 = set.stream();// MapMap&lt;String, Integer&gt; map = new HashMap&lt;&gt;();map.put(&quot;tom&quot;, 1000);map.put(&quot;jerry&quot;, 1200);Stream stream5 = map.entrySet().stream(); Stream对象对于基本数据类型的功能封装 注意，当前只支持 int / long / double 这三种数据类型 123IntStream.of(new int[] &#123;10, 20, 30&#125;).forEach(System.out::println);IntStream.range(1, 5).forEach(System.out::println); // 开区间IntStream.rangeClosed(1, 5).forEach(System.out::println); // 闭区间 Stream对象 –&gt; 转换得到指定的数据类型 注意：这里涉及到的都是终结操作，既每一个操作后stream流就被关闭了 123456789101112131415161718// 数组Object [] objx = stream.toArray(String[]::new);// 字符串String str = stream.collect(Collectors.joining()).toString();System.out.println(str);// 列表List&lt;String&gt; listx = (List&lt;String&gt;) stream.collect(Collectors.toList());System.out.println(listx);// 集合Set&lt;String&gt; setx = (Set&lt;String&gt;) stream.collect(Collectors.toSet());System.out.println(setx);// MapMap&lt;String, String&gt; mapx = (Map&lt;String, String&gt;) stream.collect(Collectors.toMap(x-&gt;x, y-&gt;&quot;value:&quot;+y));System.out.println(mapx); Stream常见API介绍 聚合操作 stream的处理流程 数据源 数据转换 获取结果 获取Stream对象 1234567891011121314* 1. 从集合或者数组中获取[**]* Collection.stream()，如accounts.stream()* Collection.parallelStream()* Arrays.stream(T t)* 2. BufferReader* BufferReader.lines()-&gt; stream()* 3. 静态工厂* java.util.stream.IntStream.range()..* java.nio.file.Files.walk()..* 4. 自定构建* java.util.Spliterator* 5. 更多的方式..* Random.ints()* Pattern.splitAsStream().. 中间操作API{intermediate} 操作结果是一个Stream，中间操作可以有一个或者多个连续的中间操作，需要注意的是，中间操作只记录操作方式，不做具体执行，直到结束操作发生时，才做数据的最终执行。 中间操作：就是业务逻辑处理。 中间操作过程：无状态：数据处理时，不受前置中间操作的影响，如下面所列操作： map/filter/peek/parallel/sequential/unordered 有状态：数据处理时，受到前置中间操作的影响，如下面所列操作： distinct/sorted/limit/skip 终结操作|结束操作{Terminal} 需要注意：一个Stream对象，只能有一个Terminal操作，这个操作一旦发生，就会真实处理数据，生成对应的处理结果。 终结操作：非短路操作：当前的Stream对象必须处理完集合中所有 数据，才能得到处理结果，如下面所列操作： forEach/forEachOrdered/toArray/reduce/collect/min/max/count/iterator 短路操作：当前的Stream对象在处理过程中，一旦满足某个条件，就可以得到结果。 anyMatch/allMatch/noneMatch/findFirst/findAny等 Short-circuiting，无限大的Stream-&gt; 有限大的Stream。 Stream常见的API操作 基本常用的操作支持 123456789101112131415161718192021// 定义一个集合数据List&lt;String&gt; accountList = new ArrayList&lt;&gt;();accountList.add(&quot;songjiang&quot;);accountList.add(&quot;lujunyi&quot;);accountList.add(&quot;wuyong&quot;);// 1.map() 中间操作，map()方法接收一个Functional接口accountList = accountList.stream().map(x-&gt;&quot;梁山好汉:&quot; + x).collect(Collectors.toList());// 2.filter() 添加过滤条件，过滤符合条件的用户accountList = accountList.stream().filter(x-&gt; x.length() &gt; 5).collect(Collectors.toList());// 3.forEach 增强型循环accountList.forEach(x-&gt; System.out.println(&quot;forEach-&gt;&quot; + x));// 4.peek() 中间操作，迭代数据完成数据的依次处理过程，这里只循环一次，但是能处理两件事情accountList.stream() .peek(x -&gt; System.out.println(&quot;peek 1: &quot; + x)) .peek(x -&gt; System.out.println(&quot;peek 2:&quot; + x)) .forEach(System.out::println); Stream中对于数字运算的支持 123456789101112131415161718192021222324252627282930// 定义一个数字集合List&lt;Integer&gt; intList = new ArrayList&lt;&gt;();intList.add(20);intList.add(19);intList.add(7);intList.add(12);intList.add(5);// 1.skip() 中间操作，有状态，跳过部分数据，示例跳过前三个数据intList.stream().skip(3).forEach(System.out::println);// 2.limit() 中间操作，有状态，限制输出数据量，示例跳过前三个数据，然后取跳过后的前两个数据intList.stream().skip(3).limit(2).forEach(System.out::println);// 3.distinct() 中间操作，有状态，剔除重复的数据intList.stream().distinct().forEach(System.out::println);// 4.sorted() 中间操作，有状态，排序// 5.max() 获取最大值Optional optional = intList.stream().max((x, y)-&gt; x-y);System.out.println(optional.get());// 6.min() 获取最小值Optional optional = intList.stream().min((x, y)-&gt; x-y);System.out.println(optional.get());// 7.reduce() 合并处理数据Optional optional2 = intList.stream().reduce((sum, x)-&gt; sum + x);System.out.println(optional2.get()); 补充 ParallelStreamParallelStream是一个并发多线程的Stream 使用示例： 12345678910111213141516171819202122Optional optional = list.parallelStream().max(Integer::compare);System.out.println(optional.get());// 整数列表，注意：下面示例是为了说明并行的Stream会出现线程安全问题List&lt;Integer&gt; lists = new ArrayList&lt;Integer&gt;();// 增加数据for (int i = 0; i &lt; 1000; i++)&#123; lists.add(i);&#125;// 串行StreamList&lt;Integer&gt; list2 = new ArrayList&lt;&gt;();lists.stream().forEach(x-&gt;list2.add(x));System.out.println(lists.size());System.out.println(list2.size());// 并行StreamList&lt;Integer&gt; list3 = new ArrayList&lt;&gt;();lists.parallelStream().forEach(x-&gt; list3.add(x));System.out.println(list3.size());//List&lt;Integer&gt; list4 = lists.parallelStream().collect(Collectors.toList());System.out.println(list4.size());","categories":[{"name":"Lamda","slug":"Lamda","permalink":"https://jjw-story.github.io/categories/Lamda/"}],"tags":[{"name":"Lamda","slug":"Lamda","permalink":"https://jjw-story.github.io/tags/Lamda/"}],"author":"JJW"},{"title":"内存与磁盘管理","slug":"内存与磁盘管理","date":"2019-11-10T06:22:53.000Z","updated":"2019-12-15T05:02:11.349Z","comments":true,"path":"2019/11/10/内存与磁盘管理/","link":"","permalink":"https://jjw-story.github.io/2019/11/10/内存与磁盘管理/","excerpt":"","text":"内存与磁盘管理内存查看命令free命令通过free命令可以查看当前内存的使用情况，使用方法： free [参数] 参数： -m 将数据按以MB为单位进行显示 -g 将数据按以GB为单位进行显示 注意我们一般不用 -g 参数，因为例如我们内存使用了1990M，但是-g会显示1g，将超出的都舍去了，所以会显示的不精准 下面是使用示例： 1234567891011121314151617181920# 1.直接查看[root@iZm5ehzqow4ijp2ya2g2drZ ~]# free total used free shared buffers cachedMem: 1019980 838124 181856 160 142184 337192-/+ buffers/cache: 358748 661232Swap: 0 0 0# 2.使用-m参数[root@iZm5ehzqow4ijp2ya2g2drZ ~]# free -m total used free shared buffers cachedMem: 996 818 177 0 138 329-/+ buffers/cache: 350 645Swap: 0 0 0# 3.使用-g参数[root@iZm5ehzqow4ijp2ya2g2drZ ~]# free -g total used free shared buffers cachedMem: 0 0 0 0 0 0-/+ buffers/cache: 0 0Swap: 0 0 0 以上查询结果我们需要注意的是，buffers/cache 项目查询的结果，有的时候我们查看内存发现我们 used 的内存已经非常大，free 的内存就剩一点点这个时候我们先不要着急去想办法加内存，我们首先查看 buffers/cache 下的内存有多少，如果这里显示的内存使用了很多，那么我们就不需要加内存，因为这里使用的内存是缓冲剂缓存使用的，这些都是可以被回收的内存，所以当内存真的撑不住的时候，会回收这一部分，当然，CentOS6没有查询出 available 的内容，如果可以看到这个的占用，这里是汇总的所有可释放的占用，我们可以直接查看这一块的占用，来判断需不需要加内存 还有要注意的就是Swap，它是交换分区的意思，它占用的并不是实际的内存，而是使用的磁盘空间，这里一般查看都是0，如果我们发现查询的时候这里的占用不是0，那么就真的该加内存了，这里就跟Windows中虚拟内存是一样的，指的是内存本身不够用了，我们需要将一些超出的使用转移到硬盘上，将硬盘的一些容量当做内存使用，但是磁盘的读写速度比内存慢十倍，所以一般我们看到交换分区被使用了，就该加内存了。这个交换分区大小一般默认为2G，可以手动指定。 如果我们不指定swap分区，既超出内存占用后不使用交换分区，那么系统会随机杀掉一些占用内存较大的进程，当然这样是不安全的，所以我们一般还是保留swap存在。还有一些Redies了，Memcache等内存数据库，它们不会使用swap分区，内存超出后就停止新增数据了 top命令top命令我们在进程管理中已经讲解完毕了，可以直接去进程管理章节查看 磁盘查看命令fdisk命令fdisk命令既可以查看磁盘的信息，又可以修改磁盘的分区，一般不要直接执行此命令进行分区，会有风险 使用方法： fdisk [参数] -l 查询磁盘信息，使用示例如下： 1234567891011[root@iZm5ehzqow4ijp2ya2g2drZ ~]# fdisk -lDisk /dev/vda: 42.9 GB, 42949672960 bytes255 heads, 63 sectors/track, 5221 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x0003a7b4 Device Boot Start End Blocks Id System/dev/vda1 * 1 5222 41940992 83 Linux Linux的磁盘都是用文件来表示的，磁盘文件所在的位置是:/dev目录下，vda表示是主机固有磁盘，我们有时候会见到很多 /dev/sda 的磁盘，这种表示的是可插拔的磁盘，类似于我们外接的移动硬盘等，磁盘一般表示都是 vda ~ vdb …这样按顺序往下排，同样挂载的硬盘也是，sda ~ sdb 等排列 上述命令查询出信息的第一行有磁盘总大小的表示方法，一种是以GB为单位，一种是换算为字节为单位，一般我们关注第一行的信息即可，还有就是最后的表格展示的项目，当我们有多快磁盘在机器中时，我们需要看到boot选项的 * 号在哪块磁盘上，* 号表示的就是我们系统启动的引导盘 我们可以直接进入 /dev 目录具体的查看硬盘的文件信息。如下示例： 12345[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ls -l /dev/vd?brw-rw---- 1 root disk 252, 0 Sep 14 09:44 /dev/vda[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ls -l /dev/vd??brw-rw---- 1 root disk 252, 1 Sep 14 09:44 /dev/vda1 如上查询，disk后跟的数字252表示磁盘的主设备号，第二个数字表示磁盘的从设备号，我们第二条命令查询出来的发现是 vda1，其中1表示这块磁盘的分区为1分区，如同我们Windows中一块磁盘可以划分为CDE几块不同的分区，Linux也有分区的概念，如果有多个分区，就是从 1 … n，这样的表示方法 df命令此命令也可以查询磁盘的具体大小，它可以作为fdsik命令的补充，因为fdisk命令只能查看到磁盘的大小，看不到磁盘具体挂载的目录以及目录的大小，这个时候我们就可以使用df命令来查看，一般我们会使用 -h 参数来具体查看，使用示例如下： 1234[root@iZm5ehzqow4ijp2ya2g2drZ ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 2.5G 35G 7% /tmpfs 499M 0 499M 0% /dev/shm 如上我们就可以看到我们机器的分区挂载目录，以及此目录的大小，注意：此命令是我们最常用的 du命令上述命令都是直接查看磁盘大小的，我们还可以通过du命令查看具体文件夹的大小，但是注意，我们同样可以是用ls -l命令查看文件的大小，但是ls命令查询出的文件大小可能与du命令查出的文件大小是不一样的，这是因为，ls查询出的文件包含了文件占用的全部大小，而这全部大小可能包含了很多空洞文件，空洞文件表示的是可以理解为一块磁盘空间，我们只给空间头和尾加了标记，其他位置全部都是空的，这样的文件就叫做空洞文件，我们可以使用dd命令来创建空洞命令，这里不做详细解释 在查看文件的具体大小时我们可以使用此两种命令查看文件的具体大小，使用示例如下： 12345[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# ls -lh SparrowNet-1.0-SNAPSHOT.jar -rw-r--r-- 1 root root 51M Oct 19 14:27 SparrowNet-1.0-SNAPSHOT.jar[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# du -h SparrowNet-1.0-SNAPSHOT.jar 51M SparrowNet-1.0-SNAPSHOT.jar 以上两种命令所查出的文件大小是一样的，说明此文件中没有包含空洞文件 我们需要查看文件夹的大小时，我们就可以使用du命令，因为我们发现ls命令查看的文件夹大小并不包含文件夹中的内容，所以无法知道文件夹及其中内容的总大小，这时我们就可以使用 du -sh *命令，注意此命令是重点，要牢记，使用示例如下： 1234567891011121314151617181920# 首先使用 ls -lh 命令查看，这里只包含一级目录的大小，并没有包含子目录中文件的总大小[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# ls -lhtotal 51Mdrwxrwxrwx 2 root root 4.0K Nov 15 01:02 datadrwxr-xr-x 2 root root 4.0K Nov 17 00:07 logsdrwxr-xr-x 2 root root 4.0K Oct 19 14:28 packagedrwxrwxrwx 5 root root 4.0K Sep 14 17:20 resources-rw-r--r-- 1 root root 51M Oct 19 14:27 SparrowNet-1.0-SNAPSHOT.jar# 使用 du 命令的 -sh 参数查询文件的总大小，包含了子目录中所有文件的大小[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# du -sh84M .# 使用 du -sh * 命令分别查看一级目录中所有文件及子目录的总大小[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# du -sh *60K data9.0M logs4.0K package24M resources51M SparrowNet-1.0-SNAPSHOT.jar 通过上述示例我们很容易能看出区别，所以此命令我们需要牢记 常见文件系统Linux支持多种文件系统，常见的有： ext4 xfs NTFS(需要安装额外转件) 目前比较流行的是前两种，CentOS7使用的xfs文件系统，之前的版本使用的都是ext4文件系统，这两个操作系统都是足够稳定的，NTFS文件系统Windows使用的文件系统，我们将这种格式的磁盘挂载到Linux系统上的时候，它会是只读状态，因为这种文件系统是有版权的，需要额外安装一个软件才能操作它 这里我们介绍最常用的ext4文件系统，它分为四个部分： 超级块 超级块副本 I节点(iNode) 数据块(datablock) 超级块是在文件系统最开始的部分的空间，它所存储的主要是磁盘的大小，磁盘中文件所占用的大小，包含了多少个文件，文件的总数，我们在使用df命令很快就能查看出文件的总数及大小，就是因为它查看的是超级块中的信息 超级块副本是超级块的备份，它不止有一份，当超级块出现故障的时候，可以在这里恢复 i节点记录的是每一个文件，它记录的是文件的大小，位置，创建时间，修改时间，文件的权限，编号等等信息，但是这里不包括文件的名称，注意：文件的名称是记录在自己文件的父目录的i节点中，当文件过多的时候，也会记录在父目录的数据块中 数据块就是记录的真实的文件数据，数据块是挂在i节点后的，我们只要能找到文件的i节点，就能找到文件的数据块，就可以通过挂的数据块的个数来知道文件的大小 所以我们使用ls查看文件的大小的时候，查看的是i节点里面文件的大小信息，而du统计的是i节点挂在的数据块的个数统计文件的大小信息，所以说du查看的是真实的文件大小 i节点和数据块操作我们首先观察cp和mv命令 12345678910111213141516171819202122232425# 首先我们创建一个文件并查看文件信息，发现它占用0k大小[root@iZm5ehzqow4ijp2ya2g2drZ test]# touch file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# ls -litotal 01179654 -rw-r--r-- 1 root root 0 Dec 1 14:34 file1# 然后写入文件123字符，发现大小时4k[root@iZm5ehzqow4ijp2ya2g2drZ test]# echo &gt;file1 123[root@iZm5ehzqow4ijp2ya2g2drZ test]# ls -litotal 41179657 -rw-r--r-- 1 root root 4 Dec 1 14:34 file1# 然后copy这个文件，发现两个文件占用大小相同，复制的文件创建了一个新的i节点[root@iZm5ehzqow4ijp2ya2g2drZ test]# cp file1 file2[root@iZm5ehzqow4ijp2ya2g2drZ test]# ls -litotal 81179657 -rw-r--r-- 1 root root 4 Dec 1 14:34 file11179654 -rw-r--r-- 1 root root 4 Dec 1 14:35 file2# 然后我们移动file2到当前目录修改名称为file3，发现file2的节点没有发生变化，只是文件名称发生变化[root@iZm5ehzqow4ijp2ya2g2drZ test]# mv file2 file3[root@iZm5ehzqow4ijp2ya2g2drZ test]# ls -litotal 81179657 -rw-r--r-- 1 root root 4 Dec 1 14:34 file11179654 -rw-r--r-- 1 root root 4 Dec 1 14:35 file3 通过上述示例，我们发现，复制名称是创建了一个新的i节点及文件，而移动操作并不会创建新的i节点及数据块，只是修改了目中的文件的信息，比如上述文件名 注意：上述移动我们只是移动到当前目录下，所以只需要修改名称，这个过程是非常快的。如果我们将文件移动到此分区的其他目录，我们发现移动也非常快，通过原理我们知道，移动到其他目录，我们也并不需要创建新的i节点及数据块等，只需要将目录链接做修改即可。但是如果我们将文件移动到其他分区，这里因为发生了文件系统的变换，从一个文件系统移动到另一个文件系统，这里就需要在创建新的i节点和数据块，所以这个过程就比较慢了。 注意：一般我们创建文件，默认大小就是4K的大小，虽然上述示例我们只写入了三个数字，只占用102位，也就是12个字节，但是文件的大小还是4K，所以我们要创建很多小文件的时候，需要注意一下浪费空间，这个当然有办法解决，可以自行google 上述我们给文件写入数据使用的是echo命令，注意的是，如果我们使用vim来修改文件内容的话，文件的i节点及数据块都会发生变化，这里在CentOS6、7版本都是这样。如果i节点发生变化，那么此文件就已经不是原来的文件，这里主要是vim对于文件一致性的考虑，所以会有这样的操作机制，vim打开的文件实际是一个交换文件，具体可以自行google rm命令，其实也不是将文件直接删除，而是将i节点与数据块的链接给断开了，所以我们在删除文件的时候操作也是非常快，不管多大的文件 利用ext4和xfs文件系统的特性修改文件的权限我们之前知道原始的文件的权限就三种，rwx，原始修改文件权限的方式是一改全改，但是当我们有特殊的业务场景，比如我们需要一个文件，user1有读权限，user2有写权限，user3有执行权限，这个时候我们原始的设置权限的方式就不能满足我们的需求了，这个时候我们就可以利用ext4和xfs文件系统的特性来满足此需求，这两个文件系统支持了一个文件访问控制列表，叫facl的功能 getfacl命令我们可以使用getfacl命令来查看文件的控制访问列表，使用方法: getfacl 文件名，示例如下： 12345678910111213# 普通查看方式[root@iZm5ehzqow4ijp2ya2g2drZ test]# ll file1-rw-r--r-- 1 root root 4 Dec 1 14:34 file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# lsfile1[root@iZm5ehzqow4ijp2ya2g2drZ test]# getfacl file1# file: file1# owner: root# group: rootuser::rw-group::r--other::r-- 这里我们使用faclget命令查看的文件控制访问列表跟我们使用ll查询出的信息一致，这是基本的一些控制信息，那我们怎么赋予指定用户或用户组的一些权限呢？使用setfacl命令 setfacl命令我们可以使用setfacl为指定的用户或用户组赋予权限，使用方法:setfacl -m u:用户名称:权限符号 文件名，具体使用示例如下： 123456# 设置[root@iZm5ehzqow4ijp2ya2g2drZ test]# setfacl -m u:user01:r file1# 查看文件权限[root@iZm5ehzqow4ijp2ya2g2drZ test]# ls -l file1-rw-r--r--+ 1 root root 4 Dec 1 14:34 file1 我们通过setfacl命令设置完权限之后，我们使用普通ll命令查看此文件权限的时候，发现权限列表中 -rw-r–r–+ 多了一个+号，这个+号表示除了我们标准权限之后，此文件还被设置了文件访问控制权限facl的权限，我们必须使用getfacl命令来查看，如下： 123456789[root@iZm5ehzqow4ijp2ya2g2drZ test]# getfacl file1# file: file1# owner: root# group: rootuser::rw-user:user01:r--group::r--mask::r--other::r-- 我们发现此文件user01用户有了读的权限，我们还可以给user02设置读和写的权限，设置示例如下： 12345678910111213141516[root@iZm5ehzqow4ijp2ya2g2drZ test]# setfacl -m u:user02:rw file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# lltotal 4-rw-rw-r--+ 1 root root 4 Dec 1 14:34 file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# getfacl file1# file: file1# owner: root# group: rootuser::rw-user:user01:r--user:user02:rw-group::r--mask::rw-other::r-- 如上我们就设置成功了，user02有了读写权限 我们还可以为指定的组赋予权限，使用的还是setfacl命令，使用方法：setfacl -m g:组名称:权限 文件名，使用示例如下： 1234567891011121314151617[root@iZm5ehzqow4ijp2ya2g2drZ test]# setfacl -m g:group01:x file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# lltotal 4-rw-rwxr--+ 1 root root 4 Dec 1 14:34 file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# getfacl file1# file: file1# owner: root# group: rootuser::rw-user:user01:r--user:user02:rw-group::r--group:group01:--xmask::rwxother::r-- 如上我们就为新建组group01分配了执行权限 收回权限我们只需要将setfacl命令参数中的 -m 修改为 -x，使用方法：setfacl -x u:用户名称 文件名，使用示例： 12345678910111213# 收回root@iZm5ehzqow4ijp2ya2g2drZ test]# setfacl -x u:user02 file1[root@iZm5ehzqow4ijp2ya2g2drZ test]# getfacl file1# file: file1# owner: root# group: rootuser::rw-user:user01:r--group::r--group:group01:--xmask::r-xother::r-- 注意：收回只能按照用户级别或者用户组级别，而不能收回单个权限编码，例如只收回某个用户的读权限，上述命令收回组权限使用用法与收回用户一样，直降u改为g即可，使用用法：setfacl -x g:用户组名称 文件名 这样我们就可以非常细粒度及灵活的设置文件的具体权限 磁盘分区和挂载这里我们不详细描述磁盘的分区和挂载，因为用的不会太多，即使使用可以网上自行百度一下，这里只列举一下步骤 首先第一种情况是需要分区的磁盘容量小于2T： 首先使用：fdisk 分区设备 命令，然后输入 m 查看帮助命令，这里我们输入 n 选择分区类型，一般我们都是将一个磁盘划分为一个主分区，所以这里再选择输入 p，这是会提示我们输入分区号及分区大小，选择完成后，我们就可以选择输入命令，选择是保存还是不保存删除刚才输入的分区信息，保存使用 w ，保存后，我们就可以使用 fdisk -l 查看我们刚才创建的分区了 注意:上述命令中分区设备示例： /dev/sdd 分好区后，路径是 /dev/sdd1 接下来使用：mkfs. 命令来格式化分区的文件系统类型，使用方法：mkfs.文件系统类型 分区位置，这里我们一般选择ext4文件系统，这样我们就给分区格式化好了磁盘格式 接下来:使用mkdir命令，创建一个目录 接下来：使用mount命令将刚刚分好区的磁盘挂载到此目录上，使用方法：mount 分区路径 目录 这样我们就可以使用mount来查看磁盘的挂载情况，发现我们新添加分好区的磁盘就挂载到了指定的目录上，这样我们操作此目录中的内容，最后就落在了我们挂载的磁盘上 第二种情况磁盘容量大有2T： 这时我们就不能使用fdisk命令来为磁盘分区了，我们需要使用parted命令来操作，同样我们可以对标fdisk来操作，使用help来查看具体使用，当然具体流程是一样的，只不过分区的命令不一样，其他流程及命令都一样 以上我们的操作都是保存在内存中的，如果我们的机器重启，配置就会丢失，如果我们需要固化下来，就需要修改一个配置文件，在此配置文件中进行指定挂载的配置，配置文件的位置：/etc/fstab, 配置说明： 磁盘分区 挂载目录 文件系统格式 default 0 0 示例： 1/dev/sdc1 /mnt/sdc1 ext4 default 0 0 将以上配置信息写入文件，就可以完成永久的挂载配置 创建交换分区swap是在硬盘中开辟的一块区域，之前我们介绍过，它的作用是临时的扩充内存的区域 新添加磁盘挂载为交换分区注意这种方式我们很少用，因为很少用新添加的磁盘作为内存使用。上面我们讲述了如何新添加磁盘然后分区、挂载，现在我们讲述如何新添加磁盘然后分区挂载为交换分区 查看交换分区大小使用free -m命令 首先我们对新添加的硬盘进行分区，命令与上述一样，首先使用：fdisk 分区设备 命令，然后我们输入 n 选择分区类型，然后输入 p 表示将一个磁盘划分为一个主分区，这是会提示我们输入分区号及分区大小，分区编号一般选择 1 ，从1开始，分区大小可以不选择，直接默认使用全部，选择完成后，我们 w 保存配置，这样我们就可以查看我们的分区了 如同使用正常的文件系统一样，我们将磁盘分区为交换分区，也需要对它进行格式化，格式化命令：mkswap 分区位置， 格式化命令使用示例： mkswap /dev/sdd1 格式化完成就需要对此分区进行挂载了，如果是文件系统使用的是mount命令，挂载为内存分区使用的是：swapon 分区位置，挂载命令使用示例：swapon /dev/sdd1 这样我们就对内存分区扩展完成，可以使用 free -m 命令查看了 如果我们想将此新挂载的swap分区关闭掉，使用命令：swapoff 分区位置，使用示例：swapoff /dev/sdd1，这是使用 free -m 命令再次查看，发现就已经被关闭掉了 使用文件挂载为交换分区上述我们是新添加了一块磁盘作为交换分区，但是一般很不常用，一般我们是在已有磁盘开辟一块空间，作为交换分区，挂载方式如下： 首先创建一个文件，文件创建未空洞文件形式，使用命令示例：dd if=/dev/zero bs=4M count=1024 of=/swapfile, 这里我们创建了一个每块为4M。一共1024块的大小的文件 下面我们需要格式化此文件，但是首先我们需要将此文件的权限处理好，负责可能会报关于权限的错误，所以这里首先：chmod 600 /swapfile 然后格式化此文件，使用命令：mkswap /swapfile 打开挂载此文件，使用命令：swapon /swapfile 这样我们就可以使用 free -m 命令来查看了 上述创建挂载方式跟我们普通磁盘文件系统挂载步骤差不多，所以一样都是保存在内存中的，如果我们的机器重启，配置就会丢失，如果我们需要固化下来，就需要修改一个配置文件，在此配置文件中进行指定挂载的配置，配置文件的位置：/etc/fstab 配置说明： 磁盘分区 挂载目录 文件系统格式 default 0 0 写入示例： 1234## 上述磁盘挂载配置/dev/sdc1 swap swap default 0 0## 上述文件挂载配置/swapfile swap swap default 0 0 将以上配置信息写入文件，就可以完成永久的挂载配置","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"内存与磁盘管理","slug":"内存与磁盘管理","permalink":"https://jjw-story.github.io/tags/内存与磁盘管理/"}],"author":"JJW"},{"title":"Elasticsearch-核心技术一","slug":"Elasticsearch-核心技术一","date":"2019-10-20T08:17:09.000Z","updated":"2021-04-11T11:08:15.990Z","comments":true,"path":"2019/10/20/Elasticsearch-核心技术一/","link":"","permalink":"https://jjw-story.github.io/2019/10/20/Elasticsearch-核心技术一/","excerpt":"","text":"ES基本概念和原理分片在分布式系统中，单机无法存储规模巨大的数据，要依靠大规模集群处理和存储这些数据，一般通过增加机器数量来提高系统水平扩展能力。因此，需要将数据分成若干小块分配到各个机器上。然后通过某种路由策略找到某个数据块所在的位置。 ES 将数据副本分为主从两部分，即主分片 primary shard 和副分片 replica shard。主数据作为权威数据，写过程中先写主分片，成功后再写副分片，恢复阶段以主分片为准。 分片（shard 是底层的基本读写单元，分片的目的是分割巨大索引，让读写可以并行操作，由多台机器共同完成。读写请求最终落到某个分片上，分片可以独立执行读写工作。ES 利用分片将数据分发到集群内各处。分片是数据的容器，文档保存在分片内，不会跨分片存储。分片又被分配到集群内的各个节点里。当集群规模扩大或缩小时，ES 会自动在各节点中迁移分片使数据仍然均匀分布在集群里。 ES 索引包含很多分片，每个分片是一个 Lucene 的索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。Lucene 索引又由很多分段组成，每个分段都是一个倒排索引。ES 每次 fresh 都会生成 个新的分段，其中包含若干文档的数据。在每个分段内部，文档的不同字段被单独建立索引。每个字段的值由若干词 Term 组成，Term 是原文本容经过分词器处理和语言处理后的最终结果（例如，去除标点符号和转换为词根）。 索引建立的时候就需要确定好主分片数，在较老的版本中（ 5.x 本之前），主分片数量不可以修改，副分片数量可以随时修改。当前版本ES已经支持一定条件的限制性下，对某个索引的主分片进行拆分（split）或缩小（Shirnk），但是我们仍然需要在一开始就尽量规划好主分片数量：先依据硬件情况定好单个分片容量，然后依据业务场景预估数量和增长量，再除以单个分片容量。 实际应用中，我们不应该向单个索引持续写数据，直到它的分片巨大无比。巨大的索引会在数据老化之后难以删除，以_id为单位删除文档不会立刻释放空间，删除的doc只会在Lucene分段合并时才会真正的删除。即使手动的触发分段合并，仍然会导致较高的IO压力，并且可能因为分段巨大导致在合并过程中磁盘空间不足。 动态更新索引为文档建立索引，会使用倒排索引数据结构，倒排索引一旦被写入文件后就具有不变性，不变形具有很多好处：对文件的访问不需要加锁，读取索引文件时可以被文件系统缓存等。 那么索引如何更新：新增的内容并写到一个新的倒排索引中，查询时每个倒排索引轮流查询，查询完在对结果进行合并。每次内存缓冲的数据被写入文件时，会产生一个新的Lucene分段，每个段都是一个倒排索引，在一个记录元信息的文件中描述了当前Lucene索引都包含哪些分段。 由于分段的不变性，更新删除等操作实际上是将数据标记为删除，记录到单独的位置，这种方式称为标记删除。因此删除部分数据不会释放磁盘空间。 近实时搜索在写操作中，一般会先在内存中缓冲一段数据，再将这些数据写入硬盘，每次写入硬盘的这批数据称为一个分段，如同任何写操作，一般情况下（ direct 方式除外〉，通过操作系统write接口写到磁盘的数据先到达系统缓存（内存）， write 函数返回成功时，数据未必被刷到磁通过手工调用 flush ，或者操作系统通过一定策略将系统缓存刷到磁盘。这种策略大幅提升了写入效率 write 函数返回成功开始，无论数据有没有被刷到磁盘，该数据已经对读取可见。 ES 正是利用这种特性实现了近实时搜索 每秒产生 个新分段，新段先写入文件系统缓存，但稍后再执行 flush 刷盘操作，写操作很快会执行完，一旦写成功，就可以像其他文件一样被打开和读取了。 由于系统先缓冲一段数据才写，且新段不会立即刷入磁盘，这两个过程中如果出现某些意外情况（如主机断电），则会存在丢失数据的风险 通用的做法是记录事务日志 每次对行操作时均记录事务日志，当 ES 启动的时候，重放 translog 中所有在最后一次提交后发生的变更操作。比如 HBase 都有自己的事务日志。 当一个文档写入Lucene后是不能被立即查询到的，Elasticsearch提供了一个refresh操作，会定时地调用lucene的reopen(新版本为openIfChanged)为内存中新写入的数据生成一个新的segment，此时被处理的文档均可以被检索到。refresh操作的时间间隔由refresh_interval参数控制，默认为1s, 当然还可以在写入请求中带上refresh表示写入后立即refresh，另外还可以调用refresh API显式refresh。 段合并ES 中，每秒清空一次写缓冲，将这些数据写入文件，这个过程称为 refresh 每次 refresh 会创建一个新的 Lucene 段。但是分段数太多会带来较大的麻烦，每个段都会消耗文件句柄、内存。每个搜索请求都需要轮流检查每个段，查询完再对结果进行合并；所以段越多，搜索也就越慢。因此需要通过一定的策略将这些较小的段合并为大的段，常用的方案是选择大小相似的分段进行合并。在合并过程中，标记为删除的数据不会写入新分段，当合并过程结束，旧的分段数据被删除，标记删除的数据才从磁盘删除。 用户还可以手动调用_forcemerge API来主动触发merge，以减少集群的segment个数和清理已删除或更新的文档。 如果段文件设置一定上限不再合井，对表中部分数据无法实现真正的物理删除。 数据存储可靠性引入translog当一个文档写入Lucence后是存储在内存中的，即使执行了refresh操作仍然是在文件系统缓存中，如果此时服务器宕机，那么这部分数据将会丢失。为此ES增加了translog，当进行文档写操作时会先将文档写入Lucene，然后写入一份到translog，写入translog是落盘的(如果对可靠性要求不是很高，也可以设置异步落盘，可以提高性能，由配置index.translog.durability和index.translog.sync_interval控制)，这样就可以防止服务器宕机后数据的丢失。由于translog是追加写入，因此性能比较好。与传统的分布式系统不同，这里是先写入Lucene再写入translog，原因是写入Lucene可能会失败，为了减少写入失败回滚的复杂度，因此先写入Lucene。 translog的落盘时机可以配置，index.translog.durability配置项，可选参数有request和async。当配置为request时，每次请求之后都同步提交，当出现硬件故障时，所有有响应的操作都肯定已经同步到了磁盘上。当设置成async时，每经过index.translog.sync_interval时长间隔，才会在后台做一次同步和提交操作。当出现硬件故障时，从最后一次提交之后的所有写入操作都会被丢弃。 flush操作另外每30分钟或当translog达到一定大小(由index.translog.flush_threshold_size控制，默认512mb), ES会触发一次flush操作，此时ES会先执行refresh操作将buffer中的数据生成segment，然后调用lucene的commit方法将所有内存中的segment fsync到磁盘。此时lucene中的数据就完成了持久化，会清空translog中的数据(6.x版本为了实现sequenceIDs,不删除translog)。 索引文档操作注意：一般我们在修改数据的时候，还是尽量使用PUT式的全量替换，而不是使用POST式的部分替换，虽然POST修改只修改我们需要修改的字段，能够节省网络开销，但是其实我们内部的修改实现是和PUT是一样的，都是先删除在插入，而且POST式的修改，ES先把要修改的所有文档的内容找出来，然后将我们POST过来的数据中的部分字段内容替换，然后将原文档删除，插入此新的文档。所以我们发现虽然能节省网络开销，但是多了重新构建数据这一步，对于数据量很大的情况来说，还是会有损失效率，所以尽量使用PUT来修改。注意：如果我们是用的POST这种方式修改数据，我们内部也是有并发控制的，同样使用CAS，如果我们在拼装好新的数据后，发现_version已经不一样了，那么它内部会重新拉去doucment数据，然后重新拼装，然后再次CAS，一共可以重试五次，如果五次都失败了，则会抛弃此数据。 评分机制评分是指对给定查询计算某个文档的分值属性的过程，文档得分是一个描述文档与查询匹配程度的参数。Lucene提供了很多算法用于评分计算，但从最早版本的Lucene发布开始，TF-IDF（词频/逆文档频率）就一直是默认评分算法，在Lucene 6.0 版本以后，默认的评分算法已经换成了BM25。 精确率：获取到的相关文档数站获取到的总文档数（包括相关与不想管的）的比例，用百分数表示。 召回率：获取到的相关记录数占数据库中相关的记录总数的比例，用百分数表示。 TF-IDFIF-IDF是Lucene评级功能的核心，融合了向量空间模型和信息获取的布尔模型。主要理念是：与一个查询词项在整个集合中出现的次数越多，这个词项在一个文档中出现的次数越多，那这个文档就和查询越相关。Lucene也会利用查询规范的布尔逻辑，先用布尔模型来缩小要打分的文档范围。用TF-IDF来为文档打分，还要考虑几个因子，包括： 词频：一个基于词项的因子，用来表示一个词项在某文档中出现了多少次。计算方法是用该此项在文档中出现的次数，除以文档的词项总数。词频越高，文档得分越高。 逆文档频率：一个基于词项的因子，用来告诉评分公式该词项有多罕见。逆文档频率越高，该词项越罕见。评分公式利用该因子来为包含罕见词项的文档加权。它的计算方法是log_e（包含词项t的文档数除以文档总数）。一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。注意IDF的影响，在我们的ES中，如果有多个主分片，如果文档较少，可能会出现各个分片文档分布不均匀，当前包含此搜索关键字的文档占此分片的总文档比例越少，此节点计算出的IDF得分越高，这就会导致同样的标题或文章内容，同样的搜索关键字，搜索出来的这两个文章的得分相差很大。（此问题会在文档较少分布不均匀的情况下出现） 协调因子：基于文档中词项个数的协调因子，一个文档内命中了查询中的词项越多，得分越高。 字段权重：查询期赋予某个字段的权重值。 文档权重：索引期赋予某个字段的权重值。 长度范数：每个字段基于词项个数的归一化因子（在索引期被计算并存储在索引中），一个字段包含的词项数越多，该因子的权重越低，这意味着，Lunece评分公式更喜欢包含更少词项的字段。 查询范数：一个基于查询的归一化因子，等于查询中词项的权重平方和，查询范数是不同查询的得分能互相比较，尽管这种比较通常是困难和不可行的。 BM25BM25也是一种根据相关性来为文档进行打分和评级，它与TF-IDF有许多的共同点，两种算法都用到了词频、逆文档频率和字段长度范化。两种模型都根据某些TD和IDF函数为每个词项算出权重，并把所有的词项的权重值相加，作为这次查询的得分。 BM25与TF-IDF有什么不同饱和点在TF-IDF中由饱和度引起的评分问题：如果你的布尔查询中的N个词项里，某一个词项在某份文档中出现了许多次，那这份文档的分值就会极高，因为它的词项饱和度很弱。如果查询条件是 x 和 y，儿某份文档中有1000个x，0个y，TF-IDF不会考虑y从未出现过，仍然会给它极高的分数。 BM25在这种场景下就表现的好很多，因为它提供了饱和度参数，可以对词项饱和度提供更有力的控制。即使查询条件中的某个词项出现了许多次，它所增加的分数也远比不上另一个词项出现的次数从0变到1。BM25天然喜欢哪些尽量多的查询词项都出现过的文档。 平均文档长度TF-IDF与BM25的另一个显著区别就是BM25也考虑了文档长度的影响。比如：某篇包含了1000个词项的文章中，假如 旅行 这个词只出现过一两次，那它的内容与 旅行 就应该没有太大关系，但如果 旅行 这个词在一篇很短的文章中就出现了两次，那这个文章就与 旅行 肯定有很大关系，TF-IDF在计算与文档长度相关的分数时，处理的很片面，篇幅较长的文档字数自然多，因此词频也会比较高，与词项不太相关，与查询条件也不太相关了，BM25针对这种情况引入了文档长度进行补偿，有些文档的内容涉及范围比较广，因此字数多也合理，从数学公式中可以看到，BM25引入了长度参数、文档长度、平均文档长度等来调节词项因子。 缓存缓存允许我们在内存中保存之前使用过的数据，并根据需要适时重用它们。当然，我们不可能缓存所有的数据，因为数据量总是大于内存容量，另外构建缓存的代价也非常高昂。 节点查询缓存查询缓存用于缓存查询的结果。每个节点上都有一个查询缓存，供节点上的所有分片公用。查询缓存使用的淘汰策略是LRU（最近最少使用）：当缓存满时，最近最少被使用的数据将被淘汰，为新数据腾出空间。 查询缓存有两个参数可以配置，一个是配置所使用的内存大小，默认为10%，或者一个具体值，如512MB。一个是是否启用查询缓存开关，这个参数是针对于索引级的。 分片查询缓存当ES针对一个或多个索引执行查询时，协调节点在接收到查询请求后，会将请求转发给所有相关的数据节点，然后每个节点上的相关分片都会在本地执行查询，并将本地结果返回给协调节点，再由协调节点将这些分片级的结果合并成一个完整的结果集，分片请求缓存负责将每个分片上的结果缓存起来，由此可以快速的响应查询次数最多（通常也是代价最大的）请求。 通过这个缓存，经常使用的汇聚结果（比如网站主页上的内容）就可以被缓存起来，让响应更快，无论是否缓存，得到的汇聚结果都是相同的，不会得到过期数据。如果只有最新的索引上的数据，会经常被更新，那就非常适合使用这个缓存了，旧索引上的结果在缓存中就可以直接得到。 当分片刷新时，或者分片中的数据被更新时，缓存的结果就会自动失效。换句话说，从缓存中得到的结果与不使用缓存得到的结果是相同的。刷新间隔越长，缓存内容的有效期就越长。如果缓存满了，最近最少使用的缓存键将被淘汰。 字段数据缓存字段数据缓存的使用时机是当查询涉及非倒排数据操作时。ES所做的是将相关字段的全部数据加载到内存中，这就是字段数据缓存，这种缓存可以被ES用于聚合和脚本计算，以及基于字段值的排序等场景。当第一次执行非倒排数相关操作时，ES会把所有相关字段的数据加载入内存，默认情况下这些给定字段的数据不会被移除。因此，可以快速用基于文档的方法访问索引文档中给定字段的值。需要注意的是，从硬件资源的角度来看，构建字段数据缓存代价通常很高，因为相关字段的所有数据都要加载到内存中，这需要消耗I/O操作和CPU资源。 可以在集群没饿节点上通过indices.fielddata.cache.size参数控制字段数据缓存。这个参数值是字段数据缓存的最大值，比如节点堆空间的30%，或者如12GB之类的绝对值。默认不设限制。 字段数据还是doc values：在ES中，doc values是字段数据的另一个选择，现在对于每个not_analyzed都是默认启用的。在索引期会进行计算，并按列格式存储在磁盘上。doc values的速度与字段数据缓存不相上下，而且需要的内存还更少。因此从ES 5.x 版本开始，就不要使用字段数据了，直接使用doc values。 doc values就是一个列式存储，存储的是不需要分词的字段原文数据（正排索引），用户聚合和脚本中等使用。 集群内部管理分布式系统的集群方式大致可以分为主从（Master-Slave）模式和无主模式。ES使用主从模式，主从模式可以简化系统设计， Master 作为权权威节点，部分操作仅由 Master 执行，并负责维护集群元信息。缺点是 Maste 节点存在单点故障，需要解决灾备问题，井且集群规模会受限于 Master 节点的管理能力。 集群节点角色主节点(Master node)主节点负责集群层面的相关操作，管理集群变更。通过配置 node.maste: true （默认）使节点具有被选举为 Master 资格。主节点是全局唯一的，将从有资格成为 Master 的节点中进行选举。 主节点也可以作为数据节点，但尽可能做少量的工作。 数据节点(Data node)负责保存数据、执行数据相关操作： CRUD 、搜索、聚合等。数据节点对 CPU 、内存、 I/O要求较高。一般情况下，，数据读写流程只和数据节点交互，不和主节点打交道。 预处理节点(Ingest node)这是从 5.0 版本开始引入的概念。预处理操作允许在索引文档之前，即写入数据之前，通过事先定义好的一系列的 processors （处理器） pipeline （管道〉 ，对数据进行某种转换、富华，processors pipeline 拦截 bulk ｛日 inde 请求 在应用 关操作后将文档传回 index Bulk API。默认情况下， 在所有的节点上启用 ingest ，如果想在某个节点上禁用 ingest ，则可以添加配node ingest false 。 协调节点(Coordinating node)客户端请求可以发送到集群的任何节点，每个节点都知道任意文档所处的位置，然后转发这些请求，收集数据井返回给客户端，处理客户端请求的节点称为协调节点。协调节点将请求转发给保存数据的数据节点。每个数据节点在本地执行请求，并将结果返回协调节点。协调节点收集完数据后，将每个数据节点的结果合井为单个全局结果。对结果收集和排序的过程可能需要很多 PU 和内存资源。 部落节点(Tribe node)tribes （部落）功能允许部落节点在多个集群之间充当联合客户端。它不做主节点，也不做数据节点，仅用于路由请求，本质上是 个智能负载均衡器（从负载均衡器的定义来说，智能和非智能的区别在于是否知道访问的内容存在于哪个节点〉，从 5.0版本开始，这个角色被协调节点取代。 集群健康状态从数据完整性的角度划分，集群健康状态分为： Green ，所有的主分片和副分片都正常运行。 Yellow ，所有的主分片都正常运行，但不是所有的副分片都正常运行 存在单点故障风险。 Red ，有主分片没能正常运行。 每个索引也有上述 种状态，假设丢失了一个副分片，该分片所属的索引和整个集群变为Yellow 状态，其他索引仍为 Green。 集群状态集群状态元数据是全局信息，元数据包括内容路由信息、配置信息等，其中最重要的是内容路由信息，它描述了“哪个分片位于哪个节点”这种信息。 集群状态由主节点负责维护，如果主节点从数据节点接收更新，则将这些更新广播到集群的其他节点，让每个节点上的集群状态保持最新 ES 2.0 版本之后，更新的集群状态信息只发增量内容，并且是被压缩的。 集群扩容当扩容集群、添加节点时，分片会均衡地分配到集群的各个节点，从而对索引和搜索过程进行负载均衡，这些都是系统自动完成的。当扩容集群、添加节点时，分片会均衡地分配到集群的各个节点，从而对索引和搜索过程进行负载均衡，这些都是系统自动完成的。 分配过程中除了 节点间均匀存储，还要保证不把主分片和副分片配到同避免单个节点故障引起数据丢失。分布式系统难免出现故障，当节点异常时，ES 会自动处理节点异常。当主节点异常时，集群会重新选举主节点。当某个主分片异常时， 会将副分片提升为主分片。 主要内部模块ClusterCluster 模块是主节点执行集群管理的封装实现，管理集群状态，维护集群层面的配置信息要功能如下： 管理集群状态，将新生成的集群状态发布到集群所有节点。 调用 llocation 模块执行分片分配，决策哪些分片应该分配到哪个节点。 在集群各节点中直接迁移分片，保持数据平衡。 allocation封装了分片分配相关的功能和策略，包括主分片的分配和副分片的分配，本模块由主节点调用。创建新索引、集群完全重启都需要分片分配的过程。 Discovery发现模块负责发现集群中的节点 ，以及选举主节点。当节点加入或退出集群时，主节点会采取相应的行动。从某种角度来说，发现模块起到类似 ZooKeep町的作用，选主并管理集群拓扑。 gateway负责对收到 Master 广播下来的集群状态（ cluster state ）数据的持久化存储，并在集群完全重启时恢复它们。 Indices索引模块管理全局级的索引设置，不包括索引级的（索引设置分为全局级和每个索引级）。它还封装了索引数据恢复功能 集群启动阶段需要的主分片恢复和副分片恢复就是在这个模块实现的。 HTTPHTTP 模块允许通过 JSON over HTTP 方式访 ES API，HTTP 模块本质上是完全异步的，这意味着没有阻塞线程等待响应，使用异步通信进行 HTTP 的好处是解决了 C10k 问题， （10k 量级的并发连接）。 在部分场景下，可考虑使用 HTTP keepalive 提升性能。注意不要在客户端使用 HTTP chunking。 Transport传输模块用于 群内节点之间的内部通信 节点到另 个节点的每个请求都使用传输模块。如同 HTTP ，传输模块本质上也是完全异步的。传输模块使用 TCP 通信，每个节点都与其他节点维持若干 TCP 长连接，内部节点间的所有通信都是本模块承载的。 EngineEngine 模块封装了对 Lucene 的操作及 translog 调用，它是对一个分片读写操作的最终提供者。ES 使用 Guice 框架进行模块化管理。 Guice 是Google研发的轻量级依赖注入框架 IoC。 模块管理定义好的模块由 Module Builder 类统一管理 ModulesBuilder 是 ES 对 Guice 的封装，内部调用 Guice 接口，主要对外提供两个方法。 add 方法：添加创建好的模块。 createlnjector 方法 调用 Guice.createlnjector 创建并返回 Injector ，后续通过 Injector 取相应 Service 类的实例。 集群启动流程集群启动的整体流程如下图所示： 选举主节点假设有若干节点正在启动，集群启动的第一件事是从己知的活跃机器列表中选择一个作为主节点，选主之后的流程由主节点触发。 ES 的选主算法是基于 Bully 算法的改进，主要思路是对节点 ID 排序，取 ID 值最大的节点作为 Master，每个节点都运行这个流程，选主的目的是确定唯一的主节点，初学者可能认为选举出的主节点应该持有最新的元数据信息，实际上这个问题在实现上被分解为两步：先确定唯一的、 大家公认的主节点，再想办法把最新的机器元数据复制到选举出节点上。 基于节点 ID 排序的简单选举算法有三个附加约定条件： 选人数需要过半，达到 quorum （多数）后就选出了临时的主。 得票数需过半。某节点被选为主节点，必须判断加入它的节点数过半，才确认 Master身份。解决第一个问题。 当探测到节点离开事件时，必须判断当前节点数是否过半。如果达不 quorum ，则放弃 Master 身份， 重新加入集群。防止集群脑裂。 选举集群元信息被选出的 Master 和集群元信息的新旧程度没有关系。因此它的第一个任务是选举元信息，让各节点把各自存储的元信息发过来 ，根据版本号确定最新的元信息，然后把这个信息广播下去，这样集群的所有节点都有了最新的元信息。 集群元信息的选举包括两个级别：集群级和索引级，不包含哪个 shard 存于哪个节点这种信息 。这种信息以节点磁盘存储的为准， 需要上报。因为读写流程是不经过Master的， Master不知道各 shard 副本直接的数据差异。 HDFS 也有类似的机制， block 信息依赖于DataNode 的上报。为了集群一致性，参与选举的元信息数量需要过半， Master 发布集群状态成功的规则也是等待发布成功的节点数过半。集群元信息选举完毕后， Master 发布首次集群状态，然后开始选举 shard 级元信息。 allocation 过程选举 shard 级元信息，构建内容路由表，是在 allocation 模块完成的。在初始阶段，所有的 shard 都处于 UNASSIGNED （未分配）状态。 ES中通过分配过程决定哪个分片位于哪个节点重构内容路由表。此时，首先要做的是分配主分片。 选主分片所有的分配工作都是 Master 来做的，此时，Master 不知道主分片在哪，它向集群的所有节点询问：大家把［website] [OJ 分片的元信息发给我。然后， Master 等待所有的请求返回，正常情况下它就有了这个 shard 的信息，然后根据某种策略选一个分片作为主分片。是不是效率有些低？这种询问量＝shard 数×节点数。 所以说我们好控制 rd 的总规模别太大。 ES 5.x 开始实施一种新的策略：给 shard 都设置一个 UUID ，然后在集群级的元信息中记录哪个shard 是最新的，因为 ES 是先写主分片，再由主分片节点转发请求去写副分片 ，所以主分片所在节点肯定是最新的，如果它转发失败了，则要求 Master 删除那个节点。 所以，从 ES 5.x 开始，主分片选举过程是通过集群级元信息中记录的“最新主分片的列表”来确定主分片的：汇报信息中存在，并且这个列表中也存在。 选副分片主分片选举完成后，从上一个汇总过程的 shard 信息中选举一个副本作为副分片。如果汇总信息不存在，则分配一个全新的副本。 index recovery（索引恢复）分片分配完成后进入recovery流程，主分片的恢复不会等待其副本分片分配成功才开始恢复。它们是独立的流程，只是副分片的回复需要主分片恢复完毕才开始。为什么需要恢复，对于主分片，可能有一些数据没来得及刷盘；对于副分片，一是没有刷盘，二是主分片写完了，副分片还没有来得及写，主副分片数据不一致。 主分片恢复：由于每次写操作都会记录事务日志（ translog ）， 事务日志中记录了哪种操作，以及相关的数据。因此将最后一次提交（ Lucene 的一次提交就是一次 fsync 盘的过程）之后的 translog 中进行重放，建立 Lucene 索引，如此完成主分片的 recovery。 副分片恢复：副分片需要恢复成与主分片一致，同时，恢复期间允许新的索引操作。在目前的 6.x 版本中，恢复分成两阶段执行： phase1：在主分片所在节点 获取 translog 保留锁，从获取保留锁开始，会保留 translog 不受其刷盘清空的影响 。然后调 Lucene 接口把 shard 做快照，这是已经刷磁盘中的分片数据。把这些 shard 数据复制到副本节点。在 phase1 完毕前，会向副分片节点发送告知对方启动 engine ，在 phase2 开始之前，副分片就可以正常处理写请求了。 phase2：对 translog 做快照，这个快照里包含从 phasel 开始，到执行 translog 快照期间的新增索引 。将这些 translog 发送到副分片所在节点进行重放。 分片数据完整性：如何做到副分片不丢数据？第二阶段的 translog 快照包括第一阶段所有的新增操作，那么第一阶段执行期间如果发生 Lucene commit （将文件系统写入缓冲中的数据刷盘，并清空 translog ），清除的translog怎么办？。从 6.0 版本开始 translog.view 被移除，引入了TranslogDeletionPolicy 的概念，它将 translog 做一个快照来保持 translog 不被清理，这样实现了在第一阶段允许 Lucene commit。 数据一致性：恢复期间没有任何写阻塞过程，在副分片节点，重放 translog 时， phase1 和 phase2 间的写操作与phase2重放操做间的时序错误和冲突，通过写流程中进行异常处理，对比版本号来过滤掉过期操作。这样，时序上存在错误的操作被忽略，对于特定的 doc ，只有最新一次操作生效，保证了主副分片一致。 第一阶段尤其漫长，因为它需要从主分片拉取全量的数据，6.x 中，对第一阶段再次优化：标记每个操作。在正常的写操作中，每次写入成功的操作都分配一个序号，通过对比序号就可以计算出差异范围。在实现方式上，添加了 global checkpoint local checkpoint，主分片负责维护 global checkpoint ，代表所有分片都己写入这个序号的位置， local checkpoint 表当前分片己写入成功的最新位置，恢复时通过对比两个序列号，计算出缺失的数据范围，然后通过translog 重放这部分数据，同时 translog 会为此保留更长的时间。 因此，有两个机会可 以跳过副分片恢复的 phase1：基于 SequenceNumber ，从主分片节点的 translog 恢复数据；主副两分片有相同的 syncid 且 doc 数相同，可以跳过 phase1。 选主流程Discovery 模块负责发现集群中的节点，以及选择主节点。ES 支持多种不同 Discovery 类型选择，称为 Zen Discove ，其他的包括公有云平台亚马逊的 EC2 、谷歌的 GCE 等。本章讨论内置的 Zen Discovery 实现。 Zen Discovery 封装了节点发现（ Ping ）、选主等实现过程。 为什么使用主从模式除主从（ Leader/Fo llower ）模式外，另一种选择是分布式哈希表（ DHT ），可以支持每小时数千个节点的离开和加入，其可以在不了解底层网络拓扑的异构网络中工作， 查询响应时间大约为 10 跳（中转次数〉，例如， Cassandra 就使用这种方案 但是在相对稳定的对等网络中，主从模式会更好。ES 典型场景中的另一个简化是集群中没有那么多节点 通常，节点的数 远远 于单个节点能够维护的连接数，并且网络环境不必经常处理节点 的加入和离开 这就是为什么主从模式更适合 ES。 选举算法Bully 算法Leader 选举的基本算法之 。它假定所有节点都有一个唯一ID ，使用该 ID 对节点进行排序。任何时候的当前 Leader 都是参与集群的最高 ID 节点。该算法的优点是易于实现。但是，当拥有最大 ID 的节点处于不稳定状态的场景下会有问题 例如，Master 负载过重而假死，集群拥有第 ID 的节点被选为新主，这时原来的 Master 恢复，再次被选为新主，然后又假死……。 ES 通过推迟选举，直到当前的 Master 失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过“法定得票人数过半”解决脑裂问题。 Paxos 算法Paxos 非常强大，尤其在什么时机，以及如何进行选举方面 灵活性比简单的 Bully 算法有很大的优势，因为在现实生活中，存在比网络连接异常更多的故障模式。但 Paxos 实现起来非常复杂。 流程概述ZenDiscovery 的选主过程如下 每个节点计算最小的己知节点 ID ，该节点为临时 Master 。向该节点发送领导投票。 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者角色，开始发布集群状态。 所有节点都会参与选举，并参与投票，但是，只有有资格成为 Master 的节点（ node.mastertrue ）的投票才有效。 获得多少选票可以赢得选举胜利，就是所谓法定人数中， 法定大小是一个可配参数。配置项 discovery.zen_minimum_master_nodes 。为了避免脑裂最小值应该是有 Master资格的节点数 n/2+ 1。 流程分析整体流程可以概括为：选举临时 Master ，如果本节点当选，则等待确立 Master ，如果其节点当选，则尝试加入集群，然后启动节点失效探测器。 选举临时 Master 选举过程的实现位于 ZenDiscovey#findMaster 该函数查找当前集群的活跃 Maste 或者从候选者中选择新的 Master 如果选主成功，则返回选定的 Maste 否则返回空。为什么是临时 Master ？因为还需要等待下个步骤，该节点的得票数足够时，才确立为真正的 Master。这里面会筛选不具有Master资格的节点，里面会有投票机制。 投票与得票的实现 在ES中，发送投票就是发送加入集群 （JoinRequest）请求。得票就是申请加入求的数量。 确立 Master 或加入集群 选举出 的临时 Master 有两种情况 临时 Master 是本节点或非本节点。为此单独处理。现在准备向其发送投票。 如果临时 Mast 是本节点： 等待足够多的具备 Master 资格的节点入本节点（投票达到法定人数），以完成选举。 超时（默认为 30 可配置）后还没有满足数量 join 请求，则选举失败，需要进行新一轮边举。 成功后发布新的clusterState。 如果其他节点被选为 Master: 不再接受其他节点的 join 请求。 向 Master 发送加入请求，并等待回复。超时时间默认为1分钟（可配置），如果遇到异常，则默认重试3次（可配置）。 最终当选的 Master 会先发布集群状态，才确认客户 join 请求，因此， joinElectedMaster返回代表收到了 join 请求的确认，并且已经收到了集群状态。本步骤检查收到的集群状态中的Master节点如果为空，或者当选的 Master 是之前选择的节点，则重新选举。 节点失效检测到此为止，选主流程己执行完毕，Master 身份己确认，非 Master 节点己加入集群。节点失效检测监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主〉。在此我们需要启动两种失效探测器： Master 节点，启动 NodesFaultDetection，简称 NodesFD 。定期探测加入集群的节点是否活跃。 在非 Master 点启动 MasterFaultDetection，简称 MasterFD 定期探测 Master 节点是否活跃。 NodesFaultDetection 和 MasterFaultDetection 是通过定期（默认为1秒）发送的 ping 请求探测节点是否正常的，当失败达到一定次数（默认为3次），或者收到来自底层连接模块的节点离线通知时，开始处理节点离开事件。 NodesFaultDetection：检查当前集群总节点数是否达到法定节点数（过半），如果不足，则会放弃 Master 身份，重新加入集群。主节点在探测到节点离线的事件处理中，如果发现当前集群节点数量不足法定人数，则放 Master 身份，从而避免产生双主。 MasterFaultDetection：探测 Master 离线的处理很简单，重新加入集群。本质上就是该节点重新执行一遍选主的流程。 数据模型PacificA 算法ES 的数据副本模型基于主从模式（或称主备模式， HDFS Cassandra 为对等模式〕，在实现过程中参考了微软的 PacificA 算法（借鉴了其中部分思想，井非完全按照这个模型实现）。我们先看一下 PacificA 算法的几个特点： 设计了一个通用的、抽象的框架，而不是具体的、特定的算法。模型的正确性容易验证。 配置管理和数据副本分离，Paxos 负责管理配置，数据副本策略采取主从模式。 将错误检测和配置更新放在数据副本的交互里实现，去中心化。 该算法涉及的几个术语如下： Replica Group：互为副本的数据集合称为副本组。其中只有一个副本是主数据Primary， 其他为从数据（Secondary）。 Configuration：配置信息 中描述了一个副本组都有哪些副本，Primary 是谁，以及它们位于哪个节点。 Configuration Version：配置信息的版本，每次发生变更时递增。 Serial Number：代表每个写操作 的顺序， 每次写操作 递增 ，简称 SN 。每个主副本维护自己的递增 SN。 Prepared List：写操作的准备序列。存储来自外部请求的列表，将请求按照 SN 排序，向列表中插入的序列号必须大于列表中中最大的 SN 。每个副本上有自己的 Prepared List。 Committed List：写操作的提交序列。 数据副本策略数据写入的流程如下： 写请求进入主副本节点，节点为该操作分配 SN ，使用该 SN 创建 UpdateRequest 结构。然后将该 UpdateRequest 插入 自己的 prepare list。 主副本节点将携带 UpdateRequest 发往从副本节点，从节点收到后同样插入prepare list，完成后给主副本节点回复 ACK。 一旦主副本节点收到所有从副本节点的响应，确定该数据已经被正确写入所有的从副本节点，此时认为可以提交了，将此 UpdateRequest 放入 committed list, committed list 向前移动。 主副本节点回复客户端更新完成。对每一个 Prepare 消息，主副本节点向从副本节点发送一个 commit 通知，告诉它们自己的 committed point 位置，从副本节点收到通知后根据指示移动 committed point 到相同的位置。 因为主副本只有在所有从副本将请求添加 prepared list 之后才可以通过移动 committed point 方式将该请求插入 committed list 中，因此主副本的 commtted list 是任何一个从副本的prepared list 的前缀（或者称为子集）。例如，从副本 prpared list SN 为 1、2、3、4，主副本committed point SN 定不会大于 4，例如 1、2、3。同时，因为一个从副本只有在主副本将一个请求添加进 committed list 后才会把同样的请求添加进 committed list 中，因此一个从副本上的 committe list 是主副本上 committed list 的前缀，此不变式称为 Commit Invariant。 配置管理全局的配置管理器负责管理所有副本组的配置。节点可以向管理器提出添加／移除副本的请求，每次请求都需要附带当前配置版本号，只有这个版本号和管理器记录的版本号一致才会被执行，如果请求成功，则这个新配置会被赋予新的版本号。 错误检测分布式系统经常存在网络分区、节点离线等异常。全局的配置管理器维护权威配置信息，但其他各节点上的配置信息不一定同步，我们必须处理旧的主副本和新的主副本同时存在的情况 一一 旧的主副本可能没有意识到重新分配了已个新的主副本，从而违反了强一致虚性。 PacificA使用了租约 Clease 机制来解决这个问题。 主副本定期向其他从副本获取租约。这个过程中可能产生两种情况： 如果主副本节点在一定时间内（ lease period ）未收到从副本节点的租约回复，则主副本节点认为从副本节点异常，向配置管理器汇报，将该异常从副本从副本组中移除，同时，它也将自己降级，不再作为主副本节点。 如果从副本节点在一定时间内（ grace period ）未收到主副本节点的租约请求，则认为主副本异常，向配置管理器汇报，将主副本从副本组中移除，同时将自己提升为新的主。如果存在多个从副本，则哪个从副本先执行成功，哪个从副本就被提升为新主。 PacificA 算法的这些概念对应在 ES 中： Master 负责维护索引元信息，类似配置管理器维护配置信息。 集群状态中的 routing table 存储了所有索引、索引有哪些 shard、各自的主分片，以及位于哪个节点等信息，类似副本组。 SequenceNumber和Checkpoint 类似 PacificA 算法中的 Serial Number 和 Committed Point。 ES 的数据副本模型ES 中的每个索引都会被拆分为多个分片，并且每个分片都有多个副本。这些副本称为replication group （副本组，与 PacificA 中的副本组概念一致），并且在删除或添加文档的时候，各个副本必须同步。否则，从不同副本中读取的数据会不一致。我们把保持分片副本之间的同步，以及从中读取的过程称为数据副本模型(data replication model)。 ES 的数据副本模型基于主备模式（ primary backup model 主分片是所有索引操作的入口，它负责验证索引操作是否有效，一旦主分片接受一个索引操作，主分片的副分片也会接受该操作。 基本写入模型每个索引操作首先会使用 routing 参数解析到副本组，通常基于文档 ID 。一旦确定副本组，就会内部转发该操作到分片组的主分片中，主分片负责验证操作和转发它到其它副分片。ES维护一个可以接收该操作的分片的副本列表。这个列表叫作同步副本列表（ in-sync copies ）， 并由Master 节点维护。正如它的名字，这个“好”分片副本列表中的分片，都会保证己成功处理所有的索引和删除操作，并给用户返回 ACK。主分片负责维护不变性（各个副本保持一致〉，因此必须复制这些操作到这个列表中的每个副本。 写入流程遵循以下基本流程： 请求到达协调节点，协调节点先验证操作，如果有错就拒绝该操作。然后根据当前集群状态，请求被路由到主分片所在节点。 该操做在主分片上本地执行，例如，索引、更新或删除文挡，会验证字段的内容，如果未通过就拒绝操作（例如，字段串的长度超出 Lucene 定义的长度）。 操作成功执行后，转发该操作到当前 in-sync 副本组的所有副分片。如果有多个副分片，会并行转发。 一旦所有的副分片成功执行操作并回复主分片，主分片会把请求执行成功的信息返回给协调节点，协调节点返回给客户端。 写故障处理写入期间可能会发生很多错误－一硬盘损坏、节点离线，或者某些配置错误，这些错误都可能导致无法在副分片上执行某个操作，虽然这比较少见，但是主分片必须汇报这些错误信息。 对于主分片自身错误的情况，它所在的节点会发送一个消息到 Master 节点 这个索引操作会等待（默认为最多一分钟） Master 节点提升一个副分片为主分片，这个操作会被转发给新的主分片。注意：Master 同样会监控节点的健康，井且可能会主动阵级主分片，这通常发生在主分片所在的节点离线的时候。 在主分片上执行的操作成功后，该主分片必须处理在副分片上潜在发生的错误。错误发生的原因可能是在副分片上执行操作时发生的错误，也可能是因为网络阻塞，导致主分片无法转发操作到副分片，或者副分片无法返回结果给主分片。这些错误都会导致相同的结果 in-sync replica set 中的一个分片丢失一个即将要向用户确认的操作，为了避免出现不一致，主分片会发送一条消息到 Master 节点，要求它把有问题的分片从 in-sync replica set 中移除 。一旦Master认移除了该分片，主分片就会确认这次操作 注意， Master 会指导另一个节点建立新的副本分片，以便把系统恢复成健康状态。 基本读取模型基本流程如下： 把读请求转发到相关分片。注意，因为大多数搜索都会发送到一个或多个索引，通常需要从多个分片中读取，每个分片都保存这些数据的一部分。 从副本组中选择一个相关分片的活跃副本，它可以是主分片或副分片。默认情况下，ES 会简单地循环遍历这些分片。 发送分片级的读请求到被选中的副本。 合并结果井给客户端返回响应。注意，针对通过 ID 查找的 get 请求，会跳过这个步骤，因为只有一个相关的分片。 读故障处理当分片不能响应一个读请求时，协调节点会从副本组中选择另一个副本，将请求转发给没有可用的分片副本会导致重复的错误。在某些情况下，例如，_search，ES会倾向于尽早响应，即使只有部分结果，也不等待问题被解决（可以在响应结果的 _shards 字段中检查本次结果是完整的还是部分的）。 引申的问题基本流程决定了 ES 系统在读和写时的表现 此外，由于读写可以同时执行，所以这两个基本流程互相有些影响。这有一些固定的含义。 高效读取：在正常操作下，读操作在相关副本组中只执行一次。只有在出错的时候才会在同一个分片的不同副本中执行多次。 在写操作返回应答之前读取:主分片首先在本地进行索引，然后转发请求，由于主分片己经写成功，因此在并行的读请求中，有可能在写请求返回成功之前就可以读取更新的内容。 只有单个分片可能降低索引速度:因为每次操作时主分片会等待所有在 in-sync 列表中的副本，所以单个缓慢的副本可能降低整个副本组的写速度，当然，单个缓慢的分片也会降低读取速度。 脏读：从一个被隔离的主分片进行读取，可能读取没有经过确认的写操作。这是因为只有主分片向副分片转发请求，或者向主节点发送请求的时候才会被隔离，此时数据已经在主分片写成功可以被读取到。ES通过定期（默认为1秒） ping 主节点来降低这种风险，如果没有己知的主节点，则拒绝索引操作。 Allocation IDsES 5.x 版本开始引入 Allocation IDs 的概念，用于主分片选举策略。每个分片有自己唯Allocation ID，同时集群元信息中有一个列表，记录了哪些分片拥有最新数据。如果主分片发生错误永久不可用，如果将一个旧数据的分片作为主分片，它将作为最终副本，从而导致这个副本之后的数据将会丢弃。下面我们介绍如何追踪到那个可以安全地被选为主分片的副本，称之为同步（in -sync）分片副本。 安全地分配主分片每个节点都会通过检查集群状态来判断某个分片是否可用。如果一个分片被指定为主分片则这个节点只需要加载本地分片副本，使之可以用于搜索即可。如果一个分片被分配为副分片，则节点首先需要从主分片所在节点复制差异数据。当集群中可用副分片不足时（在索引设置中指定(index.number_of_replicas)，主节点也可以将副分片分配到不含任何此分片副本的节点，从而指示这些节点创建主分片的完整副本。在创建新索引时，主节点在选择哪个节点作为主分片方面有很大的灵活性，会将集群均衡和其他约束（如分配感知及过滤器）考虑在内。。为了确保安全，主节点必须确保被选为主分片的副本含有最新数据,为此 ES 使用 Allocation IDs 的概念，这是区分不同分片的唯一标识(UUIDS）。 Allocation IDs 由主节点在分片分配时指定，并由数据节点存储在磁盘中，紧邻实际的数据分片。主节点负责追踪包含最新数据副本的子集。这些副本集合称为同步分片标识（in-sync Acallocation IDs），存储于集群状态中。集群状态存在于集群的主节点和所有数据节点。对集群状态的更改由 zen discovery 模块实现一致性支持。它确保集群中有共同的理解，即哪些分片副本被认为是同步的（ in- sync ），隐式地将那些不在同步集合中的分片副本标记为陈旧。 也就是说， Allocation IDs 存储在 shard 级元信息中，每个 shard 都有自己唯 Allocation ID, 同时集群级元信息中记录了一个被认为是最新 shard Allocation ID 集合，这个集合称为 in-syncallocation IDs。 Sequence IDsES 6.0 版本开始引入了 Sequence IDs 概念，使用唯一ID来标记每个写操作，通过这ID我们有了索引操作的总排序。写操作先到达主分片，主分片写完后转发到副分片，在转发到副分片之前，增加一个计数器，为每个操作分配一个序列号是很简单的。但是，由于节点离线随时可能发生，例如，网络分区等，主分片可能被其他副分片取代，仅仅由主分片分配一个序列号无法保证全局唯一性和单调性，因此，我们把当前主分片做一个标记，放到每个操作中，这就是 Primary Terms 这样，来自旧的主分片的迟到的操作就可以被检测到然后拒绝（虽然 Allocation IDs 可以让主分片分配在拥有最新数据的分片上，但仍然可能存在某些情况下主分片上的数据并非最新，例如，手工分配主分片到有旧数据的副本） Primary Terms 和 Sequence Numbers Primary Terms：由主节点分配给每个主分片，每次主分片发生变化时递增。然后持久化到集群状态中，从而表示集群主分片所处的一个版本。有了 Primary Terms，操作历史中的任何冲突都可以通过查看操作的 Primary Terms 来解决，新的 Terms 优先于旧 Terms，拒绝过时的操作，避免混乱的情况。 Sequence Numbers：标记发生在某个分片上的写操作。由主分片分配，只对写操作分配。假设索引 website 有2个主分片和1个副分片，当分片 website[O]的序列号增加到5时，它的主分片离线，副分片被提升为新的主分片，对于后续写操作，序列号从6开始递增。分片 website[1]有自己独立的序列号计数器。Sequence Numbers 使我们能够理解发生在主分片节点上的索引操作的特定顺序。 本地及全局检查点有了 Primary Terms 和 Sequence Numbers，我们就有了在理论上能够检测出分片之间差异并在主分片失效时，重新对齐它们的工具。旧主分片就可以恢复为与拥有更高 Primary Terms 值的新主分片一致：从旧主分片中删除新主分片操作历史中不存在的操作，并将缺少的操作引到旧主分片。 遗憾的是，当同时为每秒成百上千的事件做索引时，比较数百万个操作的历史是不切实际存储成本非常昂贵，直接进行比较的计算工作量太大。为了解决这个问题， ES 维护了名为“全局检查点”（ global checkpoint ）的安全标记。 全局检查点是所有活跃分片历史都己对齐的序列号，换句话说，所有低于全局检查点的操作都保证己被所有活跃的分片处理完毕。这意味着，当主分片失效时，我们只需要比较新主分片与其他副分片之间的最后一个全局检查点之后的操作即可。当旧主分片恢复时，我们使用它知道的全局检查点，与新主分片进行比较。这样，我们只有小部分操作需要比较，不用比较全部。 主分片负责推进全局检查点，它通过跟踪在副分片上完成的操做来实现。一旦它检测到所有副分片已经超出给定序列号，它将相应地更新全局检查点。副分片不会跟踪所有操作，而是维护一个类似全局检查点局部变量，称为本地检查点。本地检查点是一个序列号，所有序列号低于它的操作都己在该分片上处理（ Lucene translog 写成功 ，不一定刷盘）完毕。当副分片确认（ ACK 写操作到主分片节点 ，它也会更新本地检查点。使用本地检查点，主分片节点能够更新全局检查点，然后在下一次索引操作时将其发送到所有分片副本。 用于快速恢复（Recovery)当 ES 恢复一个分片时，需要保证恢复之后与主分片一致。对于冷数据来说，synced flush 可以快速验证副分片与主分片是否相同，但对于热数据来说，恢复过程需要从主分片复制整个Lucene分段，如果分段很大，则是非常耗时的操作。 现在我们使用副本所知道的最后一个全局检查点，重放来自主分片事务日志（translog）中的相关更改。也就是说，现在可以计算出待恢复分片与主分片数据的差异范围，因此避免复制整个分片。同时，我们多保留一些事务日志（默认为 512MB, 12 小时），直到“太大”或“太老”。如果不能从事务日志恢复，则使用旧的恢复模式。 _version每个文档都有一个版本号（＿version），当文档被修改时版本号递增。ES 使用这个_version来确保变更以正确顺序执行.如果旧版本的文档在新版本之后到达，则它可以被简单地忽略。例如，索引 recovery 阶段就利用了这个特性。版本号由主分片生成，在将请求转发给副本片时将携带此版本号。 版本号的另一个作用是实现乐观锁，如同其他数据库的乐观锁 样。我们在写请求中指定文档的版本号，如果文档的当前版本与请求中指定的版本号不同，则请求会失败。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://jjw-story.github.io/tags/Elasticsearch/"},{"name":"核心技术一","slug":"核心技术一","permalink":"https://jjw-story.github.io/tags/核心技术一/"}],"author":"JJW"},{"title":"进程管理","slug":"进程管理","date":"2019-10-06T08:30:13.000Z","updated":"2019-11-02T09:51:14.163Z","comments":true,"path":"2019/10/06/进程管理/","link":"","permalink":"https://jjw-story.github.io/2019/10/06/进程管理/","excerpt":"","text":"进程管理进程可以理解为程序正在运行的过程，管理的内容主要包括程序什么时候启动，程序运行的整个生命周期需要多少资源，包括需要多少内存资源，运行是需要多少CPU资源，运行结束是需要自己结束还是被其他程序结束的，还有程序运行时我们需要让他结束应该怎么让它结束等等 进程的概念及进程查看进程是运行中的程序，从程序开始运行到终止的整个生命周期是可管理的 C程序的启动时从main函数开始的，包括Java等，终止的方式并不唯一，分为正常终止和异常终止： 正常终止也分为从main返回，调用exit等方式 异常终止分为调用abort、接收信号等。当程序获取不到资源比如内存、CPU资源等，会调用abort函数来终止程序 ps命令使用方法：ps [选项] 单独使用ps命令，它只能查看到当前终端既当前shell下面能够查看到的进程状态，并不是真的只有这些进程在运行，如下只查看到两条: 1234[root@iZm5ehzqow4ijp2ya2g2drZ etc]# ps PID TTY TIME CMD27267 pts/0 00:00:00 bash27342 pts/0 00:00:00 ps PID：表示进程在系统中运行的唯一表示，注意进程的名称是可以重复的，但是PID不能重复 TTY：表示执行当前进程的终端，那么当前执行是一个虚拟终端，所以叫 pts/0 TIME：程序的运行时间，此项不具有参考价值，可以不用关注 CMD：命令 ps选项说明 -e 查看所有的进程，这样我们查出来的会非常多，我们可以通过 more 命令来进行分页查看，具体使用为：ps -e | more，这里主要是通过管道符，将管道符左边的输出传递给more，示例如下： 123456789root@iZm5ehzqow4ijp2ya2g2drZ etc]# ps -e | more PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:00 kthreadd 3 ? 00:00:00 migration/0 4 ? 00:00:01 ksoftirqd/0 5 ? 00:00:00 stopper/0 6 ? 00:00:02 watchdog/0 7 ? 00:01:35 events/0 注意我们看到的1号进程，此进程在CentOS6中叫init，在CentOS7中叫systemd -f 表示显示全格式，既对进程的信息显示更加完善，一般是结合-e选项来使用，ps -ef | more 123456UID PID PPID C STIME TTY TIME CMDroot 1 0 0 Sep14 ? 00:00:00 /sbin/initroot 2 0 0 Sep14 ? 00:00:00 [kthreadd]root 3 2 0 Sep14 ? 00:00:00 [migration/0]root 4 2 0 Sep14 ? 00:00:01 [ksoftirqd/0]root 5 2 0 Sep14 ? 00:00:00 [stopper/0] UID表示此进程是哪一个用户来启动的，但是注意，进程启动的用户是可以修改的，所以它的专业名称叫有效用户ID PPID是父进程，因为我们的程序在启动时都要集成它的父进程的一些信息下来，Linux的第一个进程是1号进程，既init进程 -L 查看更详细的进程情况，既可以查看到当前进程中包含了多少个线程，通过此选项我们可以查看到当前进程运行缓慢是不是因为并发太大及线程太多导致的，一般使用也是结合着-ef，具体：ps -eLf | more 123456UID PID PPID LWP C NLWP STIME TTY TIME CMDroot 1 0 1 0 1 Sep14 ? 00:00:00 /sbin/initroot 2 0 2 0 1 Sep14 ? 00:00:00 [kthreadd]root 3 2 3 0 1 Sep14 ? 00:00:00 [migration/0]root 4 2 4 0 1 Sep14 ? 00:00:01 [ksoftirqd/0]root 5 2 5 0 1 Sep14 ? 00:00:00 [stopper/0] LWP表示的既是线程数量 pstree命令pstree命令是将进程用树形表示出来，上述ps命令中，我们能查看到进程的父进程，在使用此命令时，我们可以看到一个树形结构的进程图，方便我们查看进程的依属关系，示例如下： 123456789root@iZm5ehzqow4ijp2ya2g2drZ etc]# pstree | moreinit-+-AliYunDun---17*[&#123;AliYunDun&#125;] |-AliYunDunUpdate---3*[&#123;AliYunDunUpdat&#125;] |-agetty |-aliyun-service---2*[&#123;aliyun-service&#125;] |-auditd---&#123;auditd&#125; |-crond |-dhclient |-java---28*[&#123;java&#125;] top命令top命令能显示系统的信息及进程信息，类似于Windows的任务管理器，也是一个日常非常实用的命令，可以不带参数，使用具体如下： 12345678910111213[root@iZm5ehzqow4ijp2ya2g2drZ etc]# toptop - 17:38:41 up 22 days, 7:54, 2 users, load average: 0.00, 0.00, 0.00Tasks: 78 total, 1 running, 77 sleeping, 0 stopped, 0 zombieCpu(s): 0.3%us, 0.3%sy, 0.0%ni, 99.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1019980k total, 732792k used, 287188k free, 138988k buffersSwap: 0k total, 0k used, 0k free, 230192k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1407 root 10 -10 121m 11m 9176 S 0.3 1.2 94:59.64 AliYunDun 27417 root 20 0 15012 1292 1004 R 0.3 0.1 0:00.01 top 1 root 20 0 19228 1508 1232 S 0.0 0.1 0:00.72 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root RT 0 0 0 0 S 0.0 0.0 0:00.00 migration/0 说明： top up 22 days： 当前系统上一次开机到现在运行的时长 users：当前系统有两个用户已经登录了 load average: 0.00, 0.00, 0.00： 平均负载，全都是0.00表示空负载，数值越大负载越高，都是1表示满负载运行，平均负载是一分钟进行一次汇总的结果，第一个数字是1分钟的数据，中间数据时5分钟的数据，最后一个是15分钟的统计数据，此数据是查看系统繁忙程度的指标数据 Tasks: 78 tota：系统中有多少个任务正在运行，这里系统将进程表示为任务 1 running, 77 sleeping, 0 stopped, 0 zombie：一个进程在运行中，77个在休眠状态中。。 Cpu(s): 0.3%us： 0.3%的资源用于用户状态的计算 0.3%sy： 进程的状态交互占用的资源 99.3%id：空闲的CPU资源 0.0%wa：磁盘IO占用的资源 Cpu(s)是说统计的是一个整体情况，如果我们是多核的，可以按下数字键1来分别查看每一个CPU的运行状况，再次按下数字1即可恢复成整体模式 123456789101112top - 17:54:45 up 1 min, 0 users, load average: 0.52, 0.58, 0.59Tasks: 4 total, 1 running, 3 sleeping, 0 stopped, 0 zombie%Cpu0 : 5.4 us, 8.0 sy, 0.0 ni, 84.0 id, 0.0 wa, 2.6 hi, 0.0 si, 0.0 st%Cpu1 : 2.0 us, 3.0 sy, 0.0 ni, 95.1 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 5.6 us, 10.8 sy, 0.0 ni, 83.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 3.0 us, 3.0 sy, 0.0 ni, 94.1 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu4 : 4.6 us, 3.0 sy, 0.0 ni, 92.4 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu5 : 2.3 us, 0.3 sy, 0.0 ni, 97.4 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu6 : 1.6 us, 2.0 sy, 0.0 ni, 96.4 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu7 : 1.6 us, 14.8 sy, 0.0 ni, 83.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 16670136 total, 9322736 free, 7118048 used, 229352 buff/cacheKiB Swap: 15518328 total, 15430616 free, 87712 used. 9418356 avail Mem Mem： 1019980k total：总共有多少内存 732792k used：已经使用了的内存 287188k free：空闲的内存 138988k buffers：读写缓存使用的内存 Swap：交换分区，当内存不够用的时候，需要使用交换分区的内存 0k used：这里只需要关注此选项即可，因为在内存占用比较大的时候才会与系统进行交换分区内存，这里有占用说明程序占用内存很大了 进程信息： 这里的进程信息我们可以看到还显示了每个进程当前的CPU占用率，以及内存占用情况，默认是3秒已刷新，我们还可以修改刷新时间，使用按键s，即可输入你需要的刷新间隔时间，然后回车，注意单位是秒，这样既可以按照你输入的时间间隔来刷新 进程信息中PR表示进程的系统优先级，NI表示Nice值，可以理解为此进程占用了多少系统资源，%CPU表示进程占用的CPU资源百分比 参数说明 -p 使用-p参数然后指定进程id可以只查看指定id的进程信息，使用示例： 123456789[root@iZm5ehzqow4ijp2ya2g2drZ ~]# top -p 35top - 16:33:23 up 29 days, 6:49, 2 users, load average: 0.00, 0.00, 0.00Tasks: 1 total, 0 running, 1 sleeping, 0 stopped, 0 zombieCpu(s): 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1019980k total, 758240k used, 261740k free, 139084k buffersSwap: 0k total, 0k used, 0k free, 239196k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 35 root 25 5 0 0 0 S 0.0 0.0 0:00.00 ksmd 进程的优先级调整进程的优先级我们可以通过查看进程的信息来具体查看，进程信息的NI选项就表示进程的优先级，NI的值在 -20 ~ 19 之间，值越小，优先级越高，抢占资源就越多，一般我们启动的程序默认的优先级是0 使用nice命令修改启动进程的优先级我们首先启动一个程序，查看它的进程信息如下： 1234567891011121314# 第一个终端启动此自定义进程[root@iZm5ehzqow4ijp2ya2g2drZ home]# ./test.sh2946# 重新打开一个终端，查看此进程的信息[root@iZm5ehzqow4ijp2ya2g2drZ ~]# top -p 2946top - 17:14:15 up 29 days, 7:29, 3 users, load average: 0.89, 0.85, 0.55Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombieCpu(s):100.0%us, 0.0%sy, 0.0%ni, 0.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1019980k total, 760396k used, 259584k free, 139088k buffersSwap: 0k total, 0k used, 0k free, 239332k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2946 root 20 0 103m 1332 1152 R 99.9 0.1 0:53.76 test.sh 我们看到此进程的NI值为0，CPU的占用率达到了100%，因为我们这个程序一直在循环执行中，然后我们停止此程序，然后使用nice命令指定优先级 nice命令使用方法：nice -n [优先级值] ./应用程序 使用示例： 1234567891011121314# 使用第一个终端启动自定义的应用程序并指定进程优先级为15[root@iZm5ehzqow4ijp2ya2g2drZ home]# nice -n 15 ./test.sh2953# 使用第二个终端查看此进程信息[root@iZm5ehzqow4ijp2ya2g2drZ ~]# top -p 2953top - 17:12:54 up 29 days, 7:28, 3 users, load average: 0.92, 0.84, 0.52Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombieCpu(s): 0.0%us, 0.0%sy,100.0%ni, 0.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1019980k total, 760396k used, 259584k free, 139088k buffersSwap: 0k total, 0k used, 0k free, 239332k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2953 root 35 15 103m 1332 1152 R 99.6 0.1 1:42.75 test.sh 通过更改程序的优先级来启动程序，我们发现更改后的此进程NI值就变成了我们设置的值，并且很明显的区别就是CPU占用率，我们将优先级值调大之后，CPU占用率在0.0% - 0.3%摆动，表示资源占用急剧下降，所以我们指定程序的优先级是很有效果的一个操作 使用renice命令修改进程的优先级nice命令有一个问题，我们一般程序在运行中不会终止，上述更改优先级需要将程序停止然后重新指定优先级后启动，这个问题怎么解决呢？ 使用renice命令就可以解决，可以在程序运行中直接修改，无需重启 使用方法：renice -n [优先级值] ./应用程序 使用示例： 1234567891011121314151617181920212223# 第一个终端启动此自定义进程[root@iZm5ehzqow4ijp2ya2g2drZ home]# ./test.sh2966# 使用第二个终端查看此进程的信息[root@iZm5ehzqow4ijp2ya2g2drZ ~]# top -p 2966...Cpu(s):100.0%us, 0.0%sy, 0.0%ni, 0.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2966 root 20 0 103m 1332 1152 R 99.7 0.1 0:47.87 test.sh # 使用第三个终端使用renice修改此进程的优先级值为18[root@iZm5ehzqow4ijp2ya2g2drZ ~]# renice -n 18 29662966: old priority 0, new priority 18# 切换回第二个终端查看此进程信息，我们发现进程的优先级已经被修改，且程序并没有停止[root@iZm5ehzqow4ijp2ya2g2drZ ~]# top -p 2966...Cpu(s): 0.0%us, 0.0%sy,100.0%ni, 0.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2966 root 38 18 103m 1332 1152 R 99.6 0.1 5:07.68 test.sh 进程的启动前台后台切换程序后台启动我们上述启动程序发现程序一直在终端显示，无法输入其他命令，此终端不能进行其他操作，这种启动的方式叫做前台启动，后台启动就是程序是后台运行的，不影响我们在终端进行操作 后台启动程序的方式是在启动程序命令后 加 &amp; ，使用示例： 123456789[root@iZm5ehzqow4ijp2ya2g2drZ home]# ./test.sh &amp;[3] 3019[root@iZm5ehzqow4ijp2ya2g2drZ home]# 3019lstest.sh[root@iZm5ehzqow4ijp2ya2g2drZ home]# top -p 3019... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 3019 root 20 0 103m 1332 1152 R 49.9 0.1 0:14.74 test.sh 通过示例我们发现，启动程序后终端返回了程序的进程ID，然后我们就可以在此终端上进行任意其他操作 后台运行程序切换回前台运行有时候我们需要将后台运行的程序切换回前台运行，可以使用 jobs命令查询出所有的后台程序，然后根据后台运行编号使用fg命令切换回来，使用如下： 1234[root@iZm5ehzqow4ijp2ya2g2drZ home]# jobs[3]+ Running ./test.sh &amp;[root@iZm5ehzqow4ijp2ya2g2drZ home]# fg 3./test.sh 我们发现程序就恢复前台运行了，此时可以通过 CTRL + C 来终止此程序 前台运行程序切换到后台运行使用 CTRL + Z 可以将前台运行程序切换为后台运行，但是使用此命令切换到后台运行，程序就不是运行的状态了，而是一个暂停运行的状态，它保存在了内存中 使用示例： 12345# 首先前台启动进程，然后使用 CTRL + Z 将程序转为后台运行，并且提示你程序为Stopped状态，既停止运行的状态[root@iZm5ehzqow4ijp2ya2g2drZ home]# ./test.sh3043^Z[4]+ Stopped ./test.sh 上述示例我们发现程序进入后台运行但是是停止状态，我们需要让程序继续运行怎么办？使用jobs命令，然后使用 fg 将程序切换为前台运行或者使用 bg 将程序切换为后台运行 切换为前台运行我们上面已经有过示例了，现在我们示例切换为后台运行，完整示例如下： 12345678910111213# 前台运行程序然后切换为后台暂停运行[root@iZm5ehzqow4ijp2ya2g2drZ home]# ./test.sh3050^Z[5]+ Stopped ./test.sh# 将程序切换为后台运行[root@iZm5ehzqow4ijp2ya2g2drZ home]# jobs[5]+ Stopped ./test.sh[root@iZm5ehzqow4ijp2ya2g2drZ home]# bg 5[5]+ ./test.sh &amp;[root@iZm5ehzqow4ijp2ya2g2drZ home]# jobs[5] Running ./test.sh 上述后，我们发现程序就为后台运行了 进程的通信方式与信号进程之间的通信就是两个进程之间可以有一些消息进行交互，还有就是两个进程之间可以互相进行控制。进程通信有很多种的方式，比如管道，信号，这里我们主要学习信号 信号：终端用户通过快捷键或者命令来输入信号，通过信号的机制可以让程序停止运行，经常使用的有 CTRL + C、kill等 kill命令我们可以使用kill命令的 -l 参数来查看我们系统支持的所有信号，示例如下： 1234567891011121314[root@iZm5ehzqow4ijp2ya2g2drZ ~]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 以上就是我们系统中所有的信号，平时我们使用的CTRL + C对应的就是2号信号，SIGINT kill命令具体使用方法：kill [-信号编号] 进程ID 比如我们平时使用的更多的是 kill -9 9号命令，它是用于强制停止进程，直接杀掉，一般我们在生产中直接使用 kill 命令来结束程序，可以达到优雅下线的效果 守护进程很多时候我们希望我们有些进程在系统启动的时候就启动，或者有些我们在终端启动的进程在关闭终端之后还继续运行，Linux提供了实现这种需求的方法，就是使用守护进程，英文叫 daemon，就是精灵的意思，守护进程实现的就是我们不需要使用终端就可以把进程启动起来，另外启动的时候它的输出也会打印出来，并且这个程序使用的目录是根目录，避免占用其他一些移动硬盘的目录，导致此移动硬盘无法卸载的情况 nohup启动进程在讲守护进程之前我们先讲另外一个进程，叫nohup启动进程，以此来对比守护进程 nohup启动进程会使进程忽略掉 hangup 挂起信号，使用此进程我们可以实现守护进程的效果，比如关掉终端之后进程还可以继续运行，但它还是有一些不同，首先这里示例一下使用nohup进程的效果 123456789101112131415# 首先我们使用第一个终端然后运行 tail 进程[root@iZm5ehzqow4ijp2ya2g2drZ ~]# tail -1000f /balyu/logs/platformrun.log 2019-10-19 01:02:12.606 [http-nio-80-exec-16] INFO com.sparrow.portal.IndexController - access index...2019-10-19 01:18:06.548 [http-nio-80-exec-18] INFO com.sparrow.manage.monitor.AccessMonitor - 123.126.113.160...# 然后打开第二个终端查看此进程 这里是可以查看到的[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ps -ef | grep tailroot 12218 12129 0 13:46 pts/2 00:00:00 tail -1000f /balyu/logs/platformrun.logroot 12237 12221 0 13:47 pts/3 00:00:00 grep tail# 然后第三步是直接关闭终端，不退出tail直接关闭终端窗口# 然后第四步我们继续到另一个终端查看此进程 -&gt; 发现tail进程也不存在了[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ps -ef | grep tailroot 12241 12221 0 13:49 pts/3 00:00:00 grep tail 下面示例使用nohup来是进程在终端关闭后继续运行 12345678910111213# 第一步使用nohup启动tail进程，启动后提示将此进程的输出比如tail的查看内容输出放在了 nohup.out 文件下[root@iZm5ehzqow4ijp2ya2g2drZ ~]# nohup tail - 100f /balyu/logs/platformrun.log nohup: ignoring input and appending output to `nohup.out&apos;# 第二步启动另外一个终端查看此进程，这里我们查看到此进程的父进程是 pts/3[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ps -ef | grep tailroot 12251 12221 0 13:54 pts/3 00:00:00 tail -100f /balyu/logs/platformrun.logroot 12270 12254 0 13:55 pts/2 00:00:00 grep tail# 第三步关闭此终端，然后使用另一个终端查看tail进程，我们发现进程还在存活，但是父进程变成了?，这是因为我们的进程在终端关闭之后此进程就成了孤儿进程，孤儿进程一定要被其它进程所收留，那么这里它就被 1 号进程所收留，这里显示了一个 ?，但其实是 1 号进程，init进程[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ps -ef | grep tailroot 12251 1 0 13:54 ? 00:00:00 tail -100f /balyu/logs/platformrun.logroot 12272 12254 0 13:56 pts/2 00:00:00 grep tail 以上就是nohup的使用示例，这里我们发现虽然可以将进程一直运行，但是启动程序的时候还是要通过终端启动，还有一个特点就是我们的进程输出会放在我们执行命令的当前文件夹下的nohup.out文件，它会占用我们当前磁盘的空间，name我们可不可以像windows一样，设置开机自启动呢？并且有一个合理的日志文件或者输出内容的文件所在的位置呢？请看下面的内容 screen命令Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。GNU Screen可以看作是窗口管理器的命令行界面版本。它提供了统一的管理多个会话的界面和相应的功能 在Screen环境下，所有的会话都独立的运行，并拥有各自的编号、输入、输出和窗口缓存。用户可以通过快捷键在不同的窗口下切换，并可以自由的重定向各个窗口的输入和输出 总的来说个人理解就是使用screen启动的程序我们在退出screen之后，程序还会继续运行，退出终端也会如此，所以直接将启动的程序作为了守护进程 使用方法： 使用 screen 命令直接进入screen环境 操作完成后或者启动程序完成后，使用 CTRL + A D，退出（detached）screen环境 我们可以使用 screen -ls 查看所有screen的会话 可以使用 screen -r sessionid 恢复会话，注意这里的sessionid是上述使用-ls查询出来的信息 注意：我们可以在screen模式下结束我们的进程，退出此环境使用 exit 退出此环境 以下是使用示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 1.首先使用screen命令进入此环境[root@iZm5ehzqow4ijp2ya2g2drZ ~]# screen-bash: screen: command not found# 2.这里提示找不到这个命令，是因为我们没有安装，需要使用yum命令进行安装[root@iZm5ehzqow4ijp2ya2g2drZ ~]# yum install screenLoaded plugins: fastestmirrorSetting up Install Process...Installed: screen.x86_64 0:4.0.3-19.el6Complete!# 3.安装成功后再次使用此命令进入Screen环境，我们发现直接就进入了此环境，并没有任何提示和不一样的地方[root@iZm5ehzqow4ijp2ya2g2drZ ~]# screen[root@iZm5ehzqow4ijp2ya2g2drZ ~]# # 4.再此环境下执行命令，还是之前使用的tail命令来让它后台运行[root@iZm5ehzqow4ijp2ya2g2drZ ~]# tail -1000f /balyu/logs/platformrun.log 2019-10-26 00:15:58.351 [http-nio-80-exec-9] INFO com.sparrow.portal.IndexController - access index...2019-10-26 00:51:58.660 [http-nio-80-exec-7] ERROR org.apache.catalina.core.ContainerBase.[Tomcat].[localhost] - Exception Processing ErrorPage[errorCode=0, location=/error]org.apache.catalina.connector.ClientAbortException: java.io.IOException: Connection reset by peer# 5.使用另外一个客户端，查看此启动的进程，我们发现实可以查询到的[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ps -ef | grep tailroot 20513 20484 0 12:02 pts/1 00:00:00 tail -1000f /balyu/logs/platformrun.logroot 20532 20516 0 12:04 pts/3 00:00:00 grep tail# 6.我们使用CTRL + A D退出screen环境，退出后发现终端并没有什么变化[root@iZm5ehzqow4ijp2ya2g2drZ ~]# screen[detached][root@iZm5ehzqow4ijp2ya2g2drZ ~]# # 7.我们再次用第二个客户端查看在screen环境中启动的tail进程发现是可以查询到的，即使将第一个终端直接关闭掉，也是可以查询到此进程[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ps -ef | grep tailroot 20513 20484 0 12:02 pts/1 00:00:00 tail -1000f /balyu/logs/platformrun.logroot 20538 20516 0 12:09 pts/3 00:00:00 grep tail# 8.使用第二个终端查询当前系统上运行的screen环境， screen -ls 命令，查询发现启动的有一个，并且sessionid是20483 [root@iZm5ehzqow4ijp2ya2g2drZ ~]# screen -lsThere is a screen on: 20483.pts-0.iZm5ehzqow4ijp2ya2g2drZ (Detached)1 Socket in /var/run/screen/S-root.# 9.使用 screen -r sessionid恢复此screen环境，发现是恢复了的[root@iZm5ehzqow4ijp2ya2g2drZ ~]# screen -r 20483 at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java:726) at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:471) ... # 10.使用CTRL + C 退出此环境下前台运行的程序（tail 程序），然后使用exit退出screen环境... 这里不做演示# 11.再次使用screen -ls 查询运行的screen环境，发现已经查询不到了，因为我们已经退出了[root@iZm5ehzqow4ijp2ya2g2drZ ~]# screen -lsNo Sockets found in /var/run/screen/S-root. 系统日志这里只是一个补充说明，我们可以进入 /var/log 文件夹，这里有大量的日志文件，这些文件叫做系统日志，我们系统运行的状态都记录在这些文件中，重点关注以下文件就行： messages文件，系统的常规日志，也可以理解为当前系统运行实时产生的日志 dmesg文件，内核日志信息，内核日志一般在我们启动系统的时候启动内核所打印出来的日志，一般我们在系统启动时可能查看的不够详细，这样我们就可以直接查看此文件来详细查看内核启动运行的一些日志信息 secure文件，这个文件是我们系统的安全日志，此文件可以查看系统有没有产生一些安全的问题 cron文件，这个日志是我们系统执行计划任务，定时任务等产生的日志，查看这种任务的日志可以在此文件中查看 服务管理工具这里服务指的是提供常见功能的守护进程，是系统本身所提供的服务。服务管理工具说的是我们之前在网络管理和配置文件章节讲述过，有两种，CentOS7以前的版本是用的是service工具，之后默认使用的是systemctl工具，但是我们可以切换回service工具来管理 我们之前讲述过service的基本启动脚本在 /etc/init.d 文件夹中，这里有我们service管理工具所管理的所有服务，每个服务都有一个文件，此文件中便是服务的启动关闭重启等等的操作的命令脚本，这些脚本也可以我们自己来编写，就比如之前我们操作过的网卡服务，network文件中便有一堆关于操作网卡的脚本，每个操作我们可以理解为一个函数，函数中又具体的后台的逻辑。我们可以看到，service管理的服务脚本都很复杂。而systemctl管理工具基本的脚本在 /usr/lib/systemd/system 文件夹下，里面也是各种文件，文件所管理的就是服务，比如我们打开sshd.service文件，这个里面就是封装的sshd服务的一些操作脚本，我们明显可以看到这里的脚本比service管理的服务脚本要简单很多，这是因为脚本中具体的逻辑都已经有systemctl来管理了，不需要我们手动自己来写这些复杂的脚本，所有相对看着比较简单，我们在Linux系统的迭代中，会逐渐的将所有的service管理的服务都交给systenctl来管理。 之前我们说过service管理的服务都有级别，分为6个级别，这6个级别分别代表程序在哪个级别是启动的，在哪个级别的关闭的，我们可以通过service提供的chkconfig命令来查看及修改服务在各种级别运行的状态，而systemctl管理工具不是用过数字来代表启动级别了，而是直接通过英语单词来代表 systemctl管理工具怎么操作服务，我们列举一下常见的操作： start、stop、restart、reload、enable、disable、status 使用的方法就是：systemctl 操作命令 服务名称，例如：systemctl start sshd.service 具体有哪些服务呢？我们可以直接查看 /usr/lib/systemd/system 文件夹，里面的内容中有很多扩展名为 .service 和 .target 的文件，.service的文件就是我们管理的服务，我们就可以通过systemctl命令来操作这些服务，这些.target的文件，表示的就是服务表示的就是不同的级别，runleavel0.target 到 runleavel6.target 就表示我们之前描述的 0 - 6 的系统级别，我们可以通过命令 ls -l runleavel*.target 来查看这些文件的具体映射，这些查看出来的文件就映射到了具体的英文单词表示的级别 我们还可以通过 systemctl get-default 来查看我们当前系统的默认级别，字符级别还是图形界面级别。还可以通过 systemctl set-default 级别英文单词表示 来修改我们设置的默认级别，这样在下次启动系统的时候就切换了默认的级别，例如： systemctl set-default multy-user.target 就将默认的启动级别就切换成多用户的启动级别了，也就是字符界面级别 以上具体可以查看 网络管理和配置文件 章节","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"进程管理","slug":"进程管理","permalink":"https://jjw-story.github.io/tags/进程管理/"}],"author":"JJW"},{"title":"网络故障排除命令","slug":"网络故障排除命令","date":"2019-09-03T12:02:04.000Z","updated":"2019-09-03T11:45:01.715Z","comments":true,"path":"2019/09/03/网络故障排除命令/","link":"","permalink":"https://jjw-story.github.io/2019/09/03/网络故障排除命令/","excerpt":"","text":"网络故障排查命令ping命令Linux系统的ping命令是常用的网络命令，它通常用来测试与目标主机的连通性 使用方法：ping [参数] 主机IP或域名 参数详解： -q 不显示任何传送封包的信息，只显示最后的结果 -n 只输出数值 -R 记录路由过程 -c count 总次数 -i 时间间隔 -t 存活数值：设置存活数值TTL的大小 使用示例： 123456789wangjia3@CHJ-20190520VPS:~$ ping -c 3 192.168.1.1PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data.64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=3.18 ms64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=3.14 ms64 bytes from 192.168.1.1: icmp_seq=3 ttl=64 time=10.0 ms--- 192.168.1.1 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2002msrtt min/avg/max/mdev = 3.147/5.445/10.007/3.226 ms traceroute命令命令用于显示数据包到主机间的路径，traceroute指令让你追踪网络数据包的路由途径，预设数据包大小是40Bytes，用户可另行设置 使用方法：traceroute [参数] 主机IP或域名 参数详解： -m &lt;存活数值&gt; 设置检测数据包的最大存活数值TTL的大小 -n 直接使用IP地址而非主机名称 -v 详细显示指令的执行过程 -w &lt;超时秒数&gt; 设置等待远端主机回报的时间 12345678910traceroute to www.baidu.com (180.101.49.12), 30 hops max, 60 byte packets 1 * * * 2 11.219.5.13 (11.219.5.13) 6.841 ms 11.219.5.85 (11.219.5.85) 7.559 ms 11.219.4.85 (11.219.4.85) 6.439 ms 3 * * * 4 11.219.68.42 (11.219.68.42) 0.427 ms 0.436 ms 11.219.68.26 (11.219.68.26) 0.679 ms 5 103.41.143.69 (103.41.143.69) 1.187 ms 103.52.86.106 (103.52.86.106) 1.045 ms 103.52.86.138 (103.52.86.138) 0.956 ms 6 116.251.113.33 (116.251.113.33) 1.145 ms 116.251.113.37 (116.251.113.37) 1.447 ms 116.251.113.53 (116.251.113.53) 0.856 ms 7 150.138.130.121 (150.138.130.121) 1.377 ms 150.138.132.129 (150.138.132.129) 0.939 ms 150.138.130.133 (150.138.130.133) 1.184 ms 8 150.138.128.65 (150.138.128.65) 12.132 ms 150.138.128.173 (150.138.128.173) 17.526 ms 17.970 ms 9 202.97.46.49 (202.97.46.49) 22.672 ms 显示的访问此主机中间经过的路由，中间路由对应的IP地址及它的延时是多长时间等等，注意如果不支持traceroute方式追踪，会以 * * * 的方式展示出来 mtr命令在Linux中有一个更好的网络连通性判断工具，它可以结合ping nslookup tracert 来判断网络的相关特性,这个命令就是mtr 此命令要比上述traceroute命令展示的信息更加详细，所以使用的更多 使用方法：mtr [参数] [主机IP或域名] 参数详解： -r 已报告模式显示 使用示例： 123456789101112131415161718[root@iZm5ehzqow4ijp2ya2g2drZ ~]# mtr -r www.baidu.comHOST: iZm5ehzqow4ijp2ya2g2drZ Loss% Snt Last Avg Best Wrst StDev 1. ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 2. 11.219.4.13 0.0% 10 2.3 4.5 1.6 12.9 3.7 3. ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 4. 11.219.68.58 0.0% 10 0.4 1.8 0.4 12.5 3.8 5. 116.251.117.198 0.0% 10 0.7 0.8 0.7 1.1 0.2 6. 140.205.26.217 0.0% 10 1.2 1.5 1.2 3.0 0.5 7. 150.138.132.149 0.0% 10 1.6 1.5 1.4 1.6 0.1 8. 150.138.128.157 0.0% 10 18.6 20.8 18.6 27.0 3.5 9. 202.97.46.61 20.0% 10 24.5 23.1 22.9 24.5 0.6 10. 58.213.94.6 0.0% 10 18.2 18.4 18.0 20.4 0.7 11. 58.213.94.122 90.0% 10 24.1 24.1 24.1 24.1 0.0 12. 58.213.96.90 0.0% 10 21.4 21.6 21.2 23.7 0.7 13. ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 14. ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 15. ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 16. 180.101.49.12 0.0% 10 18.0 18.0 17.9 18.0 0.0 注意此命令可以不带任何参数及IP单独使用，My traceroute 报告解释： 第一列:显示的是IP地址和本机域名，这点和tracert很像 第二列:snt:10 设置每秒发送数据包的数量，默认值是10 可以通过参数 -c来指定 第三列:是显示的每个对应IP的丢包率 第四列:显示的最近一次的返回时延 第五列:是平均值 这个应该是发送ping包的平均时延 第六列:是最好或者说时延最短的 第七列:是最差或者说时延最常的 第八列:是标准偏差 nslookup命令查看哪台DNS服务器进行的域名解析，并解析出域名对应的IP地址 使用方法：nslookup 域名 使用示例： 12345678910wangjia3@CHJ-20190520VPS:~$ nslookup www.baidu.comServer: 192.168.1.1Address: 192.168.1.1#53Non-authoritative answer:www.baidu.com canonical name = www.a.shifen.com.Name: www.a.shifen.comAddress: 61.135.169.121Name: www.a.shifen.comAddress: 61.135.169.125 以上查询第一行显示的是DNS服务器的地址，这里百度是有对应的别名的：www.baidu.com canonical name = www.a.shifen.com. 别名下有对应的两个地址 telnet命令telnet命令通常用来远程登录，它为用户提供了在本地计算机上完成远程主机工作的 能力。在终端使用者的电脑上使用telnet程序，用它连接到服务器。终端使用者可以在telnet程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样。可以在本地就能控制服务器。要开始一个 telnet会话，必须输入用户名和密码来登录服务器。Telnet是常用的远程控制Web服务器的方法。 但是，telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。 telnet命令还可做别的用途，比如确定远程服务的状态，比如确定远程服务器的某个端口是否能访问，这里我们说明的主要使用就是确认端口是否能够访问 畅通 使用方法：telnet [参数] 主机 端口 使用示例： 1234567891011121314# 端口可达wangjia3@CHJ-20190520VPS:~$ telnet www.baidu.com 80Trying 61.135.169.121...Connected to www.a.shifen.com.Escape character is &apos;^]&apos;.^]telnet&gt; quit# 端口不可达wangjia3@CHJ-20190520VPS:~$ telnet www.baidu.com 8900Trying 61.135.169.121...Trying 61.135.169.125...telnet: Unable to connect to remote host: Resource temporarily unavailable 注意我们需要通过 CTRl + 右侧方括号 来退出此查看，然后使用 CTRL + C 或者 quit命令，退出 telnet tcpdump命令很多时候我们的系统部署在Linux系统上面，在一些情况下定位问题就需要查看各个系统之间发送数据报文是否正常，下面我就简单讲解一下如何使用tcpdump抓包。tcpdump是Linux下面的一个开源的抓包工具，和Windows下面的wireshark抓包工具一样， 支持抓取指定网口、指定目的地址、指定源地址、指定端口、指定协议的数据。 用简单的话来定义tcpdump，就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 tcpdump可以将网络中传送的数据包的“头”完全截获下来提供分析。它支 持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来帮助你去掉无用的信息 使用说明： 监视所有网卡接口的数据包： tcpdump -i any 监视指定网络接口的数据包： tcpdump -i eth1 如果不指定网卡，默认tcpdump只会监视第一个网络接口，一般是eth0，下面的例子都没有指定网络接口 捕获所有网卡发往80端口的数据包，并且如果包含域名，将域名解析成IP，主要使用 -n 参数 tcpdump -i any -n port 80 监视指定主机收到的和发出的所有的数据包： tcpdump -i any -n host 210.27.48.1 监视指定主机和端口的数据包： tcpdump -i any -n port 23 and host 210.27.48.1 netstat命令Linux netstat命令用于显示网络状态，利用netstat指令可让你得知整个Linux系统的网络情况，查看服务的监听地址 使用方法：netstat [参数] 常用参数说明： -n 直接使用IP地址，而不通过域名服务器 -t 显示TCP传输协议的连线状况 -u 显示UDP传输协议的连线状况 -p 显示正在使用Socket的程序识别码和程序名称 -l 显示监控中的服务器的Socket -v 显示指令执行过程 使用示例： 12345[root@iZm5ehzqow4ijp2ya2g2drZ ~]# netstat -ntplActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1260/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1427/master ss命令ss是Socket Statistics的缩写。顾名思义，ss命令可以用来获取socket统计信息，它可以显示和netstat类似的内容。ss的优势在于它能够显示更多更详细的有关TCP和连接状态的信息，而且比netstat更快速更高效 当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢 ss快的秘诀在于，它利用到了TCP协议栈中tcp_diag。tcp_diag是一个用于分析统计的模块，可以获得Linux 内核中第一手的信息，这就确保了ss的快捷高效 使用方法：ss [参数] 常用参数说明： -n 直接使用IP地址，而不通过域名服务器 -t 显示TCP传输协议的连线状况 -u 显示UDP传输协议的连线状况 -p 显示正在使用Socket的程序识别码和程序名称 -l 显示监控中的服务器的Socket -m 显示套接字的内存使用信息 -p 显示使用套接字的进程 使用示例： 1234[root@iZm5ehzqow4ijp2ya2g2drZ ~]# ss -ntplState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* users:((&quot;sshd&quot;,1260,3))LISTEN 0 100 127.0.0.1:25 *:* users:((&quot;master&quot;,1427,12))","categories":[],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"网络管理","slug":"网络管理","permalink":"https://jjw-story.github.io/tags/网络管理/"}]},{"title":"网络管理和配置文件","slug":"网络管理和配置文件","date":"2019-08-30T05:18:53.000Z","updated":"2019-11-02T09:50:48.312Z","comments":true,"path":"2019/08/30/网络管理和配置文件/","link":"","permalink":"https://jjw-story.github.io/2019/08/30/网络管理和配置文件/","excerpt":"","text":"网络服务管理概述当我们需要将网络的配置固化下来，既重启服务也能保持我们的配置状态，就需要修改Linux的配置文件，将配置文件中的配置修改后，就能将我们的配置固化下来 管理配置文件我们一般是使用一些管理程序，网络服务管理程序分为两种，分别是SysV和systemd，systemd是 CentOS 7.0 版本新添加的管理程序 配置文件一般是如下两类： 网卡配置文件：ifcfg-etho 注意：etho是根据网卡名称的不同不一样的，它会随着你的真实网卡名称变化， 主机名相关配置文件，控制网络常用参数：/etc/hosts 注意在CentOS7版本中，有两套服务管理的脚本，一套是 network (任何版本都支持)，一套是 NetworkManager，在工作当中我们一般只使用其中一套，不推荐两套都使用，可以使用以下命令查看当前机器是否支持使用 NetworkManager： 1systemctl list-unit-files NetworkManager.service 当我们想要关闭 network 服务管理只使用 NetworkManager 时就需要使用chkconfig命令来解决了，具体是 –level 参数 NetworkManager的作用： 一般应用于个人的主机，比如插入网线之后可以识别网卡的激活状态，自动进行网络的激活，或者比如我们连接到熟悉的无线网络连接当中，它会自动激活无线的连接。但是应用于服务器上这些功能都有些鸡肋，所以服务器上我们一般还是沿用network脚本 启用/禁用 NetworkManager 脚本命令： 12345# 启用systemctl disable NetworkManager禁用systemctl enable NetworkManager 配置文件说明网卡配置文件： 网卡配置文件在 /etc/sysconfig/network-scripts/目录下，在此目录下会有很多以 ifcfg 开头的文件，这些文件每一个对应着我们的一个网络接口，我们可以打开其中一个进行查看： 12345678910[root@iZm5ehzqow4ijp2ya2g2drZ /]# cd /etc/sysconfig/network-scripts/[root@iZm5ehzqow4ijp2ya2g2drZ network-scripts]# ls ifcfg-*ifcfg-eth0 ifcfg-lovim ifcfg-eth0NAME=ethoDEVICE=eth0BOOTPROTO=dhcpONBOOT=yes 网卡配置文件内容说明： 我们打开ifcfg-eth0文件，发现其中设置内容还是有一些的，基本格式都是前边是设置项，后面是设置值，这些设置有一些是它的关键设置，有些是IPV6它的初始化设置，我们只需要关注其中几项即可 BOOTPROTO=dhcp，表示我们的机器它的IP地址是动态分配的，我们可以把它的值修改为 none ，表示IP地址是静态分配的，静态分配具体配置将在下面专门描述 NAMR=etho，DEVICE=eth0，表示的是网卡的名称设置 ONBOOT=yes，表示网卡是否开机启用，如果是 yes 表示开机启用，如果是 no 表示开机不启用 网卡IP静态分配配置： 配置示例如下： 12345678910vim ifcfg-eth0NAME=ethoDEVICE=eth0BOOTPROTO=noneONBOOT=yesIPADDR=192.168.1.28NETMASK=255.255.255.0GATEWAY=192.168.1.1DNS1=114.114.114.114 注意：配置静态IP需要修改BOOTPROTO设置项，然后添加IP地址、子网掩码、网关、默认DNS的地址 设置完成需要使用命令来让它生效，使用命令有两种，分别如下，命令具体使用在后边分析描述： service network restart systemctl restart NetworkManager.service 如上操作完成之后，就可以使用命令 ipconfig etho 来查看IP及子网掩码配置是否生效，使用 route -n 查看网关是否配置生效，使用 nslookup 查看默认的DNS设置是否生效 chkconfigchkconfig命令主要用来更新（启动或停止）和查询系统服务的运行级信息。谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接 使用方法：chkconfig 参数 系统服务 参数详解： –list 列出所有的系统服务及状态，使用示例如下： 12[root@iZm5ehzqow4ijp2ya2g2drZ ~]# chkconfig --list networknetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:off 上述示例中，数字表示不同的级别，及不同的级别服务的运行情况，级别表示如下： 等级0表示：表示关机 等级1表示：单用户模式 等级2表示：无网络连接的多用户命令行模式 等级3表示：有网络连接的多用户命令行模式 等级4表示：不可用 等级5表示：带图形界面的多用户模式 等级6表示：重新启动 –level 指定读系统服务要在哪一个执行等级中开启或关毕，使用时需要指定等级 使用示例，将当前开启的服务全部关闭 2 3 4 5 等级都关闭： 1234[root@iZm5ehzqow4ijp2ya2g2drZ ~]# chkconfig --level 2345 network off[root@iZm5ehzqow4ijp2ya2g2drZ ~]# chkconfig --list networknetwork 0:off 1:off 2:off 3:off 4:off 5:off 6:off 注意：当我们在CentOS7及以上版本中如此操作时，就相当与关闭了 network 服务管理工具，改为使用 NetworkManager 查看网络状态service network status 命令 使用示例： 12345[root@iZm5ehzqow4ijp2ya2g2drZ ~]# service network statusConfigured devices:lo eth0Currently active devices:lo eth0 如上查询结果中，第一表示我们已配置的网卡设备，第二表示当前活跃的设备 还原网卡默认配置 service network restart 将我们使用ifconfig、route等命令自己配置的内容恢复到默认的状态 使用示例： 1234567[root@iZm5ehzqow4ijp2ya2g2drZ ~]# service network restartShutting down interface eth0: [ OK ]Shutting down loopback interface: [ OK ]Bringing up loopback interface: [ OK ]Bringing up interface eth0: Determining IP information for eth0... done. [ OK ] systemctl restart NetworkManager.service 如果在 CentOS7 及以上版本中开启了 NetworkManager 脚本，还可以使用此命令来还原配置 配置主机名称查看主机名称查看主机名称使用 hostname 命令，使用示例如下： 12wangjia3@CHJ-20190520VPS:~$ hostnameCHJ-20190520VPS 临时设置主机名称临时设置就是当前修改过来，但是主机重启之后还是恢复成默认的 使用命令：hostname 自定义主机名称 如下示例： 1234567wangjia3@CHJ-20190520VPS:~$ hostname CHJhostname: you must be root to change the host namewangjia3@CHJ-20190520VPS:~$ sudo hostname CHJ[sudo] password for wangjia3:wangjia3@CHJ-20190520VPS:~$ hostnameCHJ 永久修改主机名称使用命令：hostnamectl set-hostname 自定义主机名称 这样设置之后即使重启之后也会使用新的主机名，但是要注意，如果我们更改主机名之后，很多服务是要依赖主机名进行工作，这里我们必须一个配置文件中将新的主机名写在127.0.0.1的对应关系当中，如果不写可能会出现在启动系统的时候，在某个服务上卡住，在等待它超时，这个文件是 /etc/hosts，如下： 12345678910 2 # %WINDIR%\\System32\\drivers\\etc\\hosts. Modifications to this file will be overwritten. 3 127.0.0.1 localhost 4 127.0.1.1 CHJ-20190520VPS.localdomain CHJ-20190520VPS # 修改此行内容 5 6 # The following lines are desirable for IPv6 capable hosts 7 ::1 ip6-localhost ip6-loopback 8 fe00::0 ip6-localnet 9 ff00::0 ip6-mcastprefix10 ff02::1 ip6-allnodes11 ff02::2 ip6-allrouters","categories":[],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"网络管理","slug":"网络管理","permalink":"https://jjw-story.github.io/tags/网络管理/"}]},{"title":"软件包管理器和内核升级","slug":"软件包管理器","date":"2019-08-25T03:17:20.000Z","updated":"2019-10-06T08:22:48.743Z","comments":true,"path":"2019/08/25/软件包管理器/","link":"","permalink":"https://jjw-story.github.io/2019/08/25/软件包管理器/","excerpt":"","text":"介绍包管理器是方便软件安装、卸载、解决软件依赖关系的重要工具 CentOS、RedHat使用yum包管理器，软件安装包格式为rpm Debian、Ubuntu使用apt包管理器，软件安装包格式为deb rpmrpm包格式： vim-common-7.4.10.5.el7.x86_64.rpm 分别对应着软件名称、软件版本、系统版本、平台，然后以.rpm作为结尾。注意系统版本 el7 表示支持7版本的Linux系统，x86_64表示64位的平台系统 rpm命令使用方法：rpm [参数] 常用参数： -a： 查询所有套件 -c： 只列出组态配置文件，本参数需配合”-l”参数使用 -d： 只列出文本文件，本参数需配合”-l”参数使用 -e&lt;软件包&gt;： 删除指定的软件 -f&lt;文件&gt;+： 查询拥有指定文件的套件 -i&lt;软件包&gt;： 安装指定的软件包 -l： 显示套件的文件列表 -p&lt;软件包&gt;+： 查询指定的RPM套件档 -q： 使用询问模式，当遇到任何问题时，rpm指令会先询问用户 查询安装的软件包使用示例： 123456789101112131415# 查询所有安装的软件包[root@iZm5ehzqow4ijp2ya2g2drZ ~]# rpm -qavim-common-7.4.629-5.el6_8.1.x86_64setup-2.8.14-23.el6.noarchtcpdump-4.0.0-11.20090921gitdf3cb4.2.el6.x86_64basesystem-10.0-4.el6.noarch...# 显示内容太多还可以分页显示，命令如下：rpm -qa | more使用管道符加 more 即可实现 more 命令的翻页效果# 查询是否安装指定的软件包：使用-q参数，后面跟上要查询的软件包名称[root@iZm5ehzqow4ijp2ya2g2drZ ~]# rpm -q vim-commonvim-common-7.4.629-5.el6_8.1.x86_64 安装软件包使用示例： 1234# 主要是使用 -i 参数[root@iZm5ehzqow4ijp2ya2g2drZ ~]# rpm -i vim-enhanced-7.4.160-5.el7.x86_64.rpm# 注意很多时候我们安装软件包的时候回安装失败，错误信息为安装此安装包需要依赖另一个软件，这时我们需要先安装依赖的软件包 卸载已安装的软件包使用示例： 1234567# 主要是使用 -e 参数[root@iZm5ehzqow4ijp2ya2g2drZ ~]# rpm -e vim-enhanced注意：在删除安装的软件包时不需要指定软件包的全限定名称，只需要指定名称即可# 删除多个软件包[root@iZm5ehzqow4ijp2ya2g2drZ ~]# rpm -e vim-enhanced vim—common使用空格分开继续追加软件即可 yumyum概述及配置yum包管理器或yum仓库，它的出现是为了解决 rpm 包存在的问题： 需要自己解决依赖关系 如果我们要使用光盘中的rpm包，需要将整个光盘挂载到Linux当中，甚至如果没有光盘需要将整个光盘的iso镜像下载回来，以及软件包来源不可靠 CentOS yum源地址： http://mirror.centos.org/centos/7/ 国内镜像： https://opsx.alibaba.com/mirror 使用国内镜像有两种配置方式，第一种，需要修改yum的配置文件，修改文件如下： /etc/yum.repos.d/CentOS-Base.repo 1234567[base]name=CentOS-$releaseverenabled=1failovermethod=prioritybaseurl=http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=http://mirrors.cloud.aliyuncs.com/centos/RPM-GPG-KEY-CentOS-6 base表示基础应用的包，baseurl表示基础应用的rpm包放在哪一个源路径下，gpgcheck检测yum源软件包有没有被人恶意修改，是否为最开始发布的软件包的内容，防止被添加入木马 第二种方式，备份我们系统中yum配置文件，然后至镜像网站查询对应的OS版本的配置文件下载的地址 wget 命令，执行此 wget 命令后，就将原来的配置文件覆盖掉，之后运行 yum makecache 命令，生成缓存，让软件包指向我们要指向的开源的镜像站 使用 yum makecache 命令，可以把之前的缓存清空，然后通过网络把新的版本的软件包，配置等下载回来，然后更新，注意更新的过程不要中断 yum命令安装软件包使用yum安装软件可以自动检测软件包的依赖，并将依赖也下载安装，使用示例如下： 123456789101112131415161718192021# 首先卸载 vim-enhanced 和 vim-common 这两个软件包[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# rpm -e vim-enhanced[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# rpm -e vim-common# 然后使用 yum install 命令安装 vim-enhanced 软件包# 安装时我们发现它自动检测的安装 vim-enhanced 包的依赖关系，并将两个软件包同时进行安装[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# yum install vim-enhanced...Dependencies Resolved=============================================================================================================================================================================== Package Arch Version Repository Size===============================================================================================================================================================================Installing: vim-enhanced x86_64 2:7.4.629-5.el6_10.2 updates 1.0 MInstalling for dependencies: vim-common x86_64 2:7.4.629-5.el6_10.2 updates 6.7 MTransaction Summary===============================================================================================================================================================================Install 2 Package(s) 卸载软件包卸载软件包使用的是 yum remove 命令，使用如下： 123456789101112131415# 删除与 vim 相关的软件包[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# yum remove vim# 会提示有哪些与vim相关的软件包Dependencies Resolved=============================================================================================================================================================================== Package Arch Version Repository Size===============================================================================================================================================================================Removing: vim-enhanced x86_64 2:7.4.629-5.el6_10.2 @updates 2.2 MTransaction Summary===============================================================================================================================================================================Remove 1 Package(s) 查看软件包可以使用 yum list 命令来查看我们已经安装了哪些软件包，使用如下： 1234567[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# yum list...zvbi.i686 0.2.35-1.el6 epelzvbi.x86_64 0.2.35-1.el6 epelzvbi-devel.i686 0.2.35-1.el6 epelzvbi-devel.x86_64 0.2.35-1.el6 epelzvbi-fonts.noarch 0.2.35-1.el6 epel 还可以使用 yum list package1 查看指定的软件包的安装情况，使用如下: 12345[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# yum list vim-commonLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfileInstalled Packagesvim-common.x86_64 2:7.4.629-5.el6_10.2 @updates 升级软件包升级软件包是一个非常重要的命令，因为我们在生产环境中安装的软件可能多多少少会出现一些bug或安全漏洞，这就需要我们定期的给软件进行一定的升级，那么如何升级呢，我们就需要使用 yum update 软件包名 命令来升级指定的软件包，如果我们不加软件包名，就会对当前所有的安装的软件包进行升级，注意：并不是直接升级，是会有提示的 使用如下： 1234567891011121314151617181920212223242526272829303132333435363738# 检查所有需要升级的软件并选择是否升级Dependencies Resolved=============================================================================================================================================================================== Package Arch Version Repository Size===============================================================================================================================================================================Installing: kernel x86_64 2.6.32-754.18.2.el6 updates 32 MUpdating: binutils x86_64 2.20.51.0.2-5.48.el6_10.1 updates 2.8 M ca-certificates noarch 2018.2.22-65.1.el6 base 930 kTransaction Summary===============================================================================================================================================================================Install 1 Package(s)Upgrade 57 Package(s)Total download size: 113 MIs this ok [y/N]: # 这里提示有57个软件包需要进行升级，我们出入 y 之后，既开始升级# 升级指定的软件包[root@iZm5ehzqow4ijp2ya2g2drZ yum.repos.d]# yum update vim-filesystemDependencies Resolved=============================================================================================================================================================================== Package Arch Version Repository Size===============================================================================================================================================================================Updating: vim-filesystem x86_64 2:7.4.629-5.el6_10.2 updates 15 kTransaction Summary===============================================================================================================================================================================Upgrade 1 Package(s)Total download size: 15 kIs this ok [y/N]: 通过源代码编译安装软件包有些时候我们在安装或升级软件包时，发现官网并没有直接可以安装的软件包，既不可以通过 yum 命令直接安装，只提供了压缩源代码软件包，这时我们就需要自己来编译此源代码来进行安装 注意，此种情况比较少，且安装比较麻烦，如果 yum 或 rpm 可以直接安装的话，尽量不要通过此种方式 安装示例我们将通过安装一个 openresty 源代码包来作为示例，具体步骤如下： 下载源代码包 使用命令 wget https://openresty.org/download/openresty-1.15.8.1.tar.gz 1234567root@iZm5ehzqow4ijp2ya2g2drZ ~]# wget https://openresty.org/download/openresty-1.15.8.1.tar.gz--2019-09-07 17:03:16-- https://openresty.org/download/openresty-1.15.8.1.tar.gz...2019-09-07 17:03:17 (34.3 MB/s) - “openresty-1.15.8.1.tar.gz” saved [4904182/4904182][root@iZm5ehzqow4ijp2ya2g2drZ ~]# ll-rw-r--r-- 1 root root 4904182 May 17 05:34 openresty-1.15.8.1.tar.gz 解压此源代码包 tar -zxvf openresty-VENSION.tar.gz 123456[root@iZm5ehzqow4ijp2ya2g2drZ ~]# tar -zxvf openresty-1.15.8.1.tar.gz [root@iZm5ehzqow4ijp2ya2g2drZ ~]# ls -lhtotal 53Mdrwxrwxr-x 5 1000 1003 4.0K May 17 05:27 openresty-1.15.8.1-rw-r--r-- 1 root root 4.7M May 17 05:34 openresty-1.15.8.1.tar.gz 进入解压好的文件夹，然后进行编译源代码 123456789[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# ls -lhtotal 108Kdrwxrwxr-x 46 1000 1003 4.0K May 17 05:27 bundle-rwxrwxr-x 1 1000 1003 52K May 17 05:27 configure-rw-rw-r-- 1 1000 1003 23K May 17 05:27 COPYRIGHTdrwxrwxr-x 2 1000 1003 4.0K May 17 05:27 patches-rw-rw-r-- 1 1000 1003 4.6K May 17 05:27 README.markdown-rw-rw-r-- 1 1000 1003 8.8K May 17 05:27 README-windows.txtdrwxrwxr-x 2 1000 1003 4.0K May 17 05:27 util 我们发现 configure 是一个绿色的文件，并且是一个可执行的文件，一般情况下我们下载下源代码编译好后，都是通过 make 和 make install 命令来安装源代码，所以在我们解压源代码后就进入目录看看是否有类似于此示例中 configure 这样的可执行文件，如果没有的话，可以阅读源码包中的README文件，来查看具体的安装编译方法 接下来就是执行此可执行文件，注意：一般在执行此可执行程序时，我们都会指定它的安装目录，使用 –prefix 参数执行即可，指定完目录之后就意味着之后安装程序全都是在这个指定的目录下 12345678[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# ./configure --prefix=/usr/local/openresty==== Building LuaJIT 2.1.0-beta3 ====gmake -C srcgmake[1]: cc: Command not found...gmake: *** [default] Error 2ERROR: failed to run command: gmake TARGET_STRIP=@: CCDEBUG=-g XCFLAGS=&apos;-DLUAJIT_ENABLE_LUA52COMPAT -DLUAJIT_ENABLE_GC64&apos; CC=cc PREFIX=/usr/local/openresty/luajit 执行之后我们发现有报错信息，这时候我们就需要解决此报错，此报错说 cc 命令找不到，既没有安装 gcc 软件包，那我们使用 yum 命令安装即可 1234[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# yum install gccLoaded plugins: fastestmirrorSetting up Install Process... 安装完成之后我们发现再次执行发现可以执行了，但是还是执行失败，失败报错缺少 PCRE 这个库，这时候我们还需要安装这个库 12345678910[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# ./configure --prefix=/usr/local/openrestyplatform: linux (linux)cp -rp bundle/ build..../configure: error: the HTTP rewrite module requires the PCRE library.You can either disable the module by using --without-http_rewrite_moduleoption, or install the PCRE library into the system, or build the PCRE librarystatically from the source with nginx by using --with-pcre=&lt;path&gt; option.ERROR: failed to run command: sh ./configure --prefix=/usr/local/openresty/nginx \\... 通过 yum 安装 PCRE 这个库，安装示例： 1234[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# yum install pcre-develLoaded plugins: fastestmirrorSetting up Install Process... 安装完成后再次执行此可执行文件，发现还是报错，说缺少 OpenSSL这个库： 12345678[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# ./configure --prefix=/usr/local/openresty..../configure: error: SSL modules require the OpenSSL library.You can either do not enable the modules, or install the OpenSSL libraryinto the system, or build the OpenSSL library statically from the sourcewith nginx by using --with-openssl=&lt;path&gt; option.ERROR: failed to run command: sh ./configure --prefix=/usr/local/openresty/nginx \\... 通过 yum 安装 OpenSSL 这个库 12345[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# yum install openssl-develLoaded plugins: fastestmirrorSetting up Install ProcessLoading mirror speeds from cached hostfile... 上述示例中：-devel表示的都是开发库 安装完成后再次执行此文件，发现终于可以执行成功了 12345678root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# ./configure --prefix=/usr/local/openrestyplatform: linux (linux)cp -rp bundle/ build...cd ../..Type the following commands to build and install: gmake gmake install 执行完成后我们发现终于给了提示，说可以使用 gmake 和 gmake install 进行编译安装此软件包 通常我们都是使用 make 和 make install 进行编译安装，这是提示使用 gmake 和 gmake install，gmake 和 gmake install 是方便我们跨平台的进行编译安装的命令，所以我们这里使用 gmake 和 gmake install 这两个命令进行编译安装，此两种命令都可以使用 编译 首先使用 gmake 编译此源码包，可以使用 -j2 参数进行指定使用两个cpu进行编译，效率会更快 1234567[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# gmake -j2...gmake[2]: Leaving directory `/root/openresty-1.15.8.1/build/nginx-1.15.8&apos;gmake[1]: Leaving directory `/root/openresty-1.15.8.1/build/nginx-1.15.8&apos;[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# lsbuild bundle configure COPYRIGHT Makefile patches README.markdown README-windows.txt util 编译完成之后，我们发现源代码目录文件中多了一个 build 文件夹，这里面就存放了编译之后的文件及配置文件 安装 使用 gmake install 命令，将 build 文件中的所有文件都安装到我们之前指定的目录中 注意还是在原来的目录中，不需要进去其他目录 12345[root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# gmake installcd /root/openresty-1.15.8.1/build/LuaJIT-2.1-20190507 &amp;&amp; gmake TARGET_STRIP=@: CCDEBUG=-g XCFLAGS=&apos;-std=gnu99 -DLUAJIT_ENABLE_LUA52COMPAT -DLUAJIT_ENABLE_GC64 -msse4.2&apos; CC=cc PREFIX=/usr/local/openresty/luajit...mkdir -p /usr/local/openresty/site/lualib /usr/local/openresty/site/pod /usr/local/openresty/site/manifestln -sf /usr/local/openresty/nginx/sbin/nginx /usr/local/openresty/bin/openresty 执行完成后，就已经安装完成了，可以查看我们之前的指定目录，查看安装后的文件内容 12345678910root@iZm5ehzqow4ijp2ya2g2drZ openresty-1.15.8.1]# ll /usr/local/openresty/total 272drwxr-xr-x 2 root root 4096 Sep 7 18:20 bin-rw-r--r-- 1 root root 22924 Sep 7 18:20 COPYRIGHTdrwxr-xr-x 6 root root 4096 Sep 7 18:20 luajitdrwxr-xr-x 6 root root 4096 Sep 7 18:20 lualibdrwxr-xr-x 6 root root 4096 Sep 7 18:20 nginxdrwxr-xr-x 47 root root 4096 Sep 7 18:20 pod-rw-r--r-- 1 root root 226376 Sep 7 18:20 resty.indexdrwxr-xr-x 5 root root 4096 Sep 7 18:20 site 只有再安装完成后，才可以看到此目录及文件，这样就完成了通过源代码安装软件啦 内核升级uname命令Linux uname命令用于显示系统信息，uname可显示电脑以及操作系统的相关信息 使用方式：uname [选项] 参数说明 -a 显示全部的信息 -m 显示电脑类型 -n 显示在网络上的主机名称 -r 显示操作系统的发行编号 -s 显示操作系统名称 -v 显示操作系统的版本 使用示例： 12[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# uname -aLinux iZm5ehzqow4ijp2ya2g2drZ 2.6.32-696.16.1.el6.x86_64 #1 SMP Wed Nov 15 16:51:15 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux df命令Linux df命令用于显示目前在Linux系统上的文件系统的磁盘使用情况统计 使用方法： df [选项] 参数说明 -h -human-readable 使用人类可读的格式(预设值是不加这个选项的…) -t -type=TYPE 限制列出文件系统的 TYPE -i 输出显示inode信息而非块使用量 使用示例： 1234[root@iZm5ehzqow4ijp2ya2g2drZ boot]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 2.4G 35G 7% /tmpfs 499M 0 499M 0% /dev/shm lscpu命令查看当前主机CPU状况的命令，使用示例： 123456789101112131415161718192021222324[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 1On-line CPU(s) list: 0Thread(s) per core: 1Core(s) per socket: 1Socket(s): 1NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 85Model name: Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHzStepping: 4CPU MHz: 2500.006BogoMIPS: 5000.01Hypervisor vendor: KVMVirtualization type: fullL1d cache: 32KL1i cache: 32KL2 cache: 1024KL3 cache: 33792KNUMA node0 CPU(s): 0 top命令类似于Windows的任务管理器，使用示例： 12345678910111213[root@iZm5ehzqow4ijp2ya2g2drZ balyu]# toptop - 15:03:02 up 7 days, 5:18, 2 users, load average: 0.00, 0.00, 0.00Tasks: 79 total, 1 running, 78 sleeping, 0 stopped, 0 zombieCpu(s): 0.3%us, 0.3%sy, 0.0%ni, 99.0%id, 0.3%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1019980k total, 705416k used, 314564k free, 131904k buffersSwap: 0k total, 0k used, 0k free, 209692k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1407 root 10 -10 121m 11m 9176 S 0.3 1.2 30:52.20 AliYunDun 1 root 20 0 19228 1500 1232 S 0.0 0.1 0:00.71 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root RT 0 0 0 0 S 0.0 0.0 0:00.00 migration/0 4 root 20 0 0 0 0 S 0.0 0.0 0:00.62 ksoftirqd/0 可以看到能查询到CPU及内存等的使用状态，以及当前进程等 扩展软件仓库很多时候我们发现CentOS默认提供的软件仓库中并没有我们需要的软件，或者软件版本没有一些最新的，比如下面我们要说的内核，这时候我们就需要去别的仓库去找，所以这里我们介绍扩展软件仓库 一般比较流行的软件仓库是 epel 仓库，我们只需要通过命令来安装这个仓库即可，命令如下： 1yum install epel-release -y 升级内核以上我们已经知道了如何查看自己主机当前的内核版本，现在我们就可以对它进行升级了 使用yum命令升级第一种方式是指定内核的版本进行升级，我们可以去内核的官网查看我们需要升级的内核版本，然后使用yum命令进行升级，具体命令如下： 1yum install kernel-3.10.0 注意上面我们只使用 yum install kernel 命令，不指定具体的版本号的时候，yum就会去仓库寻找最新的 kernel 版本来进行安装 第二种方式是自动进行升级，它可以直接对我们所有安装的软件及内核进行版本检测，检测到有最新版本的话就会自动升级，具体使用如下： 1yum update 使用源代码编译方式安装升级 源代码编译的方式比较复杂，上面我们已经说过了，需要解决各种各样的依赖，以下是大神总结出来的所有的依赖： 12安装依赖包yum install gcc gcc-c++ make ncurses-devel openssl-devel elfutils-libelf-devel 接下来需要下载并解压缩内核软件包，下载地址：www.kernel.org，选择好我们要安装的内核版本并下载，然后解压缩就可以安装了 配置内核编译参数 配置内核编译参数和我们上述介绍的./configure不一样，需要进入到内核的软件包指定目录中，然后使用 make 命令来配置。具体如下： 12cd /usr/src/kernals/linux-5.1.1.0/make menuconfig | allyelsconfig | allnoconfig 上述介绍的 ./configure 是有很多的自动化配置在里面，但是我们在安装内核的时候，这些配置都需要我们自己手动来完成，所以这里需要使用 make – 命令来进行配置： make menuconfig 我们自己来根据弹出的菜单选项来进行配置 make allyelconfig 无脑设置，既有的功能全部都配置上 make allnoconfig 无脑设置，既什么功能都不配置，这样有可能会出现什么都不安装，导致安装后启动都启动不了 我们还可以使用当前的系统配置来进行配置，这里只需要将我们原先内核的配置文件拷贝到软件包的指定目录下，并且重命名为 .config 就可以使用原来内核的配置，这样就可以减少我们配置的复杂度，具体命令如下： 1cp /boot/config-kernelversion.platfrom /uer/src/kernels/linux-5.1.10/.config 注意上述命令示例中 kernelversion.platfrom 是需要替换成我们本地的文件，形式：config-2.6.32-696.16.1.el6.x86_64 编译 编译软件包与上述源代码编译一样，使用make命令直接编译即可，如下： 1make -j2 all 安装 安装与上述源代码安装不一样，多了一个步骤，我们在安装内核的时候需要先安装内核所支持的模块，然后再安装内核，具体命令如下： 123make modules_installmake install 这里我们就将内核安装升级完成了 grub配置文件升级完内核时我们需要设置启动引导软件来设置默认的内核，CentOS7使用的是gurb2版本，CentOS使用的是一版本，在一版本中，我们什么样的配置都需要自己手动去编辑，而且需要向设置网卡一样，记住每一项的功能，而二版本给我们提供了很多方便的工具，我们需要修改配置的时候只需要通过命令修改即可。所以grub2版本就不需要我们去背每一个设置项及内容了。 配置文件具体说明配置文件所在的位置：/boot/grub2/grub.cfg，由于grub2程序的特性，我们一般不要去直接编辑此文件，因为此文件可以说是一个内存级的配置文件，如果我们再通过一些其他的手段去修改了默认的配置，那么我们的的编辑就会丢失及消失 所以我们通常如果想要修改配置，会去修改：/etc/default/grub 这个文件，这个是一些基本的配置的配置文件，如果我们想修改更为详细的配置，我们可以去：/etc/grud.d/ 此文件目录下的一些其他的配置文件，此文件是从 00、01 … 等一直向后排序的 当我们修改完成后，通过命令：grub2-mkconfig-o /boot/gurb2/grub.cfg 就可以产生新的配置文件了 /etc/default/grub这个文件是修改默认的一些设置，这里面一般我们只需要关注两个配置项即可： GRUB_DEFAULT=saved GRUB_CMDLINE_LINUX=”rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet biosde vname=0 net.ifnames=0” 修改默认启动引导内核当我们需要修改默认的启动引导内核，就需要依赖上述第一项，我们需要通过命令：grub2-editenv list 能查看到当前系统启动的引导内核是什么版本的， 然后我们需要通过命令：grub2 ^menu /boot/default/grub 来查看当前我们系统中安装了哪些内核版本及顺序，我们找到所有以 menuentry 开头的行，它后边包含的内容就是我们安装的内核版本信息，我们需要找到我们需要修改的内核的顺序，第一个从0开始，然后记录。然后我们通过命令：grub2-set-default 顺序编号 来设置我们默认的系统引导内核。 当修改完成后，我们再来通过 grub2-editenv list 命令查看当前系统启动引导内核时，发现就成为了我们设置的内核（saved_entry=顺序编号），这时我们重新启动电脑，发现重启后默认的启动内核就变成了我们所设置的内核 启动项配置上述描述中第二项就是我们需要关注的，修改网卡名称所要用到的一项，此项中我们经常需要修改的是两个项： 1GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet biosde vname=0 net.ifnames=0&quot; 上述中rhgb：此项表示引导的时候使用一个图形界面，我们看到启动时的进度条，也是当启动出现问题的将此项去掉已查看启动时更为详细的信息 上述中quiet：此项为静默模式，表示引导的时候只是打印一些必要的消息，当我们发现启动出现异常的时候，我们会将此项去掉，已打印更为全面的消息来定位问题","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"软件包管理器和内核升级","slug":"软件包管理器和内核升级","permalink":"https://jjw-story.github.io/tags/软件包管理器和内核升级/"}],"author":"JJW"},{"title":"网络配置","slug":"网络配置","date":"2019-08-12T12:00:00.000Z","updated":"2019-08-20T12:06:13.128Z","comments":true,"path":"2019/08/12/网络配置/","link":"","permalink":"https://jjw-story.github.io/2019/08/12/网络配置/","excerpt":"","text":"网络配置命令设置网卡IP地址ifconfig命令我们可以使用ifconfig命令来设置网卡的ip地址，使用方法: ifconfig 接口 IP地址 [netmask 子网掩码] 使用示例： 123456789101112131415161718192021222324root@CHJ-20190520VPS:/# ifconfigeth3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.31.34.113 netmask 255.255.255.240 broadcast 172.31.34.127 inet6 fe80::4c47:3429:b72c:5130 prefixlen 64 scopeid 0x0&lt;global&gt; ether 7e:15:d8:27:73:38 (Ethernet) ...# 修改iproot@CHJ-20190520VPS:/# ifconfig eth3 172.31.34.118root@CHJ-20190520VPS:/# ifconfigeth3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.31.34.113 netmask 255.255.255.240 broadcast 172.31.34.127 inet6 fe80::4c47:3429:b72c:5130 prefixlen 64 scopeid 0x0&lt;global&gt; ether 7e:15:d8:27:73:38 (Ethernet) ...# 修改ip并指定子网掩码root@CHJ-20190520VPS:/# ifconfig eth3 172.31.34.118 netmask 255.255.255.0root@CHJ-20190520VPS:/# ifconfigeth3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.31.34.118 netmask 255.255.255.0 broadcast 172.31.34.127 inet6 fe80::4c47:3429:b72c:5130 prefixlen 64 scopeid 0x0&lt;global&gt; ether 7e:15:d8:27:73:38 (Ethernet) ... 启动和关闭网卡启动和关闭网卡动作和我们Windows中的是一样的，这两个操作其实是在一些特殊情况来使用网卡操作的命令，一般是网卡的配置被改乱了，我们希望恢复到默认的配置，可能需要这两个命令，一般情况下我们是不需要使用这两个命令的 启动命令以下都分别对用两种工具包的命令，使用哪一种都可以 ifconfig 接口 up ifup 接口 关闭命令ipconfig 接口 down ifdown 接口 网关配置命令添加网关网段使用route add命令，使用方法： route add [-net | -host] 目的网络或主机 gw 网关ip route add [-net | -host] dev 网卡接口 route add -net 指定网段 netmask 子网掩码 eth0 route add -net 指定网段 netmask 子网掩码 gw 网关ip -net是指定网段，-host是指定ip 使用示例： 12345678910111213141516171819202122232425262728293031323334353637383940[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 添加主机路由[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route add -host 192.168.1.2 dev eth0[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.1.2 * 255.255.255.255 UH 0 0 0 eth0172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 添加网络路由 10.20.30.40 ~ 255.255.255.248[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route add -net 10.20.30.40 netmask 255.255.255.248 eth0[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.1.2 * 255.255.255.255 UH 0 0 0 eth010.20.30.40 * 255.255.255.248 U 0 0 0 eth0172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 添加默认路由 default ~ 192.168.1.2[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route add default gw 192.168.1.2[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.1.2 * 255.255.255.255 UH 0 0 0 eth010.20.30.40 * 255.255.255.248 U 0 0 0 eth0172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 192.168.1.2 0.0.0.0 UG 0 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0 删除网关网段使用route del命令，使用方法： route del [-net | -host] 目的网络或主机 gw 网关ip route del [-net | -host] dev 网卡接口 route del -net 指定网段 netmask 子网掩码 eth0 route del -net 指定网段 netmask 子网掩码 gw 网关ip 使用示例： 123456789101112131415161718192021222324252627282930313233343536373839[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.1.2 * 255.255.255.255 UH 0 0 0 eth010.20.30.40 * 255.255.255.248 U 0 0 0 eth0172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 192.168.1.2 0.0.0.0 UG 0 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 删除主机路由[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route del -host 192.168.1.2 dev eth0[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.20.30.40 * 255.255.255.248 U 0 0 0 eth0172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 192.168.1.2 0.0.0.0 UG 0 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 删除默认路由[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route del -net default gw 192.168.1.2[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.20.30.40 * 255.255.255.248 U 0 0 0 eth0172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 删除网络路由[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route del -net 10.20.30.40 netmask 255.255.255.248 eth0[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0 修改网关地址如果我们要修改默认网关，需要先把网关删除，然后再进行添加 使用示例： 将 169.254.0.0 的默认网关修改为 192.168.1.2 1234567891011121314151617181920212223[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.1.2 0.0.0.0 255.255.255.255 UH 0 0 0 eth0172.31.240.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth00.0.0.0 169.254.0.0 0.0.0.0 UG 0 0 0 eth00.0.0.0 172.31.255.253 0.0.0.0 UG 0 0 0 eth0# 先删除[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route del -net default gw 169.254.0.0# 然后添加[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route add default gw 192.168.1.2[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.1.2 0.0.0.0 255.255.255.255 UH 0 0 0 eth0172.31.240.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth00.0.0.0 192.168.1.2 0.0.0.0 UG 0 0 0 eth00.0.0.0 172.31.255.253 0.0.0.0 UG 0 0 0 eth0","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"网络管理","slug":"网络管理","permalink":"https://jjw-story.github.io/tags/网络管理/"}],"author":"JJW"},{"title":"Djongo-03","slug":"Djongo-03","date":"2019-08-11T05:30:43.000Z","updated":"2019-08-11T12:01:02.866Z","comments":true,"path":"2019/08/11/Djongo-03/","link":"","permalink":"https://jjw-story.github.io/2019/08/11/Djongo-03/","excerpt":"","text":"通用视图通用视图介绍通用视图把视图开发中常用的写法和模式抽象出来，让你编写少量代码就能快速实现常见的数据视图。显示对象列表就是这样一种任务。 有了通用视图，可以把模型作为额外的参数传给 URL 配置。Django 自带的通用视图能实现下述功能： 列出对象并显示单个对象的详细信息。如果创建一个管理会议的应用程序，那么TalkListView 和RegisteredUserListView就是列表视图。某一个演讲的页面就是详细信息视图。 呈现基于日期的对象，显示为年月日归档页面（带有详细信息），以及“最新”页面。 让用户创建、更新和删除对象——需不需要授权都行。 具体使用示例及说明示例展示的是一个查询出所有厂家及根据URl中传入的厂家名称查询出此厂家生产的所有商品的示例： 视图函数如下(在项目view.py下定义)： 12345678910111213141516171819202122232425262728293031323334353637from cerealsOils.models import Manufacturers, Productfrom django.views.generic import ListViewfrom django.shortcuts import get_object_or_404# Create your views here.# 定义通用视图class ManufacturersList(ListView): # 注意此行代码表示：其实是queryset = Publisher.objects.all() 的简洁形式。 # model = Manufacturers # 动态过滤 # 根据 URL 中指定的键过滤列表页面中的对象 # 我们可以覆盖ListView 的 get_queryset() 方法。它的默认实现是返回queryset 属性的值，不过我们可以添加更多逻辑 # 这里根据url中传入的厂家名称，过滤出指定厂家生产的产品 def get_queryset(self): # 获取到GET请求捕获到的参数 self.manu = get_object_or_404(Manufacturers, name=self.args[0]) # 过滤 return Product.objects.filter(manufacturers=self.manu) # 提供“友好的”模板上下文，如果我们不自已定义模板上下文名称，默认会将上述查询结果存储在名为 object_list 的变量中 # 现在我们将它存储在 manu_list 变量中 context_object_name = &quot;product_list&quot; # 自定义了模板名称，既指定使用的模板 # 如果没明确指定，Django 将从对象的名称中推知。这里，推知的模板是cerealsOils/manufacturers_list.html template_name = &apos;manu_list.html&apos; # 提供额外的上下文变量 # 就是扩展DetailView，自己实现get_context_data 方法，然后在此方法提供额外的数据 # 这里查询出所有的厂家返回给页面 def get_context_data(self, **kwargs): # 先调用原来的实现，获取上下文 context = super(ManufacturersList, self).get_context_data(**kwargs) # 查询出所有的产品 context[&apos;manu_list&apos;] = Manufacturers.objects.all() return context url定义如下： 1234urlpatterns = [ # 注意 as_view() 函数 url(r&apos;^manulist/([\\w-]+)$&apos;, views.ManufacturersList.as_view())] 模板代码如下： 1234567891011121314151617181920&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot;&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&apos;UTF-8&apos;&gt; &lt;title&gt;通用视图测试&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;厂家名称&lt;/h1&gt; &#123;% for manu in manu_list %&#125; &lt;li&gt;&#123;&#123; manu.name &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;hr&gt; &lt;h1&gt;产品名称&lt;/h1&gt; &#123;% for prod in product_list %&#125; &lt;li&gt;&#123;&#123; prod.title &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;hr&gt; &lt;p&gt;谢谢光临&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 以上便是通用视图的实现，是不是很方便呢！","categories":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/tags/Python/"},{"name":"Django","slug":"Django","permalink":"https://jjw-story.github.io/tags/Django/"}],"author":"JJW"},{"title":"网络状态查看","slug":"网络状态查看","date":"2019-08-08T12:20:20.000Z","updated":"2019-08-03T11:13:31.476Z","comments":true,"path":"2019/08/08/网络状态查看/","link":"","permalink":"https://jjw-story.github.io/2019/08/08/网络状态查看/","excerpt":"","text":"网络状态查看网络状态查看我们列举两套工具包，一套是net-tools，一套是iproute或者有时候也叫iproute2 使用两套工具的作用是，在CentOS7以前，我们一般使用的都是net-tools工具包，而在CentOS7以后，主推的使用iproute工具包 ifconfig查看网络状态既网卡状态，使用方法：ifconfig [网卡名称] 注意网卡名称是可选的，管理用户直接输入命令，普通用户需要 /sbin/ifconfig(注意普通用户需要加上命令的完整路径) 一般使用此命令查询出来的结果中，会显示etho，第一块网卡的状态信息，这个名字是默认的，但是有可能我们查询出来不叫这个名字，这是因为在CentOS7中使用了一致性网络设备命名，它会先去检测我们的网卡，检测后具体命名如下： en01 板载网卡 ens33 PCI-E网卡 enp0s3 无法获取物理信息的PCI-E网卡 eth0 如果以上都获取不到，会使用此命名 使用示例： 123456789101112131415161718wangjia3@CHJ-20190520VPS:~$ ifconfigeth3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.31.34.113 netmask 255.255.255.240 broadcast 172.31.34.127 inet6 fe80::4c47:3429:b72c:5130 prefixlen 64 scopeid 0x0&lt;global&gt; ether 7e:15:d8:27:73:38 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 1500 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x0&lt;global&gt; loop (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 我们还可以使用命令：ifconfig 网卡名称 来查看指定网卡，例如我们只查看lo网卡的信息： 以上查询结果我们需要关注的是： etho信息中的 inet选项，第一个参数addr是IP信息，Mask参数对应的是子网掩码，还要注意 有个 ether参数，它显示的是mac地址，RX、TX：发送数据包的个数及多少 lo网卡信息中：lo网卡表示的是本地环回，它的地址永远是127.0.0.1，这个网卡的作用就是我们在自己本地搭建了一个服务，在我们访问自己主机服务的时候，就使用这个IP 123456789wangjia3@CHJ-20190520VPS:~$ ifconfig lolo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 1500 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x0&lt;global&gt; loop (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 修改网卡名称有时候我们在管理Linux网卡的时候，可能会有很多机器，我们需要写一个通用的脚本来修改，但是我们多个机器的网卡名称不一样，那么我们就需要手动的修改网卡的名称，来保证多个机器的网卡名称保持一致，这样能便于管理，下面我们介绍的就是修改网卡名称的方法，分为两步： 第一步： 网卡命名规则受 biosdevname和net.ifnames两个参数影响，这两个参数我们需要编辑 /etc/defult/grub 文件来修改，为此文件增加如下两个参数： biosdevname=0 net.ifnames=0 第二步： 更新grup，上述编辑好的文件并不会在系统启动的时候被读取到，所以我们需要将上述文件转化为系统启动可读取的文件，使用命令： grub2-mkconfig -o /boot/grup2/grup.cfg 第三步： 重启，重启命令是reboot，重启完成之后，网卡名称就变成了 eth0 biosdevname和net.ifnames组合明细： 序列 biosdevname net.ifnames 网卡名 默认 0 1 ens33 组合1 1 0 em1 组合2 0 0 eth0 查看网卡物理连接情况查看网卡物理连接情况，比如查看网线是否连接好，Windows可以直接通过图形界面来查看，Linux需要用过命令来查看 mii-tool命令 使用方法：mii-tool [网卡名称] 注意CentOS7及以上使用此命令，网卡名称是必须要有的 12345678910111213141516root@CHJ-20190520VPS:/# mii-toolNo interface specifiedusage: mii-tool [-VvRrwl] [-A media,... | -F media] [-p addr] &lt;interface ...&gt; -V, --version display version information -v, --verbose more verbose output -R, --reset reset MII to poweron state -r, --restart restart autonegotiation -w, --watch monitor for link status changes -l, --log with -w, write events to syslog -A, --advertise=media,... advertise only specified media -F, --force=media force specified media technology -p, --phy=addr set PHY (MII address) to reportmedia: 1000baseTx-HD, 1000baseTx-FD, 100baseT4, 100baseTx-FD, 100baseTx-HD, 10baseT-FD, 10baseT-HD, (to advertise both HD and FD) 1000baseTx, 100baseTx, 10base 也可以使用：ethtool命令 使用方法：ethtool 网卡名称 123[root@iZm5ehzqow4ijp2ya2g2drZ etc]# ethtool eth0Settings for eth0: Link detected: ye 查看网关命令当我们机器需要网络通信的时候，需要连接其它的网络地址范围的时候，我们就需要配置网关，也叫配置路由 使用route命令来查看网关，使用方法：route [参数] 参数：-n 如果我们单独只用route命令时，默认使用时每一个IP都会反解成主机名，这个过程很慢，所以可以使用此参数可以不解析主机名 使用示例： 12345678910111213[root@iZm5ehzqow4ijp2ya2g2drZ etc]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.31.240.0 * 255.255.240.0 U 0 0 0 eth0link-local * 255.255.0.0 U 1002 0 0 eth0default 172.31.255.253 0.0.0.0 UG 0 0 0 eth0[root@iZm5ehzqow4ijp2ya2g2drZ etc]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.31.240.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth00.0.0.0 172.31.255.253 0.0.0.0 UG 0 0 0 eth0 输出项说明： 输出项 说明 Destination 目标网段或者主机 Gateway 网关地址，”*” 表示目标是本主机所属的网络，不需要路由 Genmask 网络掩码 Flags 标记。一些可能的标记如下： - U — 路由是活动的 - H — 目标是一个主机 - G — 路由指向默认网关 - R — 恢复动态路由产生的表项 - D — 由路由的后台程序动态地安装 - M — 由路由的后台程序修改 - ! — 拒绝路由 Metric 路由距离，到达指定网络所需的中转数（linux 内核中没有使用） Ref 路由项引用次数（linux 内核中没有使用） Use 此路由项被路由软件查找的次数 Iface 该路由表项对应的输出接口 主机路由：主机路由是路由选择表中指向单个IP地址或主机名的路由记录。主机路由的Flags字段为H。例如，在下面的示例中，本地主机通过IP地址192.168.1.1的路由器到达IP地址为10.0.0.10的主机 12Destination Gateway Genmask Flags Metric Ref Use Iface10.0.0.10 192.168.1.1 255.255.240.0 UH 0 0 0 eth0 网络路由：网络路由是代表主机可以到达的网络。网络路由的Flags字段为N。例如，在下面的示例中，本地主机将发送到网络192.19.12.20的数据包转发到IP地址为192.168.1.1的路由器 12Destination Gateway Genmask Flags Metric Ref Use Iface192.19.12.20 192.168.1.1 255.255.240.0 UN 0 0 0 eth0 默认路由：当主机不能在路由表中查找到目标主机的IP地址或网络路由时，数据包就被发送到默认路由（默认网关）上。默认路由的Flags字段为G。例如，在下面的示例中，默认路由是IP地址为192.168.1.1的路由器 12Destination Gateway Genmask Flags Metric Ref Use Ifacedefault 192.168.1.1 0.0.0.0 UG 0 0 0 eth0","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"网络管理","slug":"网络管理","permalink":"https://jjw-story.github.io/tags/网络管理/"}],"author":"JJW"},{"title":"Django-01","slug":"Django-01","date":"2019-08-04T02:31:02.000Z","updated":"2019-08-05T09:23:05.959Z","comments":true,"path":"2019/08/04/Django-01/","link":"","permalink":"https://jjw-story.github.io/2019/08/04/Django-01/","excerpt":"","text":"Djongo入门Djongo介绍Django是一个开放源代码的Web应用框架，由Python写成。采用了MTV的框架模式，即模型M，视图V和模版T。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的，即是CMS（内容管理系统）软件。并于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。 Django是一个基于MVC构造的框架。但是在Django中，控制器接受用户输入的部分由框架自行处理，所以 Django 里更关注的是模型（Model）、模板(Template)和视图（Views），称为 MTV模式，各自职责如下： 层次 职责 模型（Model），即数据存取层 处理与数据相关的所有事务： 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。 模板(Template)，即表现层 处理与表现相关的决定： 如何在页面或其他类型文档中进行显示。 视图（View），即业务逻辑层 存取模型及调取恰当模板的相关逻辑。模型与模板的桥梁 Django 框架的核心组件及设计哲学： 对象关系映射 (ORM,object-relational mapping)：以Python类形式定义你的数据模型，ORM将模型与关系数据库连接起来，你将得到一个非常容易使用的数据库API，同时你也可以在Django中使用原始的SQL语句。 URL 分派：使用正则表达式匹配URL，你可以设计任意的URL，没有框架的特定限定。像你喜欢的一样灵活。 模版系统：使用Django强大而可扩展的模板语言，可以分隔设计、内容和Python代码。并且具有可继承性。 表单处理：你可以方便的生成各种表单模型，实现表单的有效性检验。可以方便的从你定义的模型实例生成相应的表单。 Cache系统：可以挂在内存缓冲或其它的框架实现超级缓冲 －－ 实现你所需要的粒度。 会话(session)，用户登录与权限检查，快速开发用户会话功能。 国际化：内置国际化系统，方便开发出多种语言的网站。 自动化的管理界面：不需要你花大量的工作来创建人员管理和更新内容。Django自带一个ADMIN site,类似于内容管理系统 Djongo安装Djongo安装分为三步： 安装 Python 安装 Python 虚拟环境 安装 Django 第一步不在赘述，注意使用Python3及以上版本 第二步安装虚拟环境的目的是为了解决电脑中的软件相互依赖，每个程序都要依赖某些其他程序，而且要找到运行其他软件的设置（环境变量），编写新软件程序时，可能（经常）要修改其他软件所需的依赖或环境变量，这一步可能会导致各种问题，因此要避免。 Python 虚拟环境能解决这个问题。它把软件所需的全部依赖和环境变量包装到一个文件系统中，与电脑中的其他软件隔离开。 直接使用命令：pip install virtualenv 即可安装完成。 安装完成后执行命令：virtualenv env_mysite为我们自己的项目创建虚拟环境，注意env_mysite可以自定义，执行完成后我们发现在我们电脑家目录下就创建好了一个 \\env_mysite 的文件夹，virtualenv 创建了一个完整的 Python 安装，它与其他软件是隔离开的，因此开发项目时不会影响系统中的其他软件。 我们需要激活此虚拟环境，使用命令：env_mysite\\scripts\\activate，激活后我们就可以使用此虚拟环境。 第三步安装Djongo，直接在虚拟环境下执行命令：pip install django==1.8.13，即可安装Djongo1.8.13版本完成 新建Djongo项目在虚拟环境中执行django-admin startproject mysite，如果不使用虚拟环境，也可以自己找到项目位置，执行此命令创建项目。命令执行完成后我们看到Djongo为我们创建了如下文件： 1234567mysite/ manage.py mysite/ __init__.py settings.py urls.py wsgi.py 外层的mysite/ 根目录是项目的容器。这个目录的名称对 Django 没有什么作用，你可以根据喜好重命名。 manage.py 是一个命令行实用脚本，可以通过不同的方式与 Django 项目交互。 内部的mysite/ 目录是项目的 Python 包。导入这里面的内容时要使用目录的名称（如mysite.urls）。 mysite/init.py 是一个空文件，目的是让 Python 把这个目录识别为 Python 包。 mysite/settings.py 是 Django 项目的设置/配置。 mysite/urls.py 是 Django 项目的 URL 声明，即 Django 驱动的网站的“目录”。 mysite/wsgi.py 是兼容 WSGI 的 Web 服务器的入口点，用于伺服项目。 这里说明一下settings文件，这是整个项目的配置和设置，我们发现Djongo框架本身已经帮我们创建好了一些项目，这是为常见场景所做的约定，具体看INSTALLED_APPS的配置（这里打开项目为大家讲解一下此文件的配置内容）： 1234567891011# Application definition# 注意：此处添加自己开发的应用INSTALLED_APPS = ( &apos;django.contrib.admin&apos;, # 管理后天 &apos;django.contrib.auth&apos;, # 身份验证系统 &apos;django.contrib.contenttypes&apos;, # 内容类型框架 &apos;django.contrib.sessions&apos;, # 会话框架 &apos;django.contrib.messages&apos;, # 消息框架 &apos;django.contrib.staticfiles&apos;, # 管理静态文件的框架 &apos;cerealsOils&apos;, # 自己开发的应用) 使用这些项目我们需要在数据库中创建这些项目所需的表，执行命令即可自动创建：python manage.py migratehibernate框架，通过实体类上使用注解，自动创建表。 这样我们的项目就创建完成了，使用命令：python manage.py runserver 就可以启动项目啦，默认端口为8000，我们访问就可以看到主页了。 视图和URl配置创建视图函数创建视图需要在项目目录下新创建一个view.py文件，此文件中我们定义函数，返回给视图我们要展示的内容，如下： 12345678910111213141516171819202122232425# 从django.http 模块中导入HttpResponse 类。导入这个类是因为后面的代码要使用from django.http import HttpResponse# 视图函数，传入参数：jango.http.HttpRequest对象实例，注意此参数是视图函数必须的且是第一位的参数# 静态内容def hello(request): # 返回 Hello World 给response return HttpResponse(&quot;Hello World&quot;)# 动态内容def currentDatetime(request): now = datetime.datetime.now() html = &quot;现在时间： % now return HttpResponse(html)# 动态urldef timeAhead(request, offset): try: offset = int(offset) except ValueError: # 如果传给int() 函数的值不能转换成整数（如字符串&quot;foo&quot;），Python 会抛出ValueError 异常 raise Http404() dt = datetime.datetime.now() + datetime.timedelta(hours=offset) html = &quot;In %s hour(s), it will be %s.&quot; % (offset, dt) return HttpResponse(html) 配置URl若想把视图函数与特定的 URL 对应起来，要使用 URL 配置（URLconf）。URL 配置相当于 Django 驱动的网站的目录。简单来说，URL 配置把 URL 映射到相应的视图函数上，具体配置及说明如下： 12345678910111213141516171819# 从django.conf.urls 模块中导入两个函数：include，用于导入另一个 URL 配置模块；url，使用正则表达式模式匹配浏览器中的 URL，把它映射到 Django 项目中的某个模块上。from django.conf.urls import include, url# 从django.contrib 模块中导入admin 函数。这个函数传给include 函数，加载 Django 管理后台的 URLfrom django.contrib import admin# url实例列表urlpatterns = [ # 注意：正则表达式字符串前面的&apos;r&apos; 字符。它的目的是告诉 Python，那是“原始字符串”，不要解释里面的反斜线 # 第一个参数是模式匹配字符串（一个正则表达式，稍后说明），第二个参数是模式使用的视图函数 # url(r&apos;^$&apos;, index), # 为根地址指定一个 URL 模式 url(r&apos;^admin/&apos;, include(admin.site.urls)), # 静态类容 url(r&apos;^hello/$&apos;, hello), # ^ 和 $ 开头和结尾匹配模式 去掉hello就可以作为根地址 # 动态内容 url(r&apos;^nowTime/$&apos;, currentDatetime), # 动态URl，Django 的核心哲学之一是，URL 应该美观，既符合REST风格，这里能匹配time/plus/2 time/plus/20 url(r&apos;^time/plus/(\\d&#123;1,2&#125;)/$&apos;, timeAhead), # 正则匹配] 处理请求过程运行python manage.py runserver 命令时，manage.py 脚本在内层mysite 目录中寻找名为settings.py 的文件。这个文件中保存着当前 Django 项目的全部配置，各个配置的名称都是大写的，然后找到如下配置： 12# 指向URl配置文件，寻找URl详细配置ROOT_URLCONF = &apos;mysite.urls&apos; 找到匹配的模式之后，调用对应的视图函数，并把一个HttpRequest 对象作为第一个参数传给视图,视图函数必须返回一个HttpResponse 对象。 随后，余下的工作交给 Django 处理：把那个 Python 对象转换成正确的 Web 响应，并且提供合适的 HTTP 首部和主体（即网页的内容）。综上： 请求/hello/。 Django 查看ROOT_URLCONF 设置，找到根 URL 配置。 Django 比较 URL 配置中的各个 URL 模式，找到与/hello/ 匹配的那个。 如果找到匹配的模式，调用对应的视图函数。 视图函数返回一个HttpResponse 对象。 Django 把HttpResponse 对象转换成正确的 HTTP 响应，得到网页。 Djongo模板Django 模板是一些文本字符串，作用是把文档的表现与数据区分开。模板定义一些占位符和基本的逻辑（模板标签），规定如何显示文档。通常，模板用于生成 HTML，不过 Django 模板可以生成任何基于文本的格式。 变量和模板标签123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141* &#123;&#123; xxx &#125;&#125; 指定变量的值插入这里* &#123;% xxx %&#125; 模板标签，基本模板标签有如下 # 逻辑判断标签 # 注意，此示例中使用了 if elif else and not or 等逻辑判断语句，与python保持一致 # if标签中不能包含括号，如果需要使用括号判断，必须写成嵌套 if 标签语句 &#123;% if a1 and a2 %&#125; &lt;p&gt;a1为真并且a2为真，显示此内容&lt;/p&gt; &#123;% elif not a3 %&#125; &lt;p&gt; else if a3 为假，显示此内容，注意elif是可选标签&lt;/p&gt; &#123;% elif a4 or not a5 %&#125; &lt;p&gt; else if a4 为真，或者 a5 为假，显示此内容，注意elif是可选标签&lt;/p&gt; &#123;% else %&#125; &lt;p&gt; else显示此内容，注意else是可选标签&lt;/p&gt; &#123;% endif %&#125; # 迭代元素标签 # 正向迭代 &#123;% for item in item_list %&#125; &lt;li&gt;&#123;&#123; item &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; # 反向迭代 &#123;% for item in item_list reversed %&#125; &lt;li&gt;&#123;&#123; item &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; # 定义迭代列表为空的行为 &#123;% for item in item_list %&#125; &lt;p&gt;&#123;&#123; item &#125;&#125;&lt;/p&gt; &#123;% empty %&#125; &lt;p&gt;此处定义要迭代的元素为空的行为&lt;/p&gt; &#123;% endfor %&#125; # 获取迭代当前元素索引 &#123;% for item in item_list %&#125; &lt;p&gt;&#123;&#123; forloop.counter &#125;&#125;: forloop.counter表示当前元素索引&lt;/p&gt; &lt;p&gt;&#123;&#123; forloop.revcounter &#125;&#125;: forloop.revcounter表示剩余元素数量&lt;/p&gt; &#123;% if forloop.first %&#125; &lt;p&gt;forloop.first返回的是boolean，表示是否第一个元素&lt;/p&gt; &#123;% elif forloop.last %&#125; &lt;p&gt;forloop.last返回的也是boolean，表示是否最后一个元素&lt;/p&gt; &#123;% endif %&#125; &#123;% endfor %&#125; # 比较值相等标签 &#123;% ifequal A B %&#125; &lt;h1&gt;A 的值和 B 的值相等!&lt;/h1&gt; &#123;% else %&#125; &lt;h1&gt;A 的值和 B 的值不相等!&lt;/h1&gt; &#123;% endifequal %&#125; # 比较值不相等标签 &#123;% ifnotequal A &apos;JJW&apos; %&#125; &lt;h1&gt;A 的值不等于JJW!&lt;/h1&gt; &#123;% else %&#125; &lt;h1&gt;A 的值等于JJW!&lt;/h1&gt; &#123;% endifnotequal %&#125; # 模板注释 # 单行注释 &#123;# 这里是一个注释 #&#125; # 多行注释，comment标签 &#123;% comment %&#125; 第一行注释 第二行注释 &#123;% endcomment %&#125; # 过滤器 # 模板过滤器是在显示变量之前调整变量值的简单方式 # 把文本转换成小写，然后再显示 &#123;&#123; name|lower &#125;&#125; # 获取列表中的第一个元素，然后将其转换成大写 &#123;&#123; my_list|first|upper &#125;&#125; # 显示bio 变量的前 30 个词 &#123;&#123; bio|truncatewords:&quot;30&quot; &#125;&#125; # 在反斜线、单引号和双引号前面添加一个反斜线。可用于转义字符串 &#123;&#123; value|addslashes &#125;&#125; # 返回值长度，如果是列表，返回列表元素个数 &#123;&#123; name|length &#125;&#125; # include 模板标签 这个标签的作用是引入另一个模板的内容。它的参数是要引入的模板的名称，可以是变量，也可以是硬编码的字符串（放在引号里，单双引号都行） &#123;% include &apos;nav.html&apos; %&#125; &#123;% include &quot;nav.html&quot; %&#125; # 以下是使用示例 &lt;html&gt; &lt;body&gt; &#123;% include &quot;includes/nav.html&quot; %&#125; &lt;h1&gt;&#123;&#123; title &#125;&#125;&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; # includes/nav.html &lt;div id=&quot;nav&quot;&gt; You are in: &#123;&#123; current_section &#125;&#125; &lt;/div&gt; # 模板继承 # 模板继承是指创建一个基底“骨架”模板，包含网站的所有通用部分，并且定义一些“块”，让子模板覆盖。 # 首先定义一个基地模板 # &#123;% block xxx %&#125;&#123;% endblock %&#125; 表示此标签内容可以被子模板重写 &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;&#123;% block title %&#125;&#123;% endblock %&#125;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;My helpful timestamp site&lt;/h1&gt; &#123;% block content %&#125;&#123;% endblock %&#125; &#123;% block footer %&#125; &lt;hr&gt; &lt;p&gt;Thanks for visiting my site.&lt;/p&gt; &#123;% endblock %&#125; &lt;/body&gt; &lt;/html&gt; # 然后定义子模板 # 子模板需要继承父模板 &#123;% extends &quot;base.html&quot; %&#125; # 重写依然要使用 &#123;% block xxx %&#125;&#123;% endblock %&#125; 标签 &#123;% extends &quot;base.html&quot; %&#125; &#123;% block title %&#125;Future time&#123;% endblock %&#125; &#123;% block content %&#125; &lt;p&gt; In &#123;&#123; hour_offset &#125;&#125; hour(s), it will be &#123;&#123; next_time &#125;&#125;. &lt;/p&gt; &#123;% endblock %&#125; 使用模板系统Django 系统经过配置后可以使用一个或多个模板引擎（如果不用模板，那就不用配置）。Django 自带了一个内置的后端，用于支持自身的模板引擎，即 Django Template Language（DTL），若想在 Python 代码中使用 Django 的模板系统，基本方式如下： 以字符串形式提供原始的模板代码，创建Template 对象。 在Template 对象上调用render() 方法，传入一系列变量（上下文）。返回的是完全渲染模板后得到的字符串，模板中的变量和模板标签已经根据上下文求出值了。 以上步骤我们都可以使用一个函数来完成，那就是render()函数，render() 的第一个参数是请求对象，第二个参数是模板名称，第三个参数可选，是一个字段，用于创建传给模板的上下文。如果不指定第三个参数，render() 使用一个空字典，具体使用如下介绍。 模板目录及模板加载为了从文件系统中加载模板，Django 提供了便利而强大的 API，力求去掉模板加载调用和模板自身的冗余。若想使用这个模板加载 API，首先要告诉框架模板的存储位置。这个位置在设置文件中配置，打开settings.py 文件，找到TEMPLATES 设置。它的值是一个列表，分别针对各个模板引擎： 123456789101112131415161718192021222324TEMPLATES = [ &#123; # BACKEND 的值是一个点分 Python 路径，指向实现 Django 模板后端 API 的模板引擎类 &apos;BACKEND&apos;: &apos;django.template.backends.django.DjangoTemplates&apos;, # DIRS 定义一个目录列表，模板引擎按顺序在里面查找模板源文件。 # 当前设置表示在项目根目录中放一些主模板，模板目录不一定非得叫&apos;templates&apos;，可以自定义 &apos;DIRS&apos;: [os.path.join(BASE_DIR, &apos;templates&apos;)], # 去应用所在位置查找模板，APPS_DIRS 设为True 时，DjangoTemplates 会在INSTALLED_APPS 中的各个应用里查找名为“templates”的子目录。这样，即使DIRS 为空，模板引擎还能查找应用模板。 &apos;APP_DIRS&apos;: True, # OPTIONS 是一些针对后端的设置 &apos;OPTIONS&apos;: &#123; # 默认的处理器上下文 &apos;context_processors&apos;: [ &apos;django.template.context_processors.debug&apos;, &apos;django.template.context_processors.request&apos;, &apos;django.contrib.auth.context_processors.auth&apos;, &apos;django.contrib.messages.context_processors.messages&apos;, ], &#125;, &#125;,] 查找模板逻辑 如果APP_DIRS 的值是True，而且使用 DTL，在当前应用中查找“templates”目录。 如果在当前应用中没找到模板，把传给它的模板名称添加到DIRS 中的各个目录后面，按顺序在各个目录中查找。假如DIRS 的第一个元素是’/home/django/mysite/templates’，调用查找的模板是/home/django/mysite/templates/current_datetime.html。 找不到指定名称对应的模板，抛出TemplateDoesNotExist 异常。 使用示例及说明视图函数如下定义： 123456from django.shortcuts import renderimport datetimedef currentDatetime(request): now = datetime.datetime.now() return render(request, &apos;test/current_datetime.html&apos;, &#123;&apos;current_date&apos;: now&#125;) 通过以上示例我们发现： 不用再导入get_template、Template、Context 或HttpResponse 了，而要导入django.shortcuts.render import datetime 不变。 在current_datetime 函数中，仍然要计算now，不过加载模板、创建上下文、渲染模板和创建HttpResponse对象全由render() 调用代替了。render() 的返回值是一个HttpResponse 对象，因此在视图中可以直接返回那个值。 注意上述我们还使用了模板子目录：test/current_datetime.html，表示去根目录下寻找templates目录下的test目录，找到模板。 Djongo模型Django 非常适合构建数据库驱动型网站，它提供了简单而强大的工具，易于使用 Python 执行数据库查询，本章就说明这个功能，即 Django 的数据库层。 配置数据库数据库连接的配置也在settings文件中，具体如下： 1234567891011121314151617181920# 数据库配置，这里使用集成的sqllite3，我们配置自己的数据库，类似于我们Java项目中的配置文件中配置即可DATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.sqlite3&apos;, # ENGINE 告诉 Django 使用哪个数据库引擎。 &apos;NAME&apos;: os.path.join(BASE_DIR, &apos;db.sqlite3&apos;), # NAME 告诉 Django 数据库的名称。 &#125;&#125;# 配置MySQL数据库# DATABASES = &#123;# &apos;default&apos;: &#123;# &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, # 或者使用 mysql.connector.django# &apos;NAME&apos;: &apos;test&apos;,# &apos;USER&apos;: &apos;test&apos;,# &apos;PASSWORD&apos;: &apos;test123&apos;,# &apos;HOST&apos;:&apos;localhost&apos;,# &apos;PORT&apos;:&apos;3306&apos;,# &#125;# &#125; 创建应用下面说明项目和应用的区别： 一个项目是一系列 Django 应用的实例，外加那些应用的配置。严格来说，一个项目唯一需要的是一个设定文件，定义数据库连接信息、安装的应用列表、DIRS，等等。 一个应用是一系列便携的 Django 功能，通常包含模型和视图。打包在一个 Python 包里。Django 自带了一些应用，例如管理后台。这些应用的独特之处是便携，可以在多个项目中复用。 在项目目录，注意是项目目录，使用如下命令创建新的应用，并指定自定义应用名称：python manage.py startapp 应用名称。 Django 通过模型在背后执行 SQL，返回便利的 Python 数据结构，表示数据库表中的行。Django 还通过模型表示 SQL 无法处理的高层级概念，这么做的好处如下： 内省（introspection）有开销，而且不完美。为了提供便利的数据访问 API，Django 需要以某种方式知晓数据库布局，而这一需求有两种实现方式。第一种是使用Python 明确描述数据，第二种是在运行时内省数据库，推知数据模型。第二种方式在一个地方存储表的元数据，看似更简单，其实会导致几个问题。首先，运行时内省数据库肯定有消耗。如果每次执行请求，或者只是初始化 Web 服务器都要内省数据库，那带来的消耗是无法接受的，其次，有些数据库，尤其是旧版 MySQL，存储的元数据不足以完成内省。 Python 编写起来让人心情舒畅，而且使用 Python 编写所有代码无需频繁让大脑切换情境。在一个编程环境（思维）中待久了，有助于提升效率。在 SQL 和 Python 之间换来换去容易打断状态。 把数据模型保存在代码中比保存在数据库中易于做版本控制，易于跟踪数据布局的变化。 SQL 对数据布局的元数据只有部分支持。例如，多数数据库系统没有提供专门表示电子邮件地址或URL 的数据类型。而 Django 模型有。高层级的数据结构有助于提升效率，让代码更便于复用。 不同数据库平台使用的 SQL 不一致。 分发 Web 应用程序时，更务实的做法是分发一个描述数据布局的 Python 模块，而不是分别针对MySQL、PostgreSQL 和 SQLite 的CREATE TABLE 语句。 创建自己的模型在我们新创建的应用的models.py文件中，添加自己定义的模型，如下： 让 Django 具有基本的数据访问能力只需编写这些代码。一般，一个模型对应于一个数据库表，模型中的各个属性分别对应于数据库表中的一列。属性的名称对应于列的名称，字段的类型（如CharField）对应于数据库列的类型(如varchar) 123456789101112131415161718192021222324252627282930313233343536# 厂家class Manufacturers(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50) city = models.CharField(max_length=60) province = models.CharField(max_length=30) website = models.URLField(blank=True, null=True, verbose_name=&apos;网址&apos;) # 表单允许为空 修改数据库结构，表示字段可为空 def __str__(self): return u&apos;%s&apos; % (self.name)# 经销商class Vender(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50) city = models.CharField(max_length=60) province = models.CharField(max_length=30) telephone = models.CharField(max_length=20) def __str__(self): return u&apos;%s %s %s %s %s&apos; % (self.name, self.address, self.city, self.province, self.telephone)# 产品class Product(models.Model): title = models.CharField(max_length=100) vender = models.ManyToManyField(Vender) # 多对多关系 manufacturers = models.ForeignKey(Manufacturers) # 一对多 product_date = models.DateField() # fields = (&apos;title&apos;, &apos;product_date&apos;, &apos;vender&apos;, &apos;manufacturers&apos;) def __str__(self): return u&apos;%s %s&apos; % (self.title, self.product_date) # 任何模型都可以使用Meta 类指定多个针对所在模型的选项。 class Meta: ordering = [&apos;product_date&apos;] 注意：多对多关系，在上述示例模型中，Django 会创建一个额外的表，一个多对多联结表（join table），处理Product与Vender之间的对应关系。 安装模型在settings文件中注册上我们自己创建的应用，注册位置已经在之前描述过，如下： 123456789101112# Application definition# 注意：此处添加自己开发的应用INSTALLED_APPS = ( &apos;django.contrib.admin&apos;, # 管理后台 &apos;django.contrib.auth&apos;, # 身份验证系统 &apos;django.contrib.contenttypes&apos;, # 内容类型框架 &apos;django.contrib.sessions&apos;, # 会话框架 &apos;django.contrib.messages&apos;, # 消息框架 &apos;django.contrib.staticfiles&apos;, # 管理静态文件的框架 &apos;cerealsOils&apos;, # 自己开发的应用) 注册好后就可以验证和安装模型： 验证模型使用命令：python manage.py check check 命令运行 Django 系统检查框架，即验证 Django 项目的一系列静态检查。如果一切正常，你将看到这个消息：System check identified no issues (0 silenced)，不然会抛出出错位置的异常 安装模型使用命令：python manage.py makemigrations cerealsOils，这样我们就安装好了模型，并根据模型定义创建好了数据库表 查看建表语句可以执行命令：python manage.py sqlmigrate cerealsOils 0001，这样建表语句就会输出出来 Djongo后台管理对多数现代的网站而言，管理界面是基础设施的重要组成部分。这是一个 Web 界面，限制只让授信的网站管理员访问，用于添加、编辑和删除网站内容。常见的示例有：发布博客的界面，网站管理人员审核用户评论的后端，客户用来更新新闻稿的工具。不过，管理界面有个问题：构建起来繁琐。构建面向公众的功能时，Web 开发是有趣的，但是管理界面一成不变，要验证用户身份、显示并处理表单、验证输入，等等。这个过程无趣、乏味。在 Django 中，构建管理界面不算个事， Django 为我们自动生成的管理界面，了解它为模型提供的便利界面，以及可以使用的其他功能。 使用管理后台之前创建项目时我们执行了django-admin startproject mysite命令时，Django 为我们创建并配置了默认的管理后台。我们只需创建一个管理员用户（超级用户），就可以登录管理后台。 创建管理用户：python manage.py createsuperuser 输入要创建的用户：Username: admin 输入邮箱地址：Email address: admin@example.com 输入用户密码，需要输入两次确认：Password: ** 启动服务器，访问ttp://127.0.0.1:8000/admin/就可以进入登录页面，登录后具体操作就可以被看懂了，这里不再赘述，可以启动项目为大家演示一下。 将我们创建的模型添加至后台管理在我们新创建的项目目录下创建admin.py文件，并添加如下代码，这样就将我们创建的模型添加至了后天管理系统： 1234567from django.contrib import adminfrom .models import Manufacturers, Vender, Product# 注册我们创建的模型admin.site.register(Manufacturers, ManufacturersAdmin)admin.site.register(Vender, VenderAdmin)admin.site.register(Product, ProductAdmin) 将字段定义为可选（非必填）及自定义字段标注默认我们创建的模型在后台管理页面都是必填字段，及显示的名称都是字段的英文名称，我们可以修改model，添加一下属性，来设置字段为非必填及显示的名称为可以看懂的名称，如下示例： 1234567891011# 厂家class Manufacturers(models.Model): name = models.CharField(max_length=30, verbose_name=&apos;厂家名称&apos;) address = models.CharField(max_length=50, verbose_name=&apos;具体地址&apos;) city = models.CharField(max_length=60, verbose_name=&apos;所在地市&apos;) province = models.CharField(max_length=30, verbose_name=&apos;所在省份&apos;) website = models.URLField(blank=True, null=True, verbose_name=&apos;网址&apos;) # 表单允许为空 修改数据库结构，表示字段可为空 def __str__(self): return u&apos;%s&apos; % (self.name)... 注：上述示例中：blank=True表示表单输入可为空， null=True表示数据库存值是如果是空也存为空，而不是字符”NULL”,verbose_name=’网址’表示自定义名称 自定义ModelAdmin类具体使用及属性设置明细见如下示例（此处可以打开后台管理界面给大家演示一下）： 123456789101112131415161718192021222324252627from django.contrib import adminfrom .models import Manufacturers, Vender, Product# Register your models here.class ProductAdmin(admin.ModelAdmin): list_display = (&apos;title&apos;, &apos;product_date&apos;, &apos;manufacturers&apos;) # 注意 此处不能包含多对多字段 search_fields = (&apos;title&apos;, &apos;product_date&apos;) # 添加列表搜索框内容，注意只有一个框，但输入内容后悔搜索多个字段，类似于ES的机制 list_filter = (&apos;product_date&apos;,) # 右侧的过滤条 可以是日期 布尔 一对一外键类型 date_hierarchy = &apos;product_date&apos; # 显示日期导航，注意只能添加一个导航 ordering = (&apos;-product_date&apos;,) # 列表根据日期降序排序 fields = (&apos;product_date&apos;, &apos;title&apos;, &apos;manufacturers&apos;, &apos;vender&apos;) # 自定义编辑表单，修改卡片页面排序，还可以排除字段，排除的字段将不能编辑 raw_id_fields = (&apos;manufacturers&apos;,) # 针对一对一内容太多而做的界面优化，一般为下拉框内容太多的优化 filter_horizontal = (&apos;vender&apos;,) # 针对一对多内容太多而做的通过弹出框筛选class VenderAdmin(admin.ModelAdmin): list_display = (&apos;name&apos;, &apos;address&apos;) search_fields = (&apos;name&apos;, &apos;address&apos;)class ManufacturersAdmin(admin.ModelAdmin): list_display = (&apos;name&apos;, &apos;address&apos;) search_fields = (&apos;name&apos;, &apos;address&apos;) # 将自定义的ModelAdmin注册到后台管理admin.site.register(Manufacturers, ManufacturersAdmin)admin.site.register(Vender, VenderAdmin)admin.site.register(Product, ProductAdmin)","categories":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/tags/Python/"},{"name":"Django","slug":"Django","permalink":"https://jjw-story.github.io/tags/Django/"}],"author":"JJW"},{"title":"Django-02","slug":"Django-02","date":"2019-08-04T02:31:02.000Z","updated":"2019-08-11T05:40:34.779Z","comments":true,"path":"2019/08/04/Django-02/","link":"","permalink":"https://jjw-story.github.io/2019/08/04/Django-02/","excerpt":"","text":"Djongo表单从请求对象中获取数据 属性 方法说明 示例 request.path 完整的路径，不含域名，但是含前导斜线 “/hello/” request.get_host() 主机名（即通常所说的“域名”） “127.0.0.1:8000”或“www.example.com” request.get_full_path() 包含查询字符串（如果有的话）的路径 “/hello/?print=true” request.is_secure() 通过 HTTPS 访问时为True，否则为False True 或False request.META[‘HTTP_USER_AGENT’] 入站前的 URL（可能没有） request.META[‘HTTP_USER_AGENT’] 浏览器的用户代理（可能没有），请求头信息 “Mozilla/5.0 (X11; U; Linux i686; fr-FR; rv:1.8.1.17) Gecko/20080829 Firefox/2.0.0.17” request.META[‘REMOTE_ADDR’] 客户端的 IP 地址 “12.345.67.89”。（如果请求经由代理，这个首部的值可能是一组 IP 地址，以逗号分隔） 简单的表单使用示例这里可以打开页面及项目代码为大家演示 视图函数如下： 123456789101112131415161718192021222324from django.shortcuts import renderfrom django.http import HttpResponsefrom cerealsOils.models import Product# Create your views here.def product_search(request): return render(request, &apos;product_search.html&apos;)def search(request): errors = [] if &apos;q&apos; in request.GET : # 通过request对象直接获取传统get请求的参数： q=xxx if ( request.GET[&apos;q&apos;]) : # 判断表单q参数是否为空 q = request.GET[&apos;q&apos;] if len(q) &gt; 5 : errors.append(&quot;查询关键字不能超过五个字符&quot;) else : products = Product.objects.filter(title__contains=q) # 过滤查询 # name = products[0].vender[0] return render(request, &apos;product_search.html&apos;, &#123;&apos;products&apos;: products, &apos;query&apos;: q&#125;) else : # products = Product.objects.all() # 查询所有 errors.append(&quot;查询关键字不能为空&quot;) return render(request, &apos;product_search.html&apos;, &#123;&apos;errors&apos;:errors&#125;) URL配置如下： 12url(r&apos;^product_search/$&apos;, views.product_search), # 查询页面跳转url(r&apos;^search/$&apos;, views.search), # 查询控制层 product_search模板如下： 1234567891011121314151617181920212223242526272829303132&lt;html&gt; &lt;head&gt; &lt;title&gt;产品查询&lt;/title&gt; &lt;meta charset=&apos;UTF-8&apos;&gt; &lt;/head&gt; &lt;body&gt; &#123;% if errors %&#125; &lt;ul&gt; &#123;% for error in errors %&#125; &lt;li&gt;&#123;&#123; error &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% endif %&#125; &lt;!-- &lt;form action=&quot;/search/&quot; method=&quot;get&quot;&gt; --&gt; &lt;form action=&quot;/search/&quot; method=&quot;get&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Search&quot;&gt; &lt;/form&gt; &lt;p&gt;查询关键字: &lt;strong&gt;&#123;&#123; query &#125;&#125;&lt;/strong&gt;&lt;/p&gt; &#123;% if products %&#125; &lt;p&gt;产品总数 &#123;&#123; products|length &#125;&#125; 产品&#123;&#123; books|pluralize &#125;&#125;&lt;/p&gt; &lt;ul&gt; &#123;% for product in products %&#125; &lt;li&gt;产品名称：&#123;&#123; product.title &#125;&#125; 生产日期：&#123;&#123; product.product_date &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% else %&#125; &lt;p&gt;没有查询到商品&lt;/p&gt; &#123;% endif %&#125; &lt;/body&gt;&lt;/html&gt; 这样就完成了一个表单的使用示例 联系表单Django 自带了一个表单库，django.forms，它能处理从显示 HTML 表单到验证。 这个表单框架的主要用法是为要处理的每个 HTML 表单定义一个Form 类，这个类可以放在任意位置，例如直接放在views.py 文件中，不过社区的约定是，把Form 类放在单独的forms.py 文件中。 下面对表单框架的各部分功能以及自定义校验规则等进行示例： 在views.py 文件所在的目录（mysite）中创建这个文件，然后输入下述内容： 12345678910111213141516# 引入框架的表单库from django import formsclass ContactForm(forms.Form): subject = forms.CharField(max_length=20, min_length=2) # 校验长度 email = forms.EmailField(required=False) # 选项表示表单内容非必填 message = forms.CharField(widget=forms.Textarea) # 可以直接设置表单样式，注意表单样式设置也可以很灵活，使用时具体查看资料 # 自定义校验规则 # 注意校验方法名，需要以clean_开头，字段名称为结尾 # 再检查此规则前定义字段时设置的校验规则已经校验完毕 def clean_message(self): message = self.cleaned_data[&apos;message&apos;] if &quot;香港&quot; in message: raise forms.ValidationError(&quot;消息内容不能包含敏感词&quot;) return message # 一定要显式的返回清理后的值，cleaned_data是清理值 定义视图函数： 1234567891011121314151617def contact(request): if request.method == &apos;POST&apos;: form = ContactForm(request.POST) # Post请求体接受为form表单对象，这点类似于mvc if form.is_valid(): # 校验表单输入是否符合定义的校验规则 cd = form.cleaned_data send_mail( cd[&apos;subject&apos;], cd[&apos;message&apos;], cd.get(&apos;email&apos;, &apos;noreply@example.com&apos;), [&apos;siteowner@example.com&apos;], ) return HttpResponseRedirect(&apos;/contact/thanks/&apos;) else: form = ContactForm( initial=&#123;&apos;subject&apos;: &apos;默认值&apos;&#125; # 设置form表单默认值 ) return render(request, &apos;test/contact_form.html&apos;, &#123;&apos;form&apos;: form&#125;) 定义模板文件： 123456789101112131415161718192021222324252627282930313233&lt;html&gt; &lt;head&gt; &lt;title&gt;联系表单&lt;/title&gt; &lt;meta charset=&apos;UTF-8&apos;&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;表单测试&lt;/h1&gt; &#123;% if form.errors %&#125; &lt;!-- 接受表单对象数据 --&gt; &lt;p style=&quot;color: red;&quot;&gt; 请处理表单错误内容&#123;&#123; form.errors|pluralize &#125;&#125; &lt;/p&gt; &#123;% endif %&#125; &lt;form action=&quot;&quot; method=&quot;post&quot;&gt; &lt;div class=&quot;field&quot;&gt; &#123;&#123; form.subject.errors &#125;&#125; &lt;label for=&quot;id_subject&quot;&gt;科目:&lt;/label&gt; &#123;&#123; form.subject &#125;&#125; &lt;/div&gt; &lt;div class=&quot;field&quot;&gt; &#123;&#123; form.email.errors &#125;&#125; &lt;label for=&quot;id_email&quot;&gt;输入您的邮箱:&lt;/label&gt; &#123;&#123; form.email &#125;&#125; &lt;/div&gt; &lt;div class=&quot;field&quot;&gt; &#123;&#123; form.message.errors &#125;&#125; &lt;label for=&quot;id_message&quot;&gt;消息内容:&lt;/label&gt; &#123;&#123; form.message &#125;&#125; &lt;/div&gt; &#123;% csrf_token %&#125; &lt;!-- 跨域处理 --&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 至此一个联系表单就创建完成了 高级视图和URL简化导入函数方式简化导入函数方式就是说我们直接导入views 模块自身，如下示例： 123456# 直接导入viewfrom cerealsOils import viewsurlpatterns = [ url(r&apos;^product_search/$&apos;, views.product_search), # 查询页面跳转 url(r&apos;^search/$&apos;, views.search), # 查询控制层 具名分组（Python具名函数的使用）正则表达式分组（通过括号实现）捕获 URL 中的片段， 123456urlpatterns = [ url(r&apos;^reviews/2003/$&apos;, views.special_case_2003), url(r&apos;^reviews/([0-9]&#123;4&#125;)/$&apos;, views.year_archive), url(r&apos;^reviews/([0-9]&#123;4&#125;)/([0-9]&#123;2&#125;)/$&apos;, views.month_archive), url(r&apos;^reviews/([0-9]&#123;4&#125;)/([0-9]&#123;2&#125;)/([0-9]+)/$&apos;, views.review_detail),] 如上捕获参数时，我们可以通过具名分组，为参数赋予名称： 123456urlpatterns = [ url(r&apos;^reviews/2003/$&apos;, views.special_case_2003), url(r&apos;^reviews/(?P&lt;year&gt;[0-9]&#123;4&#125;)/$&apos;, views.year_archive), url(r&apos;^reviews/(?P&lt;year&gt;[0-9]&#123;4&#125;)/(?P&lt;month&gt;[0-9]&#123;2&#125;)/$&apos;, views.month_archive), url(r&apos;^reviews/(?P&lt;year&gt;[0-9]&#123;4&#125;)/(?P&lt;month&gt;[0-9]&#123;2&#125;)/(?P&lt;day&gt;[0-9]&#123;2&#125;)/$&apos;,views.review_detail),] 如上示例含义如下： 对/reviews/2005/03/ 的请求调用views.month_archive(request, year=’2005’, month=’03’) 函数，而不是views.month_archive(request,’2005’,’03’)。 对/reviews/2003/03/03/ 的请求调用views.review_detail(request, year=’2003’, month=’03’,day=’03’) 函数。 匹配/分组算法URL 配置解析器解析正则表达式中具名分组和非具名分组所采用的算法如下： 如果有具名分组，使用具名分组，忽略非具名分组。 否则，以位置参数传递所有非具名分组。 不论如何，额外的关键字参数都会传给视图。 URL及视图一些特性 注意url的匹配规则，它不区分请求的类型，例如GET POST等，只要url一样，都交给同一个视图函数处理。 不管正则表达式匹配的是什么类型，捕获的每个参数都以普通的 Python 字符串传给视图。 例如： 1url(r&apos;^reviews/(?P&lt;year&gt;[0-9]&#123;4&#125;)/$&apos;, views.year_archive) 虽然[0-9]{4} 只匹配字符串中的整数，但是传给views.year_archive() 视图函数的year 参数是字符串，而不是整数。 可以为视图函数的参数指定默认值(利用的也是Python函数的特性) 例如： 12345678urlpatterns = [ url(r&apos;^reviews/$&apos;, views.page), # 使用默认参数 url(r&apos;^reviews/page(?P&lt;num&gt;[0-9]+)/$&apos;, views.page), # 这里有传值，不使用默认参数]# 视图（在 reviews/views.py 文件中）def page(request, num=&quot;1&quot;):# 输出指定数量的书评 性能问题：urlpatterns 中的每个正则表达式在首次访问时编译，因此系统的速度异常得快。 引入其他URl配置urlpatterns 在任何位置都可以“引入”其他 URL 配置模块。通过这一行为可以把一些 URL 放在另一些名下。 例如： 123456urlpatterns = [ # ... url(r&apos;^community/&apos;, include(&apos;django_website.aggregator.urls&apos;)), url(r&apos;^contact/&apos;, include(&apos;django_website.contact.urls&apos;)), # ...] 注意，这里的正则表达式没有$（匹配字符串末尾的符号），但是末尾有斜线。Django 遇到include() 时，会把截至那一位置匹配的 URL 截断，把余下的字符串传给引入它的 URL 配置，做进一步处理。 可以利用这一点将公共的路径提取出来，示例如下： 1234567891011注意，url的配置可以抽取共性配置urlpatterns = [ url(r&apos;^chehejia/&apos;, include([ # 引入 ，引入可以引入其他URL配置模块，这里是写死的，还可以写成引入 xxx，其中xxx指的是 .py 文件，此文件中也有urlpatterns变量定义了url映射。 可以根据url的全路径匹配到引入模块的映射函数 url(r&apos;^add/$&apos;, views.add), url(r&apos;^edit/$&apos;, views.edit), url(r&apos;^delete/$&apos;, views.delete), url(r&apos;^save/$&apos;, views.save), ]) ),] 注意：通过此方式引入的URl配置，我们在父URl中使用正则捕获到的参数，是可以传递给子URl视图函数的，所以可以放心使用 传递额外参数URL 配置允许向视图函数传递额外的参数，这些参数放在一个 Python 字典中。django.conf.urls.url() 函数的第三个参数是可选的，如果指定，应该是一个字典，指定要传给视图函数的额外关键字参数及其值。 例如： 12345urlpatterns = [ url(r&apos;^reviews/(?P&lt;year&gt;[0-9]&#123;4&#125;)/$&apos;, views.year_archive, &#123;&apos;foo&apos;: &apos;bar&apos;&#125; # 具名分组及设置默认参数 ),] 同样include()也同样适用此特性： 1234567891011# main.pyfrom django.conf.urls import include, url urlpatterns = [ url(r&apos;^reviews/&apos;, include(&apos;inner&apos;), &#123;&apos;reviewid&apos;: 3&#125;),]# inner.pyurlpatterns = [ url(r&apos;^archive/$&apos;, views.archive), url(r&apos;^about/$&apos;, views.about),] 反向解析URl这个不太明白，我认为就是转发请求的时候使用，注意讨论一下 Django 提供了一种方案，只需在 URL 映射中设计 URL。我们为其提供 URL 配置，然后可以双向使用： 从用户（浏览器）请求的 URL 开始，这个方案能调用正确的 Django 视图，并从 URL 中提取可能需要的参数及其值，传给视图。 从 Django 视图对应的标识以及可能传入的参数值开始，获取相应的 URL。 第一点就是我们目前所讨论的处理方式。第二点称为反向解析 URL、反向匹配 URL、反向查找 URL 或 URL反转。 Django 在不同的层中提供了执行 URL 反转所需的工具： 在模板中，使用url 模板标签。 在 Python 代码中，使用django.core.urlresolvers.reverse() 函数。 在处理 Django 模型实例 URL 相关的高层代码中，使用get_absolute_url() 方法。 示例： 123456789101112131415161718192021# URl描述urlpatterns = [ url(r&apos;^reviews/([0-9]&#123;4&#125;)/$&apos;, views.year_archive, name=&apos;reviews-year-archive&apos;),]# 模板中如下描述&lt;ul&gt; &#123;% for yearvar in year_list %&#125; &lt;li&gt;&lt;a href=&quot;&#123;% url &apos;reviews-year-archive&apos; yearvar %&#125;&quot;&gt;&#123;&#123; yearvar &#125;&#125; Archive&lt;/a&gt;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt;# 视图函数如下from django.core.urlresolvers import reversefrom django.http import HttpResponseRedirectdef redirect_to_year(request): # ... year = 2012 # ... return HttpResponseRedirect(reverse(&apos;reviews-year-archive&apos;, args=(year,))) URl命名空间URL 命名空间在反转具名 URL 模式时具有唯一确定性，即便不同的应用使用相同的名称也不怕。 正确使用 URL 命名空间的 Django 应用程序可以在同一个网站中多次部署。例如，django.contrib.admin 中有个AdminSite 类，可以轻易部署多个管理后台。URL 命名空间分为两部分，而且都是字符串： 应用命名空间。指明应用的名称。一个应用的每个实例都具有相同的应用命名空间。例如，你可能猜到了，Django 管理后台的应用命名空间是admin。 实例命名空间。标识具体的应用程序实例。实例命名空间在整个项目范围内应该是唯一的。不过，实例命名空间可以与应用命名空间相同，供应用的默认实例使用。例如，Django 管理后台实例的默认实例命名空间是admin 命名空间中的 URL 使用: 运算符指定。例如，管理后台的主页使用 admin:index 引用。其中，admin 是命名空间，index 是 URL 的名称。 命名空间还可以嵌套。members:reviews:index 在命名空间members 中查找命名空间reviews，再在里面查找index URL。 反转命名空间的URl的步骤 首先，Django 查找有没有匹配的应用命名空间（这里的reviews）。为此，会产出那个应用的实例列表。 如果有这么一个应用实例，Django 返回它的 URL 解析程序。当前应用可以通过请求的一个属性指定。预期有多个部署实例的应用应该在处理的请求上设定current_app 属性。 当前应用也可以手动指定，方法是作为参数传给reverse() 函数。 如果没有当前应用，Django 查找默认的应用实例。默认应用实例是指实例命名空间与应用命名空间匹配的实例（在这里是指名为reviews 的reviews 实例）。 如果没有默认的应用实例，Django 选中最后部署的应用实例，而不管实例的名称。 如果第 1 步找不到匹配的应用命名空间，Django 直接把它视作实例命名空间查找。 URL 命名空间和引入的 URL 配置示例把引入的 URL 配置放入命名空间中有两种方式。 第一种，在 URL 模式中为include() 提供应用和实例命名空间： 1url(r&apos;^reviews/&apos;, include(&apos;reviews.urls&apos;, namespace=&apos;author-reviews&apos;, app_name=&apos;reviews&apos;)) 上述示例把reviews.urls 中定义的 URL 放在应用命名空间reviews 中，放在实例命名空间author-reviews中。 第二种，引入包含命名空间数据的对象。如果使用include() 引入一组url() 实例，那个对象中的 URL 都添加到全局命名空间中。然而，include() 的参数还可以是一个三元素元组： 123456reviews_patterns = [ url(r&apos;^$&apos;, views.IndexView.as_view(), name=&apos;index&apos;), url(r&apos;^(?P&lt;pk&gt;\\d+)/$&apos;, views.DetailView.as_view(), name=&apos;detail&apos;),]url(r&apos;^reviews/&apos;, include((reviews_patterns, &apos;reviews&apos;, &apos;author-reviews&apos;))), 上述示例把指定的 URL 模式引入指定的应用和实例命名空间中。 记得要把一个元组传给include()。如果直接传入三个参数，例如include(reviews_patterns, ‘reviews’,’author-reviews’)，Django 不会抛出错误，但是根据include() 的签名，’reviews’ 是实例命名空间，’author-reviews’ 是应用命名空间，而正确的顺序应该反过来。 高级模板技术RequestContext和上下文处理器模板要在上下文中渲染。上下文是django.template.Context 的实例，不过 Django 还提供了一个子类，django.template.RequestContext，其行为稍有不同。 RequestContext 默认为模板上下文添加很多变量，例如HttpRequest 对象或当前登录用户的信息，例如我们在视图函数中获取到的request对象，它就是Djongo为我们提供的上下文处理器，我们可以在此处理器中获取很多参数: 示例： 12345def view_1(request): # ... t = loader.get_template(&apos;template1.html&apos;) c = RequestContext(request, &#123;&apos;message&apos;: &apos;I am view 1.&apos;&#125;, processors=[custom_proc]) return t.render(c) 上下文处理器使用示例，提供公共的上下文处理器： 12345678910111213141516171819202122232425262728# 定义一个上下文处理器，提供 &apos;app&apos;、&apos;user&apos; 和 &apos;ip_address&apos;，可以在处理器中为多个请求提供共同的必要的上下文# 注意：上下文处理器必须返回一个字典# 自定义上下文处理器一般放在单独项目或者项目下的context_processors.py中def custom_proc(request): return &#123; &apos;app&apos;: &apos;我的上下文测试&apos;, &apos;user&apos;: request.user, &apos;ip_address&apos;: request.META[&apos;REMOTE_ADDR&apos;] &#125;# 引用上下文处理器直接返回给模板进行渲染# 注意processors函数的参数，第一个必须是request对象，第二个是可选的上下文处理器列表或元组def view_1(request): return render(request, &apos;template1.html&apos;, &#123;&apos;message&apos;: &apos;消息1&apos;&#125;, context_instance=RequestContext( request, processors=[custom_proc] ) )# 与上述一致，公用上下文处理器def view_2(request): return render(request, &apos;template2.html&apos;, &#123;&apos;message&apos;: &apos;消息2&apos;&#125;, context_instance=RequestContext( request, processors=[custom_proc] ) ) Djongo提供的上下文处理器在settings文件中，我们来逐一说明Djongo提供的上下文处理器 123456789101112131415161718192021222324252627282930313233343536&apos;OPTIONS&apos;: &#123; # 默认的处理器上下文 &apos;context_processors&apos;: [ # 启用这个处理器后，RequestContext 中将包含下面两个变量 # debug：True。可以在模板中测试是否在DEBUG 模式中。 # sql_queries：&#123;&apos;sql&apos;: …, &apos;time&apos;: …&#125; 字典构成的列表，表示处理请求的过程中执行的 SQL 查询及其用时。列表中的值按查询的执行顺序排列，在访问时惰性生成 &apos;django.template.context_processors.debug&apos;, # 启用这个处理器后，RequestContext 中将包含request 变量，它的值是当前的HttpRequest 对象 &apos;django.template.context_processors.request&apos;, # 启用此处理器后将包含： # user：auth.User 的实例，表示当前登录的用户（如未登录，是AnonymousUser 实例）。 # perms：django.contrib.auth.context_processors.PermWrapper 实例，表示当前登录用户拥有的权限。 &apos;django.contrib.auth.context_processors.auth&apos;, # 启用这个处理器后，RequestContext 中将包含下面两个变量： # messages：消息框架设定的消息列表（里面的值是字符串） # DEFAULT_MESSAGE_LEVELS：消息等级名称到数字值的映射 &apos;django.contrib.messages.context_processors.messages&apos;, # 非默认，启用后将包含： # LANGUAGES：LANGUAGES 设置的值 # LANGUAGE_CODE：如果request.LANGUAGE_CODE 存在，返回它的值；否则返回LANGUAGE_CODE 设置的值 # django.template.context_processors.i18n # 非默认，启用这个处理器后，RequestContext 中将包含MEDIA_URL 变量，提供MEDIA_URL 设置的值 # django.template.context_processors.media # 非默认，启用这个处理程序后，RequestContext 中将包含STATIC_URL 变量，提供STATIC_URL 设置的值 # jango.template.context_processors.static # 非默认，这个处理器添加一个令牌，供csrf_token 模板标签使用，用于防范跨站请求伪造，（暂时不清楚具体意思） # django.template.context_processors.csrf ],&#125;, 模板加载内部机制DIRS 选项告诉 Django 模板目录有哪些的方法是使用设置文件中TEMPLATES 设置的DIRS 选项，或者是Engine 的dirs 参数。这个选项的值是一个字符串列表，包含指向模板目录的完整路径： 123456789TEMPLATES = [ &#123; &apos;BACKEND&apos;: &apos;django.template.backends.django.DjangoTemplates&apos;, &apos;DIRS&apos;: [ &apos;/home/html/templates/lawrence.com&apos;, &apos;/home/html/templates/default&apos;, ], &#125;,] 模板可以放在任何位置，只要 Web 服务器有权限读取目录及里面的模板即可。模板的扩展名不限，可以是.html 或.txt，甚至可以没有。注意，这里的路径应该使用 Unix 风格的正斜线，即便在 Windows 中也是如此。 加载器类型说明123456789101112# 加载器# 默认使用 ilesystem.Loader 文件系统加载器，如果不设定DIRS 选项，这个加载器找不到任何模板。# DIRS 定义一个目录列表，模板引擎按顺序在里面查找模板源文件。# 当前设置表示在项目根目录中放一些主模板，模板目录不一定非得叫&apos;templates&apos;，可以自定义&apos;DIRS&apos;: [os.path.join(BASE_DIR, &apos;templates&apos;)],# 还有应用目录加载器 pp_directories.Loader# INSTALLED_APPS = [&apos;myproject.reviews&apos;, &apos;myproject.music&apos;]# 从文件系统中的 Django 应用里加载模板。这个加载器在INSTALLED_APPS 列出的各个应用中查找templates 子目录。如果找到，Django 在其中查找模板。# 这意味着，应用可以自带模板。通过这一行为，便于分发带默认模板的 Django 应用。例此加载器会在设置的文件夹中顺序的加载模板，最先找到的被加载，所以设置顺序很重要# 还有一些其他加载器，默认是禁用的，自行了解 Django模型的高级用法新增和修改对象save和create方法，如下： 12345678910111213141516m01 = Manufacturers(name=&apos;金龙鱼&apos;，address=&apos;铁岭&apos;, city=&apos;大连&apos;, province=&apos;沈阳&apos;, website=&apos;www.xmy.com&apos;)# save方法保存m01.save()# create方法保存m02 = Manufacturers.objects.create(name=&apos;金龙鱼&apos;，address=&apos;铁岭&apos;, city=&apos;大连&apos;, province=&apos;沈阳&apos;, website=&apos;www.xmy.com&apos;)# 当数据被保存后，及对象ID值是有的，直接再次调用save方法，就是修改对象，注意这里是全字段修改m02.name = &apos;鲁花&apos;m02.save()# 一个语句更新一个对象Manufacturers.objects.get(name=&apos;金龙鱼&apos;).save(city=&apos;黑龙江&apos;)# 一个语句中更新多条数据Manufacturers.objects.filter(name=&apos;金龙鱼&apos;).update(city=&apos;黑龙江&apos;) 查询数据all、filter、get方法等，如下： 1234567891011121314# 获取所有数据Manufacturers.objects.all()# 根据条件获取部分数据Manufacturers.objects.filter(name=&apos;金龙鱼&apos;)# 根据多个条件获取部分数据Manufacturers.objects.filter(name=&apos;金龙鱼&apos;, address=&apos;铁岭&apos;)# 类似SQL使用like语句过滤获取部分数据，字段加两个下划线，接containsManufacturers.objects.filter(name__contains=&apos;金&apos;)# 获取单个数据，此方法返回的就不是列表了，而是单条数据，如果查询出多条数据或者没有查询出数据，会抛出异常Manufacturers.objects.get(website=&apos;www.xmy.com&apos;) 排序数据order_by方法，具体如下： 12345678# 根据name排序，正向Manufacturers.objects.order_by(&apos;name&apos;)# 根据多个字段排序Manufacturers.objects.order_by(&apos;name&apos;, &apos;province&apos;)# 反向排序，方法是在字段名称前面加上“-”（减号）Manufacturers.objects.order_by(&apos;-name&apos;) 链式查找既过滤加排序，如下： 1Manufacturers.objects.filter(name=&apos;金龙鱼&apos;).order_by(&apos;-name&apos;) 切片数据及传统上理解的分页查找，如下： 12345# 返回查询出的第一条数据Manufacturers.objects.filter(name=&apos;金龙鱼&apos;)[0]# 分页查询，底层使用的是Mysql的Limit函数Manufacturers.objects.filter(name=&apos;金龙鱼&apos;)[0, 10] 删除数据delete方法，如下： 12345678# 删除一条数据Manufacturers.objects.get(name=&apos;金龙鱼&apos;).delete()# 删除多条数据Manufacturers.objects.filter(name=&apos;金龙鱼&apos;).delete()# 删除全部数据Manufacturers.objects.all().delete() 访问外键数据根据外键查询出关联的对象，如下： 123# 一对一关联manufacturers = Product.objects.get(title=&apos;小米&apos;).manufacturersmanufacturersName = Product.objects.get(title=&apos;小米&apos;).manufacturers.name 访问多对多数据多对多关联数据获取如下： 12345678910# 获取所有关联vender = Product.objects.get(title=&apos;小米&apos;).vender.all()# 过滤多对多数据vender = Product.objects.get(title=&apos;小米&apos;).vender.filter(name=&apos;供应商&apos;)# 反过来，查看经销商经销的所有产品，字段加 _setVender.objects.get(name=&apos;供应商&apos;).product_set.all()count = Product.vender.count() 管理器在Book.objects.all() 语句中，objects 是个特殊的属性，我们通过它查询数据库，这是模型的管理器（manager）。现在，我们要深入说明管理器的作用和用法。 添加额外的自定义管理器，修改模型如下： 123456789101112131415161718192021222324# 自定义模型管理器class MyProductManager(models.Manager): # 覆盖的get_queryset() 返回的是一个QuerySet 对象，它对应着我们管理器的all()方法 def get_queryset(self): return super(MyProductManager, self).get_queryset().filter(name=&apos;大米&apos;)# 产品class Product(models.Model): title = models.CharField(max_length=100, verbose_name=&apos;产品名称&apos;) vender = models.ManyToManyField(Vender, verbose_name=&apos;经销商&apos;) manufacturers = models.ForeignKey(Manufacturers, verbose_name=&apos;厂家&apos;) product_date = models.DateField(verbose_name=&apos;生产日期&apos;) # fields = (&apos;title&apos;, &apos;product_date&apos;, &apos;vender&apos;, &apos;manufacturers&apos;) # 我们明确地把objects 设为一个普通的Manager 示例，如若不然，唯一可用的管理器将是dahl_objects objects = models.Manager() # 默认的管理器 my_objects = MyProductManager() # 专门查询 产品名称为大米 的管理器 def __str__(self): return u&apos;%s %s&apos; % (self.title, self.product_date) # 任何模型都可以使用Meta 类指定多个针对所在模型的选项。 class Meta: ordering = [&apos;product_date&apos;] 下面是使用上述自定义管理器示例： 1234# get_queryset() 返回的是一个QuerySet 对象，因此可以在其上调用filter()、exclude() 和其他所有QuerySet 支持的方法Product.my_objects.all()Product.my_objects.filter(title=&apos;Matilda&apos;)Product.my_objects.count() 如果需要，我们可以在同一个模型上使用多个管理器。 模型方法模型方法就是为Model提供一些方法，我们调用这些方法的时候能处理一些我们想要的逻辑。 模型为我们自动提供的常用方法有如下： str()。这是 Python 的一个“魔法方法”，返回对象的 Unicode 表示形式。需要以普通的字符串显示模型实例时，Python 和 Django 会调用这个方法。尤其要注意，在交互式控制台或管理后台中显示对象调用的都是这个方法。这个方法一定要自定义，因为默认的实现没什么用。 get_absolute_url()。这个方法告诉 Django 如何计算一个对象的 URL。Django 在管理后台和需要生成对象的 URL 时调用这个方法。具有唯一标识的 URL 的对象都要定义这个方法。 模型方法大多可以被直接覆盖，最常见的就是覆盖str()方法 一下演示覆盖预定义的模型方法，是针对数据库执行行为来覆盖的，例如： 1234567891011# 定义Blog模型，覆盖它的save方法，如下定义某些情况下不允许保存class Blog(models.Model): name = models.CharField(max_length=100) tagline = models.TextField() def save(self, *args, **kwargs): if self.name == &quot;Yoko Ono&apos;s blog&quot;: return # Yoko 肯定不会开博客的！ else: super(Blog, self).save(*args, **kwargs) # 调用“真正的”save () 方法 执行原始SQL模型的查询 API 不够用时，可以编写原始 SQL。Django 为执行原始 SQL 查询提供了两种方式：使用Manager.raw() 执行，返回模型实例集合；或者完全不用模型层，直接执行自定义SQL 第一种方式执行示例： 123456789101112131415161718# 基本查询分页Product.objects.raw(&apos;SELECT * FROM CEREALSOILS_PRODUCT LIMIT 0, 5&apos;)# 当我们定义的模型字段名称与数据库字段名称不一致时，可以通过AS将字段对应起来Product.objects.raw(&apos;SELECT product_name as name FROM CEREALSOILS_PRODUCT&apos;)# 延期模型字段# 注意：指定查询字段的时候，必须要包含主键字段p = Product.objects.raw(&apos;SELECT id, title FROM CEREALSOILS_PRODUCT&apos;)title = p.title # 上述执行取出的数product_date = p.product_date # 又执行了一次SQL来取出的此字段的值# 为raw传递参数，注意，这里是防注入的用法，参数写在raw方法内才有防注入的作用# 注意：Djongo中的占位符是 %s ，而不是 ?# 查询中有 % ，则需要写两个 %title = &apos;大米&apos;pa = Product.objects.raw(&apos;SELECT * FROM CEREALSOILS_PRODUCT where title = %s&apos;, title)p9 = Product.objects.raw(&apos;SELECT * FROM cerealsOils_product where title like %s LIMIT 0, 5&apos;, [&apos;%米%&apos;]) 第二种方式执行示例： django.db.connection 对象表示默认的数据库连接。若想使用这个数据库连接，调用connection.cursor()，然后，调用cursor.execute(sql, [params]) 执行 SQL，再调用cursor.fetchone() 或cursor.fetchall() 返回所得的行。 12345678910111213141516171819202122232425262728293031323334353637from django.db import connectiondef my_custom_sql(self): cursor = connection.cursor() cursor.execute(&quot;UPDATE bar SET foo = 1 WHERE baz = %s&quot;, [self.baz]) cursor.execute(&quot;SELECT foo FROM bar WHERE baz = %s&quot;, [self.baz]) row = cursor.fetchone() return row# 当需要连接多个数据库时，可以获取指定的数据库连接cursor = connections[&apos;my_db_alias&apos;].cursor()# 连接和游标（类似于Java中 Try-with-resouce）with connection.cursor() as c: c.execute(...)等效于：c = connection.cursor()try: c.execute(...)finally: c.close()# 查询数据，只返回结果没有字段名称映射cursor = connection.cursor()cursor.execute(&quot;SELECT id, title FROM cerealsOils_product&quot;)row01 = cursor.fetchone()row02 = cursor.fetchone()print(row01)print(row02)# 查询数据，有字段名称映射cursor01 = connection.cursor()cursor01.execute(&quot;SELECT id, title FROM cerealsOils_product&quot;)desc = cursor01.descriptiondict(zip([col[0] for col in desc], row))for row in cursor01.fetchall(): print(row)","categories":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/tags/Python/"},{"name":"Django","slug":"Django","permalink":"https://jjw-story.github.io/tags/Django/"}],"author":"JJW"},{"title":"文件及目录权限","slug":"文件及目录权限","date":"2019-08-03T12:20:20.000Z","updated":"2019-08-03T06:13:49.717Z","comments":true,"path":"2019/08/03/文件及目录权限/","link":"","permalink":"https://jjw-story.github.io/2019/08/03/文件及目录权限/","excerpt":"","text":"文件及目录权限的表示方法当我们使用 ls -l 命令查看详细文件内容时，可以看到查询出的内容如下： 123456789root@CHJ-20190520VPS:/usr/lib# ls -ltotal 920drwxr-xr-x 1 root root 4096 May 21 22:39 kerneldrwxr-xr-x 1 root root 4096 May 21 22:39 klibcdrwxr-xr-x 1 root root 4096 May 21 22:40 language-selectorlrwxrwxrwx 1 root root 21 Feb 12 16:55 libDeployPkg.so.0 -&gt; libDeployPkg.so.0.0.0-rw-r--r-- 1 root root 31280 Feb 12 16:55 libDeployPkg.so.0.0.0lrwxrwxrwx 1 root root 20 Feb 12 16:55 libguestlib.so.0 -&gt; libguestlib.so.0.0.0-rw-r--r-- 1 root root 22656 Feb 12 16:55 libguestlib.so.0.0.0 一共查询出七列内容，分别表示： 文件属性(占10个字符空间)、拥有的文件数量、文件的创建者、所属的group、文件大小、建档日期、文件名 文件属性Linux的文件基本上分为三个属性：可读（r），可写（w），可执行（x） 但是这里有十个格子可以填（具体程序实现时，实际上是十个bit位） 文件类型第一个小格是特殊表示格，表示目录或连结文件等等 d 表示目录，这个是在创建下来文件的类型就固定了下来，不可以人为进行更改 l 表示链接文件，类似于快捷方式 - 表示这是普通文件 b 块特殊文件，其实是指的是设备，比如我们插入一个移动硬盘，插入一个硬盘之后，Linux系统就会把他当成一个特出文件块文件来表示 c 字符特殊文件，就是终端 f 命名管道 s 套接字文件 文件权限表示方法字符权限表示方法： r 读 w 写 x 执行 数字权限的表示方法（8进制数字表示）： r=4 w=2 x=1 其余剩下的格子就以每3格为一个单位，因为Linux是多用户多任务系统，所以一个文件可能同时被许多人使用，所以我们一定要设好每个文件的权限，其文件的权限位置排列顺序是（以-rwxr-xr-x为例）： rwx(Owner)r-x(Group)r-x(Other) 这个例子表示的权限是：使用者自己可读，可写，可执行；同一组的用户可读，不可写，可执行；其它用户可读，不可写，可执行。 另外，有一些程序属性的执行部分不是X,而是S,这表示执行这个程序的使用者，临时可以有和拥有者一样权力的身份来执行该程序。一般出现在系统管理之类的指令或程序，让使用者执行时，拥有root身份。 文件权限的修改修改权限命令chown命令修改属主或数组命令，使用方法： 修改属主：chown 用户名称 文件名称 修改属组：chown :用户组名称 文件名称 修改文件、目录权限。Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以藉以控制文件如何被他人所调用 首先我们新创建一个目录，查看root用户新创建目录的默认权限，如下： 12root@CHJ-20190520VPS:/tmp# ls -ld testdir/drwxr-xr-x 1 root root 4096 Jul 27 15:05 testdir/ 表示文件的属主是root用户，root用户可以读写删除等，所属用户组不能删除，其他其他用户不能删除 这里我们需要了解，Linux权限限制是非root用户的，这里既是我们将文件或目录的权限给root修改了，但是root用户还是不受限制的 所以为了方便测试我们不要用root用户来操作，命令使用示例如下： 123456789101112131415# 修改属主root@CHJ-20190520VPS:/tmp# chown wangjia3 testdirroot@CHJ-20190520VPS:/tmp# ls -ld testdirdrwxr-xr-x 1 wangjia3 root 4096 Jul 27 15:05 testdir# 修改属组root@CHJ-20190520VPS:/tmp# groupadd group01root@CHJ-20190520VPS:/tmp# chown :group01 testdirroot@CHJ-20190520VPS:/tmp# ls -ld testdirdrwxr-xr-x 1 wangjia3 group01 4096 Jul 27 15:05 testdir# 属主属组一起修改root@CHJ-20190520VPS:/tmp# chown wangjia3:group01 testdir/root@CHJ-20190520VPS:/tmp# ls -ld testdir/drwxr-xr-x 1 wangjia3 group01 4096 Aug 3 12:35 testdir/ 可以看到文件的属主和属组已经改变 chgrp命令修改属组命令，使用方法：chgrp 用户组名称 文件名称 使用示例： 123root@CHJ-20190520VPS:/tmp# chgrp root testdirroot@CHJ-20190520VPS:/tmp# ls -ld testdirdrwxr-xr-x 1 wangjia3 root 4096 Jul 27 15:05 testdir 可以看到文件的属组已经被修改回为root分组 创建文件的默认权限我们创建一个新的文件，默认的权限如下所示： 123root@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rw-r--r-- 1 root root 0 Aug 3 12:35 demoFile 那它是怎么来的呢？其实创建新的文件默认的权限是数字表示法：666，表示属主属组其他都是拥有读写权限，但它会根据数字权限表示减去一个uumask，umask表示如下： 12root@CHJ-20190520VPS:/tmp/testdir# umask0022 所以这就是我们创建文件的默认权限的由来，是使用 666 减去 umask 得到的默认权限 chmod命令chmod是Linux/Unix中修改文件或者目录权限的命令，通过修改权限可以让指定的人对文件可读、可写、可运行，极大地保证了数据的安全性 使用方法：chmod [修改内容 修改符号 权限] 文件 修改字符权限参数详解修改内容 u 修改文件属主的权限 g 修改文件属组的权限 r 修改其他以外的权限 a 以上三者都修改 具体权限修改 + 增加权限 - 取消权限 = 直接设定权限 此三条具体设置的权限就是我们之前了解的：r、w、x 使用示例： 123456789101112131415161718192021222324252627root@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rw-r--r-- 1 root root 0 Aug 3 12:35 demoFile# 属主增加执行权限root@CHJ-20190520VPS:/tmp/testdir# chmod u+x demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rwxr--r-- 1 root root 0 Aug 3 12:35 demoFile# 属组取消读权限root@CHJ-20190520VPS:/tmp/testdir# chmod o-r demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rwxr----- 1 root root 0 Aug 3 12:35 demoFile# 其他设置执行和读权限root@CHJ-20190520VPS:/tmp/testdir# chmod o=xr demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rwxr--r-x 1 root root 0 Aug 3 12:35 demoFile# 所有设置读写执行权限root@CHJ-20190520VPS:/tmp/testdir# chmod a=xwr demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rwxrwxrwx 1 root root 0 Aug 3 12:35 demoFile 修改数字权限修改方法：chmod [数字] 文件 注意：以上参数中数字为3位数 参数详解三位数字第一位代表属主权限，第二位代表属组权限，第三位代表其他权限 数字则分别用 1、2、4 来分别表示 执行、写、读 使用示例： 123456789101112131415root@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rwxrwxrwx 1 root root 0 Aug 3 12:35 demoFile# 设置取消所有权限root@CHJ-20190520VPS:/tmp/testdir# chmod 000 demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0---------- 1 root root 0 Aug 3 12:35 demoFile# 设置属主读写权限，属组和其他读权限root@CHJ-20190520VPS:/tmp/testdir# chmod 644 demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rw-r--r-- 1 root root 0 Aug 3 12:35 demoFile 特殊权限SUID用于二进制可执行文件，执行命令时取得文件的属主权限，例如 /usr/bin/password 12wangjia3@CHJ-20190520VPS:/tmp/testdir$ ls -l /usr/bin/passwd-rwsr-xr-x 1 root root 59640 Mar 23 03:05 /usr/bin/passwd 如上示例中：属主权限是 ws，s 之前是我们没有解释过的，它表示的是不管是root用户还是普通用户，它在执行这条命令时，它都会以文件的属主的这种身份来进行操作 它的作用就是，我们有些文件用户是没有任何权限的，例如保存用户账户密码的文件，/etc/shadow: 12wangjia3@CHJ-20190520VPS:/tmp/testdir$ ls -l /etc/shadow-rw-r----- 1 root shadow 1153 Jul 27 13:38 /etc/shadow 我们当前登录的用户是没有此文件的任何权限的，而root用户有此文件的权限，那我们普通用户为什么能修改密码呢，就是我们在修改密码例如passwd文件的时候，它能以root用户的身份来执行，这样就避免了我们需要主动去切换用户的修改密码的问题。 SGID用于目录，在该目录下创建新的文件和目录，权限自动更改为该目录的数组，一般是我们在文件共享的时候，一般会用到SET GID SBIT用于目录，该目录下新建的文件和目录，仅root和自己可以删除，如/tmp 12wangjia3@CHJ-20190520VPS:/$ ls -ld /tmpdrwxrwxrwt 1 root root 4096 Aug 3 13:50 /tmp 注意在其他位有一个t，这样就可以防止自己创建的文件被其他的普通用户修改或删除 设置特殊权限使用的也是 chmod命令，用法与上述修改权限用法一致，只不过多了由三位数变为了四位数，第一位为特殊权限的表示数字 特殊权限数字表示： 4 SET UID 123456789root@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0----rw---- 1 wangjia3 wangjia3 0 Aug 3 12:35 demoFileroot@CHJ-20190520VPS:/tmp/testdir# chmod 4644 demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rwSr--r-- 1 wangjia3 wangjia3 0 Aug 3 12:35 demoFile 1 SET BIT 1234root@CHJ-20190520VPS:/tmp/testdir# chmod 1644 demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rw-r--r-T 1 wangjia3 wangjia3 0 Aug 3 12:35 demoFile 2 SET GID 1234root@CHJ-20190520VPS:/tmp/testdir# chmod 2644 demoFileroot@CHJ-20190520VPS:/tmp/testdir# ls -ltotal 0-rw-r-Sr-- 1 wangjia3 wangjia3 0 Aug 3 12:35 demoFile 注意特殊权限一般不要去自己随便指定，使用系统默认就行","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"文件及目录权限","slug":"文件及目录权限","permalink":"https://jjw-story.github.io/tags/文件及目录权限/"}],"author":"JJW"},{"title":"su-sudo命令","slug":"su-sudo命令","date":"2019-07-27T01:07:11.000Z","updated":"2019-07-27T04:15:12.298Z","comments":true,"path":"2019/07/27/su-sudo命令/","link":"","permalink":"https://jjw-story.github.io/2019/07/27/su-sudo命令/","excerpt":"","text":"su命令Linux su命令用于变更为其他使用者的身份，除 root 外，需要键入该使用者的密码 使用方法：su [选项] [用户名称] 选项说明 -f 不必读启动档（如 csh.cshrc 等），仅用于 csh 或 tcsh -c 既command，变更为帐号为指定账号的使用者并执行指定指令（command）后再变回原来使用者，使用方法：su -c [指定命令] 用户名称 -m -p 既preserve、environment，执行 su 时不改变环变量 - -l login这个参数加了之后，就好像是重新login为该使用者一样，大部份环境变数（HOME SHELL USER等等）都是以该指定用户为主，并且工作目录也会改变，如果没有指定用户，内定是 root -s 指定要执行的shell （bash csh tcsh 等），预设值为 /etc/passwd 内的指定用户shell 使用示例： 1234567891011121314wangjia3@CHJ-20190520VPS:~$ pwd/home/wangjia3wangjia3@CHJ-20190520VPS:~$ su rootPassword:root@CHJ-20190520VPS:/home/wangjia3# pwd/home/wangjia3root@CHJ-20190520VPS:~# su - wangjia3wangjia3@CHJ-20190520VPS:~$ pwd/home/wangjia3wangjia3@CHJ-20190520VPS:~$ su - rootPassword:root@CHJ-20190520VPS:~# pwd/root 注意 su 与 su - 的区别 sudo命令sudo 表示 “superuser do”。 它允许已验证的用户以其他用户的身份来运行命令。其他用户可以是普通用户或者超级用户。然而，大部分时候我们用它来以提升的权限来运行命令 以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行 使用方法：su [参数] 参数说明 -V 显示版本编号 -h 显示版本编号及指令的使用方式说明你 -l 显示出当前用户的权限 -v 因为 sudo 在第一次执行时或是在 N 分钟内没有执行（N 预设为五）会问密码，这个参数是重新做一次确认，如果超过 N 分钟，也会问密码 -k 将会强迫使用者在下一次执行 sudo 时问密码（不论有没有超过 N 分钟） -b 将要执行的指令放在背景执行 -s 执行环境变数中的 SHELL 所指定的 shell ，或是 /etc/passwd 里所指定的 shell -i Linux终端命令下改变用户对命令使用权限的命令，加载用户变量，并跳转到目标用户home目录 经常使用参数示例： 1234567wangjia3@CHJ-20190520VPS:/$ sudo -l[sudo] password for wangjia3:Matching Defaults entries for wangjia3 on CHJ-20190520VPS: env_reset, mail_badpass, secure_path=/usr/local/sbin\\:/usr/local/bin\\:/usr/sbin\\:/usr/bin\\:/sbin\\:/bin\\:/snap/binUser wangjia3 may run the following commands on CHJ-20190520VPS: (ALL : ALL) ALL 12345678wangjia3@CHJ-20190520VPS:~$ pwd/home/wangjia3wangjia3@CHJ-20190520VPS:~$ sudo -sroot@CHJ-20190520VPS:~# pwd/home/wangjia3root@CHJ-20190520VPS:~# sudo -iroot@CHJ-20190520VPS:~# pwd/root 注意 -s 和 -i 的区别 sudo命令使用补充通常我们使用普通用户使用一些命令是没有权限的，在使用这些没有权限的命令时需要切换到root用户，这样就需要告知普通用户root用户的密码，这样做是不安全的 例如我们执行shutdown命令时： 12[frog@iZm5ehzqow4ijp2ya2g2drZ ~]$ shutdown -h 60shutdown: Need to be root 显示需要使用root用户来操作此命令 这时我们可以使用visudo来修改sudo的配置文件，将此命令的使用权限赋给普通用户，来让普通用户有权限使用此命令，就可以不将root用户的密码告知与普通用户，安全的使用系统 visudo的使用时以vi开头的，其实就类似于使用 vi 打开了一个文件，使用操作与 vi 一致 1234567891011121314151617181920212223242526272829303132[root@iZm5ehzqow4ijp2ya2g2drZ /]# visudo## Sudoers allows particular users to run various commands as## the root user, without needing the root password.#### Examples are provided at the bottom of the file for collections...## user MACHINE=COMMANDS#### The COMMANDS section may have other options added to it.#### Allow root to run any commands anywhereroot ALL=(ALL) ALL## Allows members of the &apos;sys&apos; group to run networking, software,## service management apps and more.# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS## Allows people in group wheel to run all commands# %wheel ALL=(ALL) ALL## Same thing without a password# %wheel ALL=(ALL) NOPASSWD: ALL## Allows members of the users group to mount and unmount the## cdrom as root# %users ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom## Allows members of the users group to shutdown this system# %users localhost=/sbin/shutdown -h now## Read drop-in files from /etc/sudoers.d (the # here does not mean a comment) 注意此文件最下方有几段关于使用sudo的设置说明，下面是设置说明： 12345## Allows people in group wheel to run all commands# %wheel ALL=(ALL) ALL## Same thing without a password# %wheel ALL=(ALL) NOPASSWD: ALL %wheel 表示的是如果我们要设置的是用户组，则需要用 % 加上用户组名称，如果只是单个用户，就直接写用户名即可 ALL=(ALL) 表示在哪台主机上可以执行哪些命令，哪来主机指的是我们登陆的主机，Linux可以在本地登陆，也可以远程登陆，如果在本地登陆那么主机就是localhost，locachost是字符端登陆，如果是字符或远程都去登陆，就赋予 ALL 的权限。 如果只赋予一些命令，如上述中：# %users localhost=/sbin/shutdown -h now，意思是赋予了用户shutdown -h now 的命令的使用权限 如果有多条命令，就将命令用 “,” 隔开，例如上述段落中示例：%users ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom，表示赋予了用户mount、unmount命令 NOPASSWD: ALL 表示普通用户在使用管理员账户赋予它的这些命令时是否需要输入密码，这里 NOPASSWD: ALL 表示不需要输入密码，但是这种是不安全的，所以我们不建议此种设置类型 使用示例我们现在配置普通用户 frog 被赋予使用 shutdown -h 60 的权限，使用visudo来设置，修改如下： 123## Read drop-in files from /etc/sudoers.d (the # here does not mean a comment)#includedir /etc/sudoers.dforg ALL=/sbin/shutdown -h 60 修改完成然后切换到frog用户执行此命令： 12345678910[root@iZm5ehzqow4ijp2ya2g2drZ /]# su -l frog[frog@iZm5ehzqow4ijp2ya2g2drZ ~]$ shutdown -h 60shutdown: Need to be root[frog@iZm5ehzqow4ijp2ya2g2drZ ~]$ sudo /sbin/shutdown -h 60[sudo] password for frog:Broadcast message from root@iZm5ehzqow4ijp2ya2g2drZ (/dev/pts/1) at 12:14 ...The system is going down for halt in 60 minutes! 注意：直接执行还是会告诉你没有权限，我们需要使用sudo命令来执行root赋予权限的命令，需要使用命令的全路径，然后提示输入密码之后就执行成功啦","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"su-sudo","slug":"su-sudo","permalink":"https://jjw-story.github.io/tags/su-sudo/"}],"author":"JJW"},{"title":"用户和权限管理","slug":"用户和用户组管理","date":"2019-07-25T12:00:00.000Z","updated":"2019-07-27T06:08:31.650Z","comments":true,"path":"2019/07/25/用户和用户组管理/","link":"","permalink":"https://jjw-story.github.io/2019/07/25/用户和用户组管理/","excerpt":"","text":"用户管理Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 每个用户账号都拥有一个惟一的用户名和各自的口令，用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。 实现用户账号的管理，要完成的工作主要有如下几个方面： 用户账号的添加、删除与修改 用户口令的管理 用户组的管理 用户管理常用命令当前使用的Linux系统中已经有了两个用户，一个是root用户，一个是自己创建的用户，root用户是超级管理员，我们自己创建的用户是普通用户。LInux多用户其实就是将用户分成了两类用户，一类是root用户，一类是普通用户 root用户的权限比较大，它可以访问自己的家目录，访问系统的配置文件，例如之前我们修改的vim的配置文件，还有就是root用户还可以访问普通用户的家目录，但是普通用户的权限就受到了下去限制，它只能访问自己的家目录，以及root用户开放给它的一些没有危害到系统安全的目录文件 普通用户和普通用户之间是没有权限互相访问他们对方的家目录的，Linux就是通过这两种用户的区别，来做了最基本的权限隔离 用户添加命令用户添加命令实际使用的是 useradd命令，使用方法：useradd [选项] 用户名称 在添加用户完成后，我们可以使用 id 命令来验证是否添加成功，通过 id 命令，我们可以查看系统中有哪些已经存在的用户 1234567root@CHJ-20190520VPS:/# useradd wangjiasroot@CHJ-20190520VPS:/# id rootuid=0(root) gid=0(root) groups=0(root)root@CHJ-20190520VPS:/# id wangjiasuid=1001(wangjias) gid=1001(wangjias) groups=1001(wangjias)root@CHJ-20190520VPS:/# id abcid: ‘abc’: no such user 下面分析我们添加用户的时候Linux都做了哪些操作： 首先第一是为新添加的用户创建了它的家目录，创建完成我们访问的时候发现家目录是空的，但其实用户的家目录是存放了很多与用户相关的隐藏配置文件 第二步就是将我们创建的用户记录在 /etc/passwd 文件中，只要包含了如下所示内容，就说明我们系统中存在这样的一个用户 123root@CHJ-20190520VPS:/# cat /etc/passwd...wangjias:x:1001:1001::/home/wangjias:/bin/sh 第三步还会在 etc/shadow 文件中添加我们创建的用户信息，这个文件是用户密码相关的文件 还有就是我们每创建一个用户，都会创建一个独立的用户id，叫uid，如上述我们通过id命令查询出来的内容。注意root用户的id是0，如果我们把普通用户的id也修改为0，那么系统就会把此用户也当成root用户 第四就是还会为用户创建用户所属的组，如果我们没有明确指定所属的组，系统就会创建一个与用户同名的组作为新创建用户所属的组。如果我们希望有一组用户他们使用同样的资源的时候，就可以建立一个这样的用户组，然后把他们都加入到这个组里面，如果我们对这个组进行修改，就相当于对一组的用户全都进行修改 注意创建用户只有root用户有这样的权限，普通用户没有创建用户的权限 上面的创建我们没有指定选项，下面介绍一些选项： -c comment 指定一段注释性描述 -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录 -g 用户组 指定用户所属的用户组 -G 用户组，用户组 指定用户所属的附加组 -s Shell文件 指定用户的登录Shell -u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号 为用户设置密码用户登录是需要登录密码的，为用户设置密码的命令使用的是 passwd命令，使用方法：passwd [用户名] 注意用户名选项是可选的，如果不指定要修改密码的用户名，那就是修改当前用户的密码 1234root@CHJ-20190520VPS:/# passwd wangjiasEnter new UNIX password:Retype new UNIX password:passwd: password updated successfully 删除用户删除用户使用的是userdel命令，使用方法： userdel [选项] 用户名 注意我们一般删除用户的时候会添加上 “-r” 选项，如果我们直接使用用户删除命令删除用户，则此用户的家目录会被保留下来，当我们确认用户的家目录中的数据都可以被直接删除的时候，就可以添加 -r 这个选项，直接删除彻底。 如果执行的是彻底删除的命令，那么 /etc/passed 和 /etc/shadow 中的用户信息也会被删除 修改账号修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。 修改已有用户的信息使用usermod命令，使用方法：usermod 选项 用户名 常用的选项包括-c, -d, -m, -g, -G, -s, -u以及-o等，这些选项的意义与useradd命令中的选项一样，可以为用户指定新的资源值 经常使用的选项是 -d 选项，既指定用户新的家目录，使用方法：usermod -d 新的家目录 用户名 如果我们修改了用户的家目录，那么我们重新登录此用户的时候，它的默认目录就会成为我们修改的目录，并且关于此用户的配置文件也会放在新的家目录中 用户组管理每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新 用户组管理命令新建用户组新建用户组使用groupadd命令，使用方法：groupadd [选项] 用户组 选项一般使用 -g 选项，此选项用于指定用户组的组标识号 新建好组之后，有两种方式将用户添加至先建好的组，第一种是使用 usermod 命令： 12345root@CHJ-20190520VPS:/# groupadd -g 1001 group1root@CHJ-20190520VPS:/# useradd user1root@CHJ-20190520VPS:/# usermod -g group1 user1root@CHJ-20190520VPS:/# id user1uid=1001(user1) gid=1001(group1) groups=1001(group1) 可以看到我们已经将user1用户添加到了group1组中 第二种就是在新建用户的时候直接将用户添加至指定用户组中: 123root@CHJ-20190520VPS:/# useradd -g group1 user2root@CHJ-20190520VPS:/# id user2uid=1002(user2) gid=1001(group1) groups=1001(group1 修改用户组修改用户组的属性使用groupmod命令，使用方法：groupmod [选项] 用户组 常用选项如下： -g GID 为用户组指定新的组标识号 -o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同 -n 新用户组 将用户组的名字改为新名字 使用示例： 123root@CHJ-20190520VPS:/# groupmod -n group2 group1root@CHJ-20190520VPS:/# id user2uid=1002(user2) gid=1001(group2) groups=1001(group2) 删除用户组如果要删除一个已有的用户组，使用groupdel命令，使用方法：groupdel 用户组 1root@CHJ-20190520VPS:/# groupdel group2 用户和用户组配置文件用户和用户组相关的配置文件主要有三个，/etc/passwd、/ect/shadow、/etc/group passwd文件内容如下： 12345671 root:x:0:0:root:/root:/bin/bash2 daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin3 bin:x:2:2:bin:/bin:/usr/sbin/nologin4 sys:x:3:3:sys:/dev:/usr/sbin/nologin...30 wangjia3:x:1000:1000:,,,:/home/wangjia3:/bin/bash31 user01:x:1001:1001::/home/user01:/bin/sh 我们发现此文件被分成了七个字段，下面分别解释每个字段的含义： 用户名称字段，表示用户的名称，可以看到此文件最后的两行是我们之前新创建的用户 第二个字段表示此用户登录需要不需要密码验证，如果把这个 “x” 删除之后，我们发现登录用户将不需要验证 1234wangjia3@CHJ-20190520VPS:~$ su -l wangjia3Password:wangjia3@CHJ-20190520VPS:~$ su -l user01user01@CHJ-20190520VPS:/$ 第三个字段是用户的uid字段，Linux并不是通过用户的名称来识别用户的，它是通过用户的id来识别用户，如果id重复了就会用最小id的用户使用 第四个字段是gid，是当前用户属于哪一个组的标识字段 第五个字段是注释 第六个字段表示用户的家目录的位置 用户登录后使用的命令解释器，现在所通用的命令解释器都是bash命令解释器。我们发现有很多第七个字段显示的是 /usr/sbin/nologin， 这里表示的是此用户是不能登录终端的，例如我们将user01用户的此字段修改为nologin： 123456 30 wangjia3:x:1000:1000:,,,:/home/wangjia3:/bin/bash 31 user01:x:1001:1001::/home/user01:/usr/sbin/nologinroot@CHJ-20190520VPS:~# su -l user01This account is currently not available.root@CHJ-20190520VPS:~# 切换用户失败，提示此账户不能登录 我们可以直接在这里添加一行数据，来添加用户 shadowshadow文件是保存用户和用户密码相关信息的，我们需要了解它的前两个字段 1234528 sshd:*:18037:0:99999:7:::29 pollinate:*:18037:0:99999:7:::30 wangjia3:$6$486gKZ88$cobO1oh/kuz4HwAmnpnb.OQtszzD78m0e.KvbbxcEbNfIA9/4cSKvU78iTMOgFL8FstKrk0hIQ/S16P/R5o6t.:18080:0: 99999:7:::31 user01:$6$Pe7lIEqi$28CGKQAIa3E4JvTvAZKeymjFgVY5HvGZXVg0RuUWetl2YTlgU5sLcMzRs6FZmJbnIvad3IeJO4bPXs082KqL10:18104:0:99 999:7::: 第一个字段是用户名称字段，用来和passwd字段来进行对应 是用户加密过的密码，加密的密码是以$开头，然后一串字符，这样主要是为了保护用户的密码，及时用户的密码是相同的，但是在此文件中的显示也是不同的，防止被破解 group和用户组相关的配置文件，里面包含四个字段： 第一个字段是组的名称 第二个字段这个组是否需要密码验证 第三个字段表示这个组的gid 第四个字段，表示其他组字段，表示哪些用户属于其他组，这个其他组里面又包含了用户名称","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"用户和权限管理","slug":"用户和权限管理","permalink":"https://jjw-story.github.io/tags/用户和权限管理/"}],"author":"JJW"},{"title":"Python","slug":"Python","date":"2019-07-20T12:00:00.000Z","updated":"2019-07-24T09:40:11.920Z","comments":true,"path":"2019/07/20/Python/","link":"","permalink":"https://jjw-story.github.io/2019/07/20/Python/","excerpt":"","text":"Python基础Python语法注释注释的三种方式如下： 123456789# 注释&apos;&apos;&apos;注释&apos;&apos;&apos;&quot;&quot;&quot;这也是注释&quot;&quot;&quot; 缩进python不需要 {} 都是使用缩进表示代码块 字符串字符串可以 ‘ ‘， “ “, “”” “”” 123456&apos;字符串&apos;&quot;字符串&quot;&quot;&quot;&quot;多行字符串&quot;&quot;&quot; 空行函数之间或类方法之间用空行分开 ; 符号同一行可以显示多条语句，使用 ; 隔开 导入 在 python 用 import 或者 from…import 来导入相应的模块 将整个模块(somemodule)导入，格式为： import somemodule 从某个模块中导入某个函数,格式为： from somemodule import somefunction 从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc 将某个模块中的全部函数导入，格式为： from somemodule import * end关键字关键字end可以用于将结果输出到同一行，或者在输出的末尾添加不同的字符 123while b &lt; 1000: print(b, end=&apos;,&apos;) b += 1 条件控制123456789101112if 表达式1: 语句 if 表达式2: 语句 elif 表达式3: 语句 else: 语句elif 表达式4: 语句else: 语句 循环while循环: 123456count = 0while count &lt; 5:print (count, &quot; 小于 5&quot;)count = count + 1else:print (count, &quot; 大于或等于 5&quot;) for循环： 12345678sites = [&quot;Baidu&quot;, &quot;Google&quot;,&quot;Runoob&quot;,&quot;Taobao&quot;]for site in sites: if site == &quot;Runoob&quot;: print(&quot;菜鸟教程!&quot;) break print(&quot;循环数据 &quot; + site)else: print(&quot;没有循环数据!&quot;) 注意：以上循环中，else为跳出循环后执行的逻辑，且只执行一次，可以不存在else range()函数： 12345for i in range(103) : print(i)for i in range(0, 10, 3) : print(i) 参数最多可以有三个，第一个为开始限定，第二个为结束限定，第三个为步长 123a = [&apos;Google&apos;, &apos;Baidu&apos;, &apos;Runoob&apos;, &apos;Taobao&apos;, &apos;QQ&apos;]for i in range(len(a)): print(i, a[i]) break、continue break 语句可以跳出 for 和 while 的循环体。如果你从 for 或 while 循环中终止，任何对应的循环 else 块将不执行 continue语句被用来告诉Python跳过当前循环块中的剩余语句，然后继续进行下一轮循环 pass 语句： 12while True: pass 遍历技巧： 在序列中遍历时，索引位置和对应值可以使用 enumerate() 函数同时得到： 1234567for i, v in enumerate([&apos;tic&apos;, &apos;tac&apos;, &apos;toe&apos;]): print(i, v)# 结果0 tic1 tac2 toe 反向遍历： 12for i in reversed(range(1, 10, 2)): print(i) 迭代器迭代器有两个基本的方法：iter( 和 next() 字符串，列表或元组对象都可用于创建迭代器 示例： 123456789101112list=[1,2,3,4]it = iter(list) # 创建迭代器对象for x in it: print (x, end=&quot; &quot;)list=[1,2,3,4]it = iter(list) # 创建迭代器对象while True: try: print (next(it)) except StopIteration: sys.exit() StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 next() 方法中我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代。 函数1.语法12def 函数名（参数列表）: 函数体 可更改(mutable)与不可更改(immutable)对象： 不可变类型：变量赋值 a = 5 后再赋值 a = 10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变a的值，相当于新生成了a 可变类型：变量赋值 la=[1,2,3,4] 后再赋值 la[2]=5 则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了 python 函数的参数传递： 不可变类型：类似 c++ 的值传递，如 整数、字符串、元组。如fun(a)，传递的只是a的值，没有影响a对象本身。比如在 fun(a)内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun(la)，则是将 la 真正的传过去，修改后fun外部的la也会受影响 2.参数必需参数： 必需参数须以正确的顺序传入函数。调用时的数量必须和声明时的一样，既只要声明了参数，调用时就必须传递 关键字参数：使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值。 例如： 1234567def printme( name, age ): &quot;打印任何传入的字符串&quot; print (str) return#调用printme函数printme( age = 10, name = &quot;菜鸟教程&quot;) 默认参数：调用函数时，如果没有传递参数，则会使用默认参数,与Kotlin一样 不定长参数：就是可变参数，python的可变参数可以有多种类型，任意指定 例如： 123456789# 可写函数说明def printinfo( arg1, **vardict ):&quot;打印任何传入的参数&quot;print (&quot;输出: &quot;)print (arg1)print (vardict)# 调用printinfo 函数printinfo(1, a=2,b=3) 3.匿名函数语法：lambda [arg1 [,arg2,…..argn]]:expression 示例： 12345# 可写函数说明sum = lambda arg1, arg2: arg1 + arg2# 调用sum函数print (&quot;相加后的值为 : &quot;, sum( 10, 20 )) 4.return语句return [表达式] 语句用于退出函数，选择性地向调用方返回一个表达式，不带参数值的return语句返回None 示例： 1234567def sum( arg1, arg2 ):total = arg1 + arg2print (&quot;函数内 : &quot;, total)return total# 调用sum函数total = sum( 10, 20 ) 变量作用域1.Python的作用域一共有4种 L： （Local） 局部作用域 E： （Enclosing） 闭包函数外的函数中 G： （Global） 全局作用域 B： （Built-in） 内置作用域（内置函数所在模块的范围） 以 L –&gt; E –&gt; G –&gt;B 的规则查找，即：在局部找不到，便会去局部外的局部找（例如闭包），再找不到就会去全局找，再者去内置中找。 示例： 12345g_count = 0 # 全局作用域def outer(): o_count = 1 # 闭包函数外的函数中 def inner(): i_count = 2 # 局部作用域 全局变量和局部变量，与Java类似 2.global 和 nonlocal 关键字当内部作用域想修改外部作用域的变量时，就要用到global和nonlocal关键字: 示例： 1234567891011num = 1 # 外部作用域定义变量def fun1(): global num # 需要使用 global 关键字声明 print(num) # 注意这两想要打印成功，一方面可思议使用global关键字，如果不适用此关键词声明，可以将num放在方法中传递进来使用，否则将会报错，这里跟Java不同，这是因为 fun1 函数中的 num 使用的是局部，未定义，无法修改。 num = 123 print(num)fun1() # 输出: 1 123print(num) # 输出： 123 如果要修改嵌套作用域（enclosing 作用域，外层非全局作用域）中的变量则需要 nonlocal 关键字: 示例： 123456789def outer(): num = 10 # 嵌套作用域 def inner(): nonlocal num # nonlocal关键字声明 num = 100 print(num) inner() print(num)outer() 输出 100 100 模块和包1.模块模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。模块可以被别的程序引入，以使用该模块中的函数等功能，这也是使用 python 标准库的方法import 语句 想使用 Python 源文件，只需在另一个源文件里执行 import 语句，语法如下： import module1, module2,... moduleNfrom … import 语句 Python 的 from 语句让你从模块中导入一个指定的部分到当前命名空间中，语法如下： from modname import name1, name2, ... nameNfrom … import * 语句 把一个模块的所有内容全都导入到当前的命名空间也是可行的，只需使用如下声明： from modname import *dir() 函数 内置的函数 dir() 可以找到模块内定义的所有名称，以一个字符串列表的形式返回2.包包是一种管理 Python 模块命名空间的形式，采用”点模块名称” 比如一个模块的名称是 A.B， 那么他表示一个包 A 中的子模块 B 用户可以每次只导入一个包里面的特定模块，比如: 123import sound.effects.echo这将会导入子模块:sound.effects.echo 它必须使用全名去访问:sound.effects.echo.echofilter(input, output, delay=0.7, atten=4) 还有一种导入子模块的方法是: 123from sound.effects import echo这同样会导入子模块: echo，并且他不需要那些冗长的前缀，所以他可以这样使用:echo.echofilter(input, output, delay=0.7, atten=4) 还有一种变化就是直接导入一个函数或者变量: 123from sound.effects.echo import echofilter同样的，这种方法会导入子模块: echo，并且可以直接使用他的 echofilter() 函数:echofilter(input, output, delay=0.7, atten=4) 基本数据类型变量不需要声明，直接赋值，且赋值后才能使用 1234counter = 1000name = &quot;wangjia&quot;a = b = c = 100 python基本类型 Number（数字） String（字符串） Tuple（元组） List（列表） Set（集合） Dictionary（字典） 注意：前三类是不可变类型 1.python数字类型 Numberint、bool、float、complex（复数） 可以删除对象引用 del var, (del var_a, var_b) 有很多数学函数可以直接调用，比如Java中Math函数中的很多计算函数，在python中直接用就可以，例如 ：abs(-1)，还有一些随机数函数，用的时候查就可以 2.字符串python中没有字符类型，单个字符当做字符串处理 字符串截取：变量[头下标:尾下标] 支持负数，负数代表从后往前截取 使用 “/“ 转义特殊字符，可以在字符串前加 “r” 表示原始字符，例如 r”abc/nvc” 注意字符串格式化： 一般用于日志输出，print (“我叫 %s 今年 %d 岁!” % (‘小明’, 10)) 注意字符串中有四十多个功能内建函数，我们在操作判断关于字符串时查看以后函数是否支持 3.list 列表可以直接初始化: list = [‘12321’, 299, 12.80] 也支持截取，截取方法特性与字符串一样 list是可变的，可以更改元素，list[0] = ‘45654’, 或者批量修改：list[1:3] = [‘45654’, 300, 11.20] 删除元素：list[0] = [], list[0:2] = [] 合并列表，直接 + ，例如： list1 + list2。 list * 2 表示列表元素复制两倍 列表函数： 123456781. 获取长度函数：len(list)2. max(list)：返回列表元素最大值3. min(list)：返回列表元素最小值4. ist(seq)：将元组转换为列表5. list.append(obj)：在列表末尾添加新的对象6. list.index(obj)：从列表中找出某个值第一个匹配项的索引位置7. list.insert(index, obj)：将对象插入列表8. list.remove(obj)：移除列表中某个值的第一个匹配项 del可以根据索引来删除列表元素，例如： del list[2:4] 可以使用append和pop方法将列表作为堆栈使用，等等 4.tuple元组与list类似，但是元素不可变，也支持截取输出 tuple不可变，但是可以包含可变的对象，或list 创建空元组：tup1 = (); 元组不可以修改，但是可组合： tup3 = tup1 + tup2 元组元素不能删除，可以删除整个元组： del tup1 元组运算符支持与list一致 内置函数有：len、max、min、tuple 5.set集合创建方式： set = {1, 2, 3, 4} 或者，set(1) 创建空set 必须使用 set = set() 可以使用 in 关键字判断元素在不在set中， 例如 bool = 2 in set set支持运算，- 表示差集， | 表示并集， &amp; 交集， ^ 不同时存在的元素 基本内置函数： 12341. 添加元素：set.add(元素)2. 删除元素：set.remove( x )3. 计算元素个数：len(s)4. 清空集合：s.clear() 6.Dictionary字典列表是有序的对象集合，字典是无序的对象集合,字典当中的元素是通过键来存取的，类似于map 使用： dict = {} dict[“jjw”] = “wangjia” 字典键不能重复，值无所谓 可以这样创建：d = {key1 : value1, key2 : value2 } 使用字典取值的时候： dict[key] 如果key不存在于字典中，就会抛出异常 字典修改与 Kotlin修改map值一样 删除元素： del dict[key] 字典的键必须是不可变的，不可以使用列表作为键 基本内置函数： 12345671. len(dict)：计算字典元素个数，即键的总数2. str(dict)：输出字典，以可打印的字符串表示，类似于toString()3. radiansdict.get(key, default=None)：返回指定键的值，如果值不在字典中返回default值4. key in dict：如果键在字典dict里返回true，否则返回false5. radiansdict.items()：以列表返回可遍历的(键, 值) 元组数组6. radiansdict.keys()：返回一个迭代器，可以使用 list() 来转换为列表7. radiansdict.values()：返回一个迭代器，可以使用 list() 来转换为列表 遍历字典技巧： 123knights = &#123;&apos;gallahad&apos;: &apos;the pure&apos;, &apos;robin&apos;: &apos;the brave&apos;&#125;for k, v in knights.items(): print(k, v) 7.数据转换方法：数据类型(数据) 即可。 例如 ： float(“10.00”) Python运算符数值运算符数值运算可以直接运算，运算的结果是精确的 (+、-、、/、%、*、\\) 主要说明：其他运算符于Java一致， /是精确除法，与Java不一样 ** 是幂，返回x的y次幂 // 取整除,向下取接近除数的整数 类似于我们Java中的 “&quot; 注意以上运算符都支持赋值运算，例如： += 、 *=、 //= 类型判断内置的 type() 函数可以用来查询变量所指的对象类型， isinstance()会判断类型是否属于某种类型 位运算与Java一致 逻辑运算符例如：a = 10; b = 20; and： x and y: 布尔”与” - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。 (a and b) 返回 20 or x or y: 布尔”或” - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10 not not x: 布尔”非” - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False 成员运算符in 如果在指定的序列中找到值返回 True，否则返回 False not in 如果在指定的序列中没有找到值返回 True，否则返回 False 一般用来判断变量在不在集合中，或者指定字符串中包含不包含特定字符串 身份运算符is: 是判断两个标识符是不是引用自一个对象 is not: 是判断两个标识符是不是引用自不同对象 例如：a = 10; b = 20; a is b 返回结果 true is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等 错误和异常异常即便Python程序的语法是正确的，在运行它的时候，也有可能发生错误。运行期检测到的错误被称为异常，异常的类型有多种，与Java类似 异常处理，使用类似于try-catch语句捕获处理： 12345678for arg in sys.argv[1:]: try: f = open(arg, &apos;r&apos;) except IOError: print(&apos;cannot open&apos;, arg) # 处理异常 else: # else语句表示没有发生任何异常的时候执行的代码块 print(arg, &apos;has&apos;, len(f.readlines()), &apos;lines&apos;) f.close() except就类似于catch，except可以处理多个异常： 12except (RuntimeError, TypeError, NameError): pass 抛出异常Python 使用 raise 语句抛出一个指定的异常，示例： 1raise NameError(&apos;HiThere&apos;) 定义清理行为try 语句还有另外一个可选的子句，它定义了无论在任何情况下都会执行的清理行为，就是finally代码块，具体执行与Java类似： 123456789def divide(x, y): try: result = x / y except ZeroDivisionError: print(&quot;division by zero!&quot;) else: print(&quot;result is&quot;, result) finally: print(&quot;executing finally clause&quot;)","categories":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jjw-story.github.io/tags/Python/"},{"name":"Python基础","slug":"Python基础","permalink":"https://jjw-story.github.io/tags/Python基础/"}],"author":"JJW"},{"title":"vim","slug":"vim","date":"2019-07-20T04:21:42.000Z","updated":"2019-07-25T02:02:10.245Z","comments":true,"path":"2019/07/20/vim/","link":"","permalink":"https://jjw-story.github.io/2019/07/20/vim/","excerpt":"","text":"vi编辑器vi编辑器是所有Unix及Linux系统下标准的编辑器，它的强大不逊色于任何最新的文本编辑器。 由于对Unix及Linux系统的任何版本，vi编辑器是完全相同的，因此您可以在其他任何介绍vi的地方进一步了解它。Vi也是Linux中最基本的文本编辑器，学会它后，您将在Linux的世界里畅行无阻。 vim vi的多模式 正常模式(Normal-mode) 启动vim后默认处于正常模式，其他模式都可以用ESC键直接转换到正常模式。在这个模式我们键盘所敲的任何按键都是对vim所下的命令，如何进行复制如何进行粘贴都是要在这个模式下进行的。 命令模式(Command-mode) 是指可以在界面最底部的一行输入控制操作命令，主要用来进行一些文字编辑的辅助功能，比如字串搜寻、替代、保存文件，以及退出vim等。在命令行模式下输入”:”，或者是使用”?”和”/”键，就可以进入命令模式了。命令模式下输入的命令都会在最底部的一行中显示，按Enter键vim便会执行命令。 插入模式(Insert-mode) 插入模式用来修改文件内容的，只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。 可视模式(Visual-mode) 有一些情况我们要进行一个高级编辑，比如对一块文件进行插入操作，就需要进入此模式。相当于高亮选取文本后的普通模式。在命令模式按下v, V, +v，ctrl+v可以进入可视模式。 vim编辑器所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在，但是目前我们使用比较多的是 vim 编辑器。 vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。 Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说，vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。 vim正常模式首先使用vim命令进入正常模式，使用方法如下： vim [文件] 文件可以不写，类似于我们Windows记事本一样，可以点击一个文本文档打开，或者直接打开记事本进行编写内容。 正常模式常用命令 在正常模式下可以使用 i、I、a、A、o、O 命令进入到插入模式 i 表示从光标当前位置进入插入模式 I 表示从光标当前所在行的行首进入插入模式 a 表示从光标当前位置的下一位进入插入模式 A 表示从光标当前所在行的行尾进图插入模式 o 表示从当前光标所在行的下一行并插入一行进入插入模式 O 表示从当前光标所在行的上一行并插入一行进入插入模式 v命令 正常模式下输入 v 命令可以进入可视模式 :命令 可以进入命令模式，也称为末行模式 esc命令 在其他模式下可以使用 esc 返回到正常模式 h、j、k、l h 光标向左移动 l 光标向右移动 j 光标向下移动 k 光标向上移动 y复制命令 yy:复制光标所在行到缓冲区 nyy:注意n表示行数，表示复制当前行向下n行的内容 y$:复制当前光标所在位置到行尾的内容 yw:与y$效果一致 ny$:注意n表示字数，表示复制当前光标所在位置后n个字符 d剪切命令 dd:剪切光标所在行到缓冲区 ndd:注意n表示行数，表示剪切当前行向下n行的内容 d$:剪切光标所在位置到行尾的内容 dw:与d$效果一致 p粘贴命令 将复制或剪切的内容粘贴到光标所在位置 u撤销命令 撤销命令，可以将失误的操作进行撤销，如果我们连续失误了很多个命令，就多次使用u命令，一条一条撤销 Ctrl + r 重做撤销命令 就是将使用u命令撤销的命令重做，类似于撤回撤销 x命令 删除光标所在的单个字符 r命令 替换光标所在单个字符，使用时先按r键，再输入新的字符 n + G命令 n表示行数，G是大写，既将光标移动到指定的行 如果不指定 n 则直接将光标跳转到文件的最后一行 ^命令 将光标移动到所在行的行尾 $命令 将光标移动到所在行的行首 vim命令模式下面介绍命令模式常用命令操作 :w [文件名] 如果是新建文件，则使用 :w 文件目录+文件名 来将编辑好的内容保存为指定的文件 如果是修改文件，则直接使用 :w 命令保存文件 :q 使用 :q 退出vim w和q命令可以组合起来使用，直接 :wq 来保存并退出vim 注意：:wq 也可以使用快捷键 shift + z z 来实现 :q! 不保存退出 注意：可以使用快捷键 shift + z + q 来实现 :! 有时候我们在打开vim的时候，需要临时执行一条命令，并查看命令执行的结果，就可以使用 ！ 命令 12345678910111213:!ifconfigroot@CHJ-20190520VPS:/# vim /tmp/test.txteth3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.31.34.113 netmask 255.255.255.240 broadcast 172.31.34.127 inet6 fe80::4c47:3429:b72c:5130 prefixlen 64 scopeid 0x0&lt;global&gt; ether 7e:15:d8:27:73:38 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ... / + 字符 向下搜索字符，例如使用 /h 可以在文本中查到h出现的地方，并将光标移动到第一次出现的位置 在查找到的时候，我们还可以使用 “n” 键来将光标移动到下一个此字符出现的位置 还可以使用 “shift + n” 来向上查到此字符出现的位置 ? + 字符 向上搜索字符，例如使用 /h 可以在文本中查到h出现的地方，并将光标移动到第一次出现的位置 在查找到的时候，我们还可以使用 “n” 键来将光标移动到下一个此字符出现的位置 还可以使用 “shift + n” 来向上查到此字符出现的位置 :s/旧的字符/新的字符 此命令的作用是将文本中旧的字符替换成新的字符，模式只是将光标当前所在行的字符替换 我们还可以将文本中每一行第一次出现的指定字符替换，可以使用命令 “:%s/旧的字符/新的字符” 如果我们需要将文本跟中所有的指定的字符替换为新的字符，就需要使用命令 “:%s/旧的字符/新的字符/g” ，命令中的g表示global 1234567891011sdaaaaaaaaaaaaaaaaaaaaaasdfsdaaaaasddsaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaag:%s/a/0/gsd0000000000000000000000sdfsd00000sdds00000000000000000000000000000000000000000000g 有的时候我们需要替换指定的行的特定字符，则需要使用命令 “:3,5/s/旧的字符/新的字符/g”, 表示将第3-5行中的旧的字符替换 :set命令 :set nu: 表示设置显示文本的行号 :set nonu: 表示关闭显示文本的行号 :set hlsearch 在使用查找字符命令时，高亮显示查找到的所有字符 :set nohlsearch 关闭查找字符高亮显示 默认情况下，我们使用set命令设置只在当次vim命令中生效，当我们下次进入vim的时候，set命令设置的东西就又会恢复为默认，这样的话很多时候回造成不必要的麻烦，如果我们需要将set命令设置的内容保存，以便于每次打开都能用，例如 set nu 命令，我们希望每次打开vim编辑文本的时候都能显示行号，这样的话，就需要去修改vim的配置文件，配置文件目录为：/etc/vim/vimrc 我们直接使用 vim /etc/vim/vimrc 命令编辑此配置文件，然后使用 G 命令直接将光标跳转到最后一行，然后使用命令 o 向下插入一行空行，直接编辑我们要设置的内容即可 添加如下： 123456&quot; Source a global configuration file if availableif filereadable(&quot;/etc/vim/vimrc.local&quot;) source /etc/vim/vimrc.localendifset nu 编辑完成之后，使用 esc 退出编辑模式，使用 :wq 保存并退出，这样当我们每次打开文件的时候就都会显示行号 vim插入模式 字符按键以及Shift组合，输入字符 ENTER，回车键，换行 BACK SPACE，退格键，删除光标前一个字符 DEL，删除键，删除光标后一个字符 方向键，在文本中移动光标 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 vim可视模式可视模式主要是针对于我们对文件的大量操作使用此模式一次性执行完成，通常我们都是配合 “I”, “d” 命令来快捷操作 可以在正常模式下使用 v、V、Ctrl + v三种方式进入: 命令 进入字符可视模式，字符可视模式就是当我们移动光标的时候，它是以字符为单位进行选择的 V 进入行可视模式，当我们移动光标的时候是对行进行选中 Ctrl + v 进入块可视模式，移动光标时选中的是上下对齐的一个块，此命令是使用较多的命令 使用示例如下： 我们要在多个行中同时插入一下字符，就可以使用vim先打开文件，然后使用 “Ctrl + v” 选中要操作的多个行为块，然后输入 “I” 命令进入行首进行编辑，插入我们要插入的字符后，连续按两次 esc 按键，就会发现，之前选中的行都被添加进去了我们新添加的字符 1234567891011 18 tyutyutyutyutyustyukcdefsdf 19 sdfsdf 20 sdf 21 sdf# 操作完成后 18 wangjia3tyutyutyutyutyustyukcdefsdf 19 wangjia3sdfsdf 20 wangjia3sdf 21 wangjia3sdf 也可以使用 “d” 命令，将选中的块或字符直接删除，使用方法同上，在块选择后，直接输入 d 即可完成删除，此命令比较常用","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"vim","slug":"vim","permalink":"https://jjw-story.github.io/tags/vim/"}],"author":"JJW"},{"title":"打包压缩与解压缩","slug":"打包压缩与解压缩","date":"2019-07-17T11:43:36.000Z","updated":"2019-07-18T12:48:43.436Z","comments":true,"path":"2019/07/17/打包压缩与解压缩/","link":"","permalink":"https://jjw-story.github.io/2019/07/17/打包压缩与解压缩/","excerpt":"","text":"打包压缩与解压缩打包与压缩windows常见压缩文件格式：.rar .zip .7z Linux常见压缩文件格式 ：.tar.gz; .tar.bz2; .tgz; tbz2 在linux系统中，文件的格式与后缀名没有关系，一般压缩工具压缩之后会在压缩文件后添加对应压缩工具的后缀名 在Windows中，打包与压缩是一个软件功能，但是在Linux中，它是由两个软件构成的 打包命令Linux早期的打包命令其实是备份命令，备份的介质是磁带，使用的命令是 tar 可以对打包后的磁带文件进行压缩存储，压缩的命令是 gzip 和 bzip2，所以我们可以看到，打包和压缩的命令是分开的 tar命令打包的使用方法：tar [选项] 打包后的文件名 要打包的文件 注意：tar命令 使用tar命令需要了解它的选项，来帮助我们完成打包过程，常用的选项如下： c 建立压缩档案，及打包必须的参数 f 打包成文件并指定文件名称，切记，这个参数是最后一个参数，后面只能接文件名 使用示例： 123root@CHJ-20190520VPS:/# tar cf /tmp/etc-backup.tar /etctar: Removing leading `/&apos; from member namesroot@CHJ-20190520VPS:/# 如上示例中，我们将 /etc 目录打包成文件，放置在 /tmp 目录下，并指定打包后的文件名称为 etc-backup.tar ，执行命令后提示会把根目录开头的斜杠 “/“ 去掉，方便我们在解包的时候可以解压到任何目录 压缩命令单纯的打包的打包后的文件一般都很大，因为他并没有做过压缩，一般我们都会在存储的时候进行压缩，如上述打包文件的大小： 12root@CHJ-20190520VPS:/# ls -lh /tmp/etc-backup.tar-rw-r--r-- 1 root root 2.7M Jul 17 20:11 /tmp/etc-backup.tar 通常使用的压缩命令有 gzip bzip2 使用方法如下： gzip [文件名] bzip2 [文件名] 1234root@CHJ-20190520VPS:/# gzip /tmp/etc-backup.tarroot@CHJ-20190520VPS:/# ls -lh /tmptotal 488K-rw-r--r-- 1 root root 459K Jul 17 20:40 etc-backup.tar.gz 在我们使用tar命令的时候，其实已经把这两个命令集成进去了，只需要使用的时候添加参数就可以完成压缩解压缩，下面介绍使用此两个压缩命令的tar选项： z 打包文件并使用gzip压缩文件 j 打包文件并使用bzip2压缩文件 一般在使用打包和压缩命令时，为了方便人看到压缩文件是使用哪种压缩方式压缩的文件，对压缩后的文件使用双扩展名，例如 “xxx.tar.xx”，具体使用如下： 12345678910tar: Removing leading `/&apos; from member namesroot@CHJ-20190520VPS:/# tar czf /tmp/etc-backup.tar.gz /etctar: Removing leading `/&apos; from member namesroot@CHJ-20190520VPS:/# ls -lh /tmptotal 1.0Mdrwxr-xr-x 1 root root 4.0K Jul 13 18:34 a-rw-r--r-- 1 root root 437K Jul 17 20:35 etc-backup.tar.bz2-rw-r--r-- 1 root root 459K Jul 17 20:35 etc-backup.tar.gz-rw-r--r-- 1 root root 551 Jul 14 14:03 testtextroot@CHJ-20190520VPS:/# 虽然压缩后的文件都不大，但是能感觉到使用 bzip2 压缩用的时间明显能更长一点，但是 bzip2 压缩后的文件更小一些，因为 bzip2 压缩后的比例更高一些 两种压缩方式都可以，如果我们希望压缩后的比例更高一下，就使用bzip2进行压缩 解压缩命令解压缩我们使用的命令还是 tar 命令，但是需要更换选项 使用方法：tar [参数] 压缩文件 [-C] [解压后目录] 常用选项说明： x 与上述 c 命令对应，x 参数是解压缩参数 f 与上述一致 v 显示所有进程，及压缩或解压明细 C 注意：C是大写，此选项是可以指定解压后的目录地址 使用示例： 12345root@CHJ-20190520VPS:/# ls /tmpetc-backup.tar etc-backup.tar.gz testtextroot@CHJ-20190520VPS:/# tar xf /tmp/etc-backup.tar -C /rootroot@CHJ-20190520VPS:/# ls /rootetc 将/tmp/etc-backup.tar压缩文件解压到 /root 目录下 实际我们见到的很多的压缩文件是 .tbz2 .tgz 的文件，这两种其实是 .tar.bz2 .tar.gz 的缩写，为了方便网络上的传播，将双扩展名的文件进行的缩写 解压我们不需要因为压缩软件的不同而使用不同的选项，只使用标准的 tar 解压就可以，如下： 12345678root@CHJ-20190520VPS:/# ls /tmpetc-backup.tar etc-backup.tar.gz testtextroot@CHJ-20190520VPS:/# tar -xvf /tmp/etc-backup.tar.gz -C /rootetc/etc/.pwd.lock...... 解压文件明细root@CHJ-20190520VPS:/# ls /rootetc","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"打包压缩与解压缩","slug":"打包压缩与解压缩","permalink":"https://jjw-story.github.io/tags/打包压缩与解压缩/"}],"author":"JJW"},{"title":"文本查看命令","slug":"文本查看命令","date":"2019-07-15T05:55:16.000Z","updated":"2019-07-14T06:58:51.714Z","comments":true,"path":"2019/07/15/文本查看命令/","link":"","permalink":"https://jjw-story.github.io/2019/07/15/文本查看命令/","excerpt":"","text":"文本查看命令cat命令显示文本文件内容，适用于查看整体内容，文件内容不多的，将所有的文本内容都显示到终端 使用方法：cat [参数] 文件 参数说明-n 显示的文本的行编号 -e 显示行结束符号$ 示例： 123456root@CHJ-20190520VPS:/tmp# cat -n -E testtext 1 sdf$ 2 $ 3 sd$ 4 f$ 5 ds$ head命令查看文件的开头的内容，默认显示文件开头的前十行 使用方法：head [参数] 文件 参数说明-n 注意：n 表示行数，意为查看文件的前n行内容 1234root@CHJ-20190520VPS:/tmp# head -3 testtextsdfsd tail命令查看文件的末尾的内容，默认显示文件末的后十行 使用方法：tail [参数] 文件 参数说明-n 注意：n 表示行数，意为查看文件的后n行内容 -f 循环读取文本信息，此命令一般用于文件内容在不断变化的文本查看，一般在查看服务器日志内容时使用 当我们看到文件在一直滚动循环查看，想要停止的时候，使用 ctrl + c命令，退出循环查看，即可停止来具体查看 wc命令wc(Word Count)命令的功能为统计指定文件中的字节数、字数、行数，并将统计结果显示输出。 参数说明-l 统计文本文件的内容行数，一般我们在使用文本查看命令时，不清粗应该使用哪种命令来查看，可以使用此命令来查看文本的行数，然后选择要使用的文本查看命令 12root@CHJ-20190520VPS:/tmp# wc -l testtext206 testtext -c 统计字节数 -m 统计字符数。这个标志不能与 -c 标志一起使用 -w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串 -L 打印最长行的长度 12root@CHJ-20190520VPS:/tmp# wc -L testtext7 testtext more命令分页显示文件内容，还支持直接跳转行等功能，最大的特点是查看每一页文本内容下方都会显示当前当前查看的文本内容所在位置百分比 使用方法：more 文件名 具体操作 Space：显示文本下一屏内容 Enter：只显示文本下一行内容 b：显示文本上一屏内容 q：退出 less命令分页显示文件内容，操作比more更为详细 使用方法：less [参数] 文件名 参数说明-m 显示类似more命令的百分比 -N 注意这里是大写N，显示每行的行号 具体操作 Space：显示文本下一屏内容 b：显示文本上一屏内容 Enter：前进一行 v：后退一行 d：前进半页 u：后退半页 /字符串 向下搜索 ?字符串 向上搜索 左右方向键 相当于水平滚动条 q键：退出","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"文本查看命令","slug":"文本查看命令","permalink":"https://jjw-story.github.io/tags/文本查看命令/"}],"author":"JJW"},{"title":"文件管理命令","slug":"文件管理命令","date":"2019-07-11T12:32:46.000Z","updated":"2019-07-14T05:53:08.747Z","comments":true,"path":"2019/07/11/文件管理命令/","link":"","permalink":"https://jjw-story.github.io/2019/07/11/文件管理命令/","excerpt":"","text":"文件管理命令mkdir命令创建目录 使用方法：mkdir [参数] [目录…] 省略号代表可以建立多个目录 例如：建立一个 demo 目录 在根目录下建立：mkdir /demo 在当前目录下建立：mkdir ./demo “./“可以省略 建立多个目录：mkdir demo1 demo2 demo3 如果目录已存在，建立会失败 12root@CHJ-20190520VPS:/# mkdir homemkdir: cannot create directory ‘home’: File exists 参数的使用-p 一次创建多级目录，既父目录不存在先创建父目录 123456789101112root@CHJ-20190520VPS:/# mkdir -p a/b/c/droot@CHJ-20190520VPS:/# ls -R /a/a:b/a/b:c/a/b/c:d/a/b/c/d: 相比Windows，我们发现命令行的好处就在于可以一次创建多级目录 -v 显示目录创建的过程 123root@CHJ-20190520VPS:/# mkdir -p -v /e/fmkdir: created directory &apos;/e&apos;mkdir: created directory &apos;/e/f&apos; rmdir命令删除目录 使用方法：rmdir [目录…] 注意：rmdir 只能删除空的目录，删除非空目录会失败，必须逐级删除目录中的文件 12root@CHJ-20190520VPS:/# rmdir armdir: failed to remove &apos;a&apos;: Directory not empty rm命令删除文件或目录 使用方法：rm [参数] [目录…] 参数的使用-i 交互模式，在删除前询问用户是否操作 -r 如果删除的是目录，则需要使用此参数，作用是即使目录是非空的，也能逐级删除，但是每一级都要手动确认 -f 即使原档案属性设为唯读，亦直接删除，无需逐一确认。 特别需要注意这条命令，注意他的目录是可以写多个的，如果我们的命令写成 “rm -r -f / a “，既 不小心在 “/“ 与 “a” 之间多了一个空格，那么系统下所有的文件都会被删除，而且不会有任何提示。 所以我们在使用此项命令时一定要留意检查，避免操作失误 -r -f 可以合并使用： 12345root@CHJ-20190520VPS:/# mkdir -p a/b/c/d/f/eroot@CHJ-20190520VPS:/# rm -rf aroot@CHJ-20190520VPS:/# lsbin boot dev e etc home init lib lib64 media mnt opt proc root run sbin snap srv sys tmp usr varroot@CHJ-20190520VPS:/# cp命令文件复制命令，copy简写 使用方法： cp [参数] [文件…] 目录 cp [参数] [文件] 文件…目录 参数的使用-r 当我们直接使用cp命令复制目录的时候，是会失败的，因为cp命令本身是只复制文件，递归复制，用于目录的复制操作 12345root@CHJ-20190520VPS:/# cp a /tmpcp: -r not specified; omitting directory &apos;a&apos;root@CHJ-20190520VPS:/# cp -r a /tmproot@CHJ-20190520VPS:/# ls /tmpa -v 复制时显示复制信息，类似进度条，直接复制文件的时候不会有任何提示 12root@CHJ-20190520VPS:/# cp -v /filea /tmp&apos;/filea&apos; -&gt; &apos;/tmp/filea&apos; -p 与文件的属性一起复制，而非使用默认属性，例如文件创建更新时间 -a 与文件的属性一起复制，包括文件的属主，权限等 123456root@CHJ-20190520VPS:/# cp -v -a filea /tmp&apos;filea&apos; -&gt; &apos;/tmp/filea&apos;root@CHJ-20190520VPS:/# ls -l /tmp/filea-rw-r--r-- 1 root root 0 Jul 13 18:47 /tmp/filearoot@CHJ-20190520VPS:/# ls -l filea-rw-r--r-- 1 root root 0 Jul 13 18:47 filea -i 若目标文件已存在，在覆盖时会先询问是否真的操作 123root@CHJ-20190520VPS:/# cp -v -a -i filea /tmpcp: overwrite &apos;/tmp/filea&apos;? yes&apos;filea&apos; -&gt; &apos;/tmp/filea&apos; 注意词命令的第二种语法 表示将文件复制并重命令为自定义名称 12345root@CHJ-20190520VPS:/# cp -v -a -i -r filea /tmp/fileb&apos;filea&apos; -&gt; &apos;/tmp/fileb&apos;root@CHJ-20190520VPS:/# ls /tmpa filea filebroot@CHJ-20190520VPS:/# mv命令mv命令有两个功能，一个是文件及文件夹的移动功能，另一个是重命名功能 使用方法： mv [参数] 源文件 目录…文件名 重命名演示： 1234root@CHJ-20190520VPS:/# mv filea filebroot@CHJ-20190520VPS:/# lsa boot e fileb init lib64 mnt proc run snap sys usrbin dev etc home lib media opt root sbin srv tmp var 注意：重命名的本质其实就是将文件移动 移动演示： 123root@CHJ-20190520VPS:/# mv fileb /tmproot@CHJ-20190520VPS:/# ls /tmpa filea fileb 还可以移动并重命名，使用命令： mv filea /tmp/filec 通配符的使用* *号表示匹配当前目录下所有目录及文件 使用示例，例如我们在 /tmp 目录下创建三个文件 filea、filebb、fileccc，然后使用通配符将此三个文件复制到其他目录下 123456root@CHJ-20190520VPS:/tmp# lsa dira dirb dirc filea fileb filecroot@CHJ-20190520VPS:/# cp /tmp/file* /root@CHJ-20190520VPS:/# lsa boot e filea fileccc init lib64 mnt proc run snap sys usrbin dev etc filebb home lib media opt root sbin srv tmp var ? ? 号与 * 作用相同，但是它只匹配一个字符，* 匹配多个字符 1234567891011root@CHJ-20190520VPS:/# lsa boot e home lib media opt root sbin srv tmp varbin dev etc init lib64 mnt proc run snap sys usrroot@CHJ-20190520VPS:/# ls /tmp/file*/tmp/filea /tmp/filebb /tmp/filecccroot@CHJ-20190520VPS:/# cp -v /tmp/file? /&apos;/tmp/filea&apos; -&gt; &apos;/filea&apos;root@CHJ-20190520VPS:/# lsa boot e filea init lib64 mnt proc run snap sys usrbin dev etc home lib media opt root sbin srv tmp var 通过示例我们发现这里只复制过来 filea 目录，所以 ? 表示只匹配一个字符 注意上面我们使用ls命令也是用了通配符，表示通配符可以在很多命令中使用","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"文件管理命令","slug":"文件管理命令","permalink":"https://jjw-story.github.io/tags/文件管理命令/"}],"author":"JJW"},{"title":"文件查看命令","slug":"文件查看命令","date":"2019-07-08T04:55:40.000Z","updated":"2019-07-27T06:23:23.261Z","comments":true,"path":"2019/07/08/文件查看命令/","link":"","permalink":"https://jjw-story.github.io/2019/07/08/文件查看命令/","excerpt":"","text":"文件查看命令文件管理命令是Linux管理的核心，因为Linux中有一个非常重要的概念-一切皆文件。在Windows系统中存在注册表、设备管理器了等等各种各样的组建来管理Windows，但是在Linux中，我们系统的管理控制等通通都是文件，所以文件管理命令是Linux管理中非常重要的内容。 pwd命令显示出完整的当前活动目录名称 注意：目录结构中 “/“ 目录和 “/root” 目录是两个不同的目录，”/“目录是我们的根目录，”/root”是root用户的家目录 ls命令列出目录的内容 使用方法：ls [选项] [文件名称…] 如果不写文件名称，默认代表当前目录 省略号表示支持多个文件或者目录名称，多个文件或目录中间用空格隔开（可以用多个空格） 查询内容有颜色不同，代表着不同的权限。不同的客户端颜色展示可能不同 ls基本选项说明-l使用详细格式列表，此命令可以直接缩写为 ll 命令执行如下及结果说明： 123456789root@CHJ-20190520VPS:/usr/lib# ls -ltotal 920drwxr-xr-x 1 root root 4096 May 21 22:39 kerneldrwxr-xr-x 1 root root 4096 May 21 22:39 klibcdrwxr-xr-x 1 root root 4096 May 21 22:40 language-selectorlrwxrwxrwx 1 root root 21 Feb 12 16:55 libDeployPkg.so.0 -&gt; libDeployPkg.so.0.0.0-rw-r--r-- 1 root root 31280 Feb 12 16:55 libDeployPkg.so.0.0.0lrwxrwxrwx 1 root root 20 Feb 12 16:55 libguestlib.so.0 -&gt; libguestlib.so.0.0.0-rw-r--r-- 1 root root 22656 Feb 12 16:55 libguestlib.so.0.0.0 一共查询出七列内容，分别表示： 文件属性(占10个字符空间)、拥有的文件数量、文件的创建者、所属的group、文件大小、建档日期、文件名 重点说明文件属性代表的内容： Linux的文件基本上分为三个属性：可读（r），可写（w），可执行（x） 但是这里有十个格子可以添（具体程序实现时，实际上是十个bit位） 第一个小格是特殊表示格，表示目录或连结文件等等，d表示目录，例如drwx——;l表示连结文件，如lrwxrwxrwx;如果是以一横“-”表示，则表示这是文件 其余剩下的格子就以每3格为一个单位，因为Linux是多用户多任务系统，所以一个文件可能同时被许多人使用，所以我们一定要设好每个文件的权限，其文件的权限位置排列顺序是（以-rwxr-xr-x为例）： rwx(Owner)r-x(Group)r-x(Other) 这个例子表示的权限是：使用者自己可读，可写，可执行；同一组的用户可读，不可写，可执行；其它用户可读，不可写，可执行。 另外，有一些程序属性的执行部分不是X,而是S,这表示执行这个程序的使用者，临时可以有和拥有者一样权力的身份来执行该程序。一般出现在系统管理之类的指令或程序，让使用者执行时，拥有root身份。 -a显示全部文件包括隐藏的文件 Linux隐藏文件的目的是为了在用户日常操作中不会误操作或修改掉一些不可修改的文件内容 Linux创建隐藏文件的方式很简单，只需要在文件名前面加一个 “.” 即可 -t按照文件创建或最后修改的时间排序，默认是根据文件的名称来逆向排序 -r逆向排序显示文件 一般是配合 -l 来使用，例如： ls -l -r 如果我们需要按照文件的创建/修改时间来进行逆向排序则可以使用命令 “-t”，例如： ls -l -r -t 可以组合命令，多个参数不需要每个都用空格隔开，例如上述命令，可以写为： ls -lrt -R递归显示文件，就是罗列出当前文件中所有的文件及文件夹，还有子文件夹中的文件夹及文件，都罗列出来 -h将文件大小数据显示转化为可以阅读清楚的大小表示单位 –full-time列出文件完整的日期时间 –color={auto,never,always}用颜色来表示不同的文件类型，大括号内是参数选项 never：从不使用颜色表示不同类型 always：总是使用颜色表示不同类型 auto：根据终端属性自动确定是否使用颜色表示不同类型 cd命令cd命令用于切换当前工作目录至 dirName(目录参数) 使用方法： cd /path/to…绝对路径 cd /path/to…相对路径 注意一些特殊参数： 路径缺省，表示切换到当前用户的目录 ~ 也是切换到当前用户的目录 / 切换到根目录 ../ 切换到上一层目录，注意 “/“ 可以省略也可以 “cd ../..” 切换到上两级目录 - 切换到上一次访问的目录 12345wangjia3@CHJ-20190520VPS:/home$ pwd/homewangjia3@CHJ-20190520VPS:/home$ cd -/usr/local/libwangjia3@CHJ-20190520VPS:/usr/local/lib$ 当我们要切换的目录离根目录比较近，那就使用绝对路径 当我们要切换的目录离当前目录比较近，那就使用相对路径","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"文件查看命令","slug":"文件查看命令","permalink":"https://jjw-story.github.io/tags/文件查看命令/"}],"author":"JJW"},{"title":"IDEA快捷键","slug":"IDEA快捷键","date":"2019-07-07T10:00:00.000Z","updated":"2019-11-20T01:37:16.449Z","comments":true,"path":"2019/07/07/IDEA快捷键/","link":"","permalink":"https://jjw-story.github.io/2019/07/07/IDEA快捷键/","excerpt":"","text":"IDEA快捷键 向上箭头 ctrl+i 向上一行选中 ctrl+shfit+i 当前行内容向上移动一行 ctrl+alt+i 当前行内容向上插入复制一行 ctrl+shift+alt+i 向下箭头 ctrl+k 向下选中一行 ctrl+shift+k 当前行内容向下移动一行 ctrl+alt+k 当前行内容向下插入复制一行 ctrl+shift+alt+k END ctrl+o 选中到END ctrl+shift+o HOME ctrl+u 选中到HOME ctrl+shift+u 向左移动一个单词 ctrl+j 向左移动一个字母 ctrl+alt+j 向左选中一个单词 ctrl+shift+j 向左选中一个字母 ctrl+shift+alt+j 向右移动一个单词 ctrl+l 向右移动一个字母 ctrl+alt+l 向右选中一个单词 ctrl+shift+l 向右选中一个字母 ctrl+shift+alt+l 打开行数跳转框 ctrl+g 切换到上一个编辑窗口 ctrl+, 切换到下一个编辑窗口 ctrl+. 关闭当前编辑窗口 ctrl+w 打开查找框 ctrl+f3 向下查找 f3 向上查找 shift+f3 显示意图动作 ctrl+空格，alt+enter project框移动 ctrl+i，ctrl+k 打开关闭：enter 删除一行 ctrl+alt+d 打开各种功能框 alt+功能框框对应数字 打开接口实现类，进入方法内部，获取方法在哪里被调用（类似eclipse的ctrl+alt+h） ctrl+b或者ctrl+shift+F7 RUN shift+f9 DEBUG shift+f10 括号跳转（头-尾） ctrl+m+b 当前文件文本查找 ctrl+f 查找文件（查找类文件） 双击shift project中查找包含指定文本的文件 ctrl+shift+f 查看某个类包含的所有属性方法 ctrl+F12 打开后可以直接输入方法名查找过滤，对应Eclipse ctrl+o","categories":[{"name":"IDEA","slug":"IDEA","permalink":"https://jjw-story.github.io/categories/IDEA/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"https://jjw-story.github.io/tags/IDEA/"}],"author":"JJW"},{"title":"帮助命令","slug":"帮助命令","date":"2019-07-06T12:58:05.000Z","updated":"2019-07-27T05:13:22.166Z","comments":true,"path":"2019/07/06/帮助命令/","link":"","permalink":"https://jjw-story.github.io/2019/07/06/帮助命令/","excerpt":"","text":"man命令man命令 man是manual的缩写（有问题找男人帮忙） 使用方法：man [章数] [命令] 可以查询到要查询的命令具体作用、参数选项、描述等。查看完毕后按 q 退出 man命令本身也是一个命令，所以可以通过 [man man] 命令查看此命令本身的一些帮助文档 man命令帮助内容man命令帮助内容一共可以有九章的帮助内容，分别如下： Commands 用户可从 shell 运行的命令,查询第一章的内容的时候 1 可以省略 System calls 必须由内核完成的功能，系统调用 Library calls 大多数 libs 函数，如 sort(3) 此命令与第三章命令一般是用在我们在编程过程中获取函数的帮助文档使用的 Special files /dev 目录中的文件，第四章和第五章主要是文件的帮助 File formats and conventions /etc/pass 等人类可读的配置文件等格式及说明 Game Macro packages and conveentions 文件系统标准描述，网络协议，ASCII和其他字符集等 System Management commands 类似 mount(8) 等命令，大部分只能由 root 执行 Kernel routes 废弃的章节，原本是想把一些关于核心的文件放在这里 man一共有九个章节的帮助，分为这么多章主要是因为命令和系统调用还有文件有的时候会出现重名的情况，一旦重名，我们只单用一个man不加章节很难区分 例如 passwd 命令，这个命令是进行用户密码设置的命令，但是在我们的 /etc 目录下，还有一个 passwd 的一个配置文件，如果我们只使用 man passwd 的命令，很难区分出到底是对这个命令的帮助文档，还是对这个配置文件的帮助文档，这时，我们就可以通过章节这个参数来进行区分 有的时候我们并不知道要查看的帮助到底是命令还是配置文件等，可以使用 man a [参数] 来详细查看所有的帮助文档，在查看完一条之后，按 q 退出，即会提示有其他条的帮助文档，我们可以选择查看 man命令说明页含义 标头 含义 Name 命令的名称和用途 Synopsis 命令语法 Description 完整描述 Environment 命令使用的环境变量 Author 开发该程序者 Files 对该命令重要的文件列表 See also 相关信息 Diagnostics 可能的错误和警告 Bugs （可能没有） help命令help命令也是帮助命令，它使用分为内部命令使用帮助、外部命令使用帮助 内部命令和外部命令shell（命令解释器）自带的命令成为内部命令，其他的是外部命令 内部命令内部命令实际上是shell程序的一部分，其中包含的是一些比较简单的linux系统命令，这些命令由shell程序识别并在shell程序内部完成运行，通常在linux系统加载运行时shell就被加载并驻留在系统内存中。内部命令是写在bashy源码里面的，其执行速度比外部命令快，因为解析内部命令shell不需要创建子进程。比如：exit，history，cd，echo等。 外部命令外部命令是linux系统中的实用程序部分，因为实用程序的功能通常都比较强大，所以其包含的程序量也会很大，在系统加载时并不随系统一起被加载到内存中，而是在需要时才将其调用内存。通常外部命令的实体并不包含在shell中，但是其命令执行过程是由shell程序控制的。shell程序管理外部命令执行的路径查找、加载存放，并控制命令的执行。外部命令是在bash之外额外安装的，通常放在/bin，/usr/bin，/sbin，/usr/sbin 等等。可通过 “echo $PATH” 命令查看外部命令的存储路径，比如：ls、vi等。 使用type命令区分内外部命令使用方法：type [命令] 1234root@CHJ-20190520VPS:~# type cdcd is a shell builtinroot@CHJ-20190520VPS:~# type mkdirmkdir is /bin/mkdir 内部命令和外部命令最大的区别之处就是性能。内部命令由于构建在shell中而不必创建多余的进程，要比外部命令执行快得多。因此和执行更大的脚本道理一样，执行包含很多外部命令的脚本会损害脚本的性能。 help命令用法 内部命令 help [命令] 外部命令 [命令] --help 帮助命令总结Linux的基本操作方式是命令行，通过命令行的话就需要熟记很多的操作命令，但是海量的命令不适合死记硬背。 当我们使用到陌生的命令的时候，就可以使用 man help 等帮助命令查询它的帮助文档，来帮助我们了解这些命令。 注意：很多内部命令 man 是没有帮助文档的，所以我们使用更多的应该是 help 命令。 which命令查看可执行文件的位置，从全局环境变量PATH里面查找对应的路径，默认是找 bash内所规范的目录，一般用来确认系统中是否安装了指定软件 在PATH变量指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果 使用方法：which [参数] 命令 -a 打印出PATH中的所有匹配项，而不是仅仅第一个 –skip-dot 跳过PATH中以点开头的目录 –skip-tilde 跳过PATH中以波浪号开头的目录 使用示例： 1234root@CHJ-20190520VPS:~# which shutdown/sbin/shutdownroot@CHJ-20190520VPS:~# which cdroot@CHJ-20190520VPS:~# 注意：我们发现查找 cd 命令竟然没有找到，这是因为 cd 是bash 内建的命令！ 但是 which 默认是找 PATH 内所规范的目录，所以当然一定找不到的","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"command","slug":"command","permalink":"https://jjw-story.github.io/tags/command/"},{"name":"帮助命令","slug":"帮助命令","permalink":"https://jjw-story.github.io/tags/帮助命令/"}],"author":"JJW"},{"title":"初识Linux","slug":"什么是Linux","date":"2019-07-06T01:49:19.000Z","updated":"2019-07-07T04:57:08.258Z","comments":true,"path":"2019/07/06/什么是Linux/","link":"","permalink":"https://jjw-story.github.io/2019/07/06/什么是Linux/","excerpt":"","text":"什么是LinuxLinux有两种含义 一种是Linus编写的操作系统的内核 另一种是广义的操作系统 一般我们所说的Linux就是说广义的操作系统 服务端操作系统一般都是使用命令行的方式进行操作,主要因为服务端操作系统与客户端操作系统所做的事情不一样,服务端主要追求稳定 Linux版本内核版本 内核版本分为三个部分,一般使用的是稳定版 稳定版又分为三个版本号，分别是 主版本号 次版本号 末版本号 次版本号为奇数为开发版，偶数为稳定版 发行版本 Red Hat EnterPrise 特点：软件经过专业人员的测试，非常稳定，有大公司支持，但是在技术支持和更新最新的安全漏洞补丁的时候是需要付费的 Fedora 特点：也是Red Hat公司发行的，不同之处是发行方式是组建一个社区，来免费提供操作系统，软件要比上述新，但是没有经过专业的测试，稳定性要差 CentOS 特点：基于Red Hat EnterPrise源代码进行编译的，可以免费试用 Ubuntu 特点：定制了非常华丽的界面，可以直接安装在PC机上进行操作 Debian 特点：与Ubantu一样 终端的使用 图形终端 命令行终端 远程终端（SSH VNC） 通过互联网远程连接终端，实际生产使用较多 Linux常见目录介绍 / 根目录 /root root用户的家目录 /home/username 普通用户的家目录 /etc 配置文件目录 /bin 命令目录 /sbin 管理命令目录 /usr/bin /usr/sbin 系统预装的其他命令","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jjw-story.github.io/categories/Linux/"}],"tags":[{"name":"overview","slug":"overview","permalink":"https://jjw-story.github.io/tags/overview/"}],"author":"JJW"}]}